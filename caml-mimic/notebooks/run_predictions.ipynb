{"cells":[{"cell_type":"markdown","source":["# Run Predictions\n","\n","### Mount colab files"],"metadata":{"id":"LxZkfsA_pXfD"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16230,"status":"ok","timestamp":1651812280267,"user":{"displayName":"Chris Rock","userId":"13280703149428069160"},"user_tz":420},"id":"SGbbvNVEoMuf","outputId":"58ff5325-580f-4f7d-a4ea-870c6c36bb60"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n"]},{"cell_type":"code","source":["%cd /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UQFgzCce2_1v","executionInfo":{"status":"ok","timestamp":1651812288915,"user_tz":420,"elapsed":2532,"user":{"displayName":"Chris Rock","userId":"13280703149428069160"}},"outputId":"9d817ff8-2a99-4a96-dc83-754e98bf1d17"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic\n"]}]},{"cell_type":"markdown","source":["### Add CAML-MIMIC to PYTHONPATH"],"metadata":{"id":"DkFmgFP0race"}},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":165,"status":"ok","timestamp":1651812296938,"user":{"displayName":"Chris Rock","userId":"13280703149428069160"},"user_tz":420},"id":"spupZyvFoMuf","outputId":"8adf2120-f19e-4306-c54f-4b5b7f732494"},"outputs":[{"output_type":"stream","name":"stdout","text":["env: PYTHONPATH=$PYTHONPATH:/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/\n","$PYTHONPATH:/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/\n"]}],"source":["%env PYTHONPATH=$PYTHONPATH:/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/\n","!echo $PYTHONPATH"]},{"cell_type":"markdown","source":["### Train CAML_mimic3_full\n","Starting with model.pth which is saved in the directory"],"metadata":{"id":"E4c6n7iQrh5-"}},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":93172,"status":"ok","timestamp":1651543280248,"user":{"displayName":"Chris Rock","userId":"13280703149428069160"},"user_tz":420},"id":"8LqyHD2v_gk9","outputId":"84d05dc6-1d91-4a6c-9030-a1c29ce285a9"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/predictions/CAML_mimic3_full\n","total 40403\n","-rw------- 1 root root      284 Apr 27 01:57 train_new_model.sh\n","-rw------- 1 root root 24608583 Apr 27 01:57 model.pth\n","-rw------- 1 root root      244 Apr 28 06:03 evaluate_model.sh\n","-rw------- 1 root root   155637 May  2 21:09 preds_dev.psv\n","-rw------- 1 root root  5307653 May  2 21:09 pred_100_scores_dev.json\n","-rw------- 1 root root   329509 May  2 21:10 preds_test.psv\n","-rw------- 1 root root 10966069 May  2 21:10 pred_100_scores_test.json\n","-rw------- 1 root root     1474 May  2 21:10 metrics.json\n","-rw------- 1 root root      715 May  2 21:10 params.json\n","ARGS: Namespace(Y='full', batch_size=16, bidirectional=None, cell_type='gru', code_emb=None, command='python ../../learn/training.py ../../mimicdata/mimic3/train_full.csv ../../mimicdata/mimic3/vocab.csv full conv_attn 200 --filter-size 10 --num-filter-maps 50 --dropout 0.2 --patience 10 --lr 0.0001 --public-model --test-model model.pth --gpu', criterion='f1_micro', data_path='../../mimicdata/mimic3/train_full.csv', dropout=0.2, embed_file=None, embed_size=100, filter_size='10', gpu=True, lmbda=0, lr=0.0001, model='conv_attn', n_epochs=200, num_filter_maps=50, patience=10, pool=None, public_model=True, quiet=None, rnn_dim=128, rnn_layers=1, samples=None, stack_filters=None, test_model='model.pth', version='mimic3', vocab='../../mimicdata/mimic3/vocab.csv', weight_decay=0)\n","loading lookups...\n","ConvAttnPool(\n","  (embed_drop): Dropout(p=0.2, inplace=False)\n","  (embed): Embedding(51919, 100, padding_idx=0)\n","  (conv): Conv1d(100, 50, kernel_size=(10,), stride=(1,), padding=(5,))\n","  (U): Linear(in_features=50, out_features=8921, bias=True)\n","  (final): Linear(in_features=50, out_features=8921, bias=True)\n",")\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:08, 198.62it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0558, 0.0778, 0.0762, 0.0770, 0.9038\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3725, 0.6086, 0.4899, 0.5429, 0.9855\n","rec_at_8: 0.3883\n","prec_at_8: 0.7135\n","rec_at_15: 0.5404\n","prec_at_15: 0.5582\n","\n","\n","evaluating on test\n","file for evaluation: ../../mimicdata/mimic3/test_full.csv\n","3372it [00:15, 215.79it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0608, 0.0913, 0.0856, 0.0884, 0.8948\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3686, 0.6070, 0.4842, 0.5387, 0.9855\n","rec_at_8: 0.3727\n","prec_at_8: 0.7094\n","rec_at_15: 0.5257\n","prec_at_15: 0.5611\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/predictions/CAML_mimic3_full\n","\n","TOTAL ELAPSED TIME FOR conv_attn MODEL AND 1 EPOCHS: 90.432810\n"]}],"source":["%cd /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/predictions/CAML_mimic3_full/\n","!ls -latr\n","!python ../../learn/training.py\\\n","    ../../mimicdata/mimic3/train_full.csv\\\n","    ../../mimicdata/mimic3/vocab.csv\\\n","    full conv_attn 200 --filter-size 10 --num-filter-maps 50\\\n","    --dropout 0.2 --patience 10 --lr 0.0001 --public-model\\\n","    --test-model model.pth --gpu"]},{"cell_type":"markdown","source":["### Train CAML_mimic3_50\n","Starting with model.pth which is saved in the directory"],"metadata":{"id":"VcJJE93PrvSo"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15084,"status":"ok","timestamp":1651300662443,"user":{"displayName":"Chris Rock","userId":"13280703149428069160"},"user_tz":420},"id":"choYg-S0ANvF","outputId":"da231f65-293a-479d-dc93-5a1a93c1b6a9"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/predictions/CAML_mimic3_50\n","ARGS: Namespace(Y='50', batch_size=16, bidirectional=None, cell_type='gru', code_emb=None, command='python ../../learn/training.py ../../mimicdata/mimic3/train_50.csv ../../mimicdata/mimic3/vocab.csv 50 conv_attn 200 --filter-size 10 --num-filter-maps 50 --dropout 0.2 --patience 10 --criterion prec_at_8 --lr 0.0001 --public-model --test-model model.pth --gpu', criterion='prec_at_8', data_path='../../mimicdata/mimic3/train_50.csv', dropout=0.2, embed_file=None, embed_size=100, filter_size='10', gpu=True, lmbda=0, lr=0.0001, model='conv_attn', n_epochs=200, num_filter_maps=50, patience=10, pool=None, public_model=True, quiet=None, rnn_dim=128, rnn_layers=1, samples=None, stack_filters=None, test_model='model.pth', version='mimic3', vocab='../../mimicdata/mimic3/vocab.csv', weight_decay=0)\n","loading lookups...\n","ConvAttnPool(\n","  (embed_drop): Dropout(p=0.2, inplace=False)\n","  (embed): Embedding(51919, 100, padding_idx=0)\n","  (conv): Conv1d(100, 50, kernel_size=(10,), stride=(1,), padding=(5,))\n","  (U): Linear(in_features=50, out_features=50, bias=True)\n","  (final): Linear(in_features=50, out_features=50, bias=True)\n",")\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:03, 476.97it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3951, 0.6139, 0.4824, 0.5403, 0.8800\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4468, 0.7163, 0.5428, 0.6176, 0.9114\n","rec_at_5: 0.5899\n","prec_at_5: 0.6043\n","\n","\n","evaluating on test\n","file for evaluation: ../../mimicdata/mimic3/test_50.csv\n","1729it [00:03, 456.76it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3933, 0.6035, 0.4799, 0.5346, 0.8768\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4431, 0.7144, 0.5384, 0.6141, 0.9101\n","rec_at_5: 0.5862\n","prec_at_5: 0.6109\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/predictions/CAML_mimic3_50\n","\n","TOTAL ELAPSED TIME FOR conv_attn MODEL AND 1 EPOCHS: 12.964699\n"]}],"source":["%cd /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/predictions/CAML_mimic3_50/\n","!python ../../learn/training.py ../../mimicdata/mimic3/train_50.csv ../../mimicdata/mimic3/vocab.csv 50 conv_attn 200 --filter-size 10 --num-filter-maps 50 --dropout 0.2 --patience 10 --criterion prec_at_8 --lr 0.0001 --public-model --test-model model.pth --gpu"]},{"cell_type":"markdown","source":["### Run DRCAML_mimic3_full\n","Start from model.pth"],"metadata":{"id":"kIfqrJ3Qr19w"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":90540,"status":"ok","timestamp":1651301216052,"user":{"displayName":"Chris Rock","userId":"13280703149428069160"},"user_tz":420},"id":"Uzz31IC5o2Co","outputId":"a679f0f6-4b31-4e4f-e65d-e11dab55e7c7"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/predictions/DRCAML_mimic3_full\n","ARGS: Namespace(Y='full', batch_size=16, bidirectional=None, cell_type='gru', code_emb=None, command='python ../../learn/training.py ../../mimicdata/mimic3/train_full.csv ../../mimicdata/mimic3/vocab.csv full conv_attn 200 --filter-size 10 --num-filter-maps 50 --dropout 0.2 --patience 10 --criterion prec_at_8 --lr 0.0001 --lmbda 0.01 --public-model --test-model model.pth --gpu', criterion='prec_at_8', data_path='../../mimicdata/mimic3/train_full.csv', dropout=0.2, embed_file=None, embed_size=100, filter_size='10', gpu=True, lmbda=0.01, lr=0.0001, model='conv_attn', n_epochs=200, num_filter_maps=50, patience=10, pool=None, public_model=True, quiet=None, rnn_dim=128, rnn_layers=1, samples=None, stack_filters=None, test_model='model.pth', version='mimic3', vocab='../../mimicdata/mimic3/vocab.csv', weight_decay=0)\n","loading lookups...\n","ConvAttnPool(\n","  (embed_drop): Dropout(p=0.2, inplace=False)\n","  (embed): Embedding(51919, 100, padding_idx=0)\n","  (conv): Conv1d(100, 50, kernel_size=(10,), stride=(1,), padding=(5,))\n","  (U): Linear(in_features=50, out_features=8921, bias=True)\n","  (final): Linear(in_features=50, out_features=8921, bias=True)\n","  (desc_embedding): Embedding(51919, 100, padding_idx=0)\n","  (label_conv): Conv1d(100, 50, kernel_size=(10,), stride=(1,), padding=(5,))\n","  (label_fc1): Linear(in_features=50, out_features=50, bias=True)\n",")\n","getting set of codes not in training set\n","num codes not in train set: 236\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:09, 180.19it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0536, 0.0734, 0.0779, 0.0756, 0.9056\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3635, 0.5563, 0.5118, 0.5331, 0.9851\n","rec_at_8: 0.3793\n","prec_at_8: 0.6984\n","rec_at_15: 0.5308\n","prec_at_15: 0.5458\n","\n","\n","evaluating on test\n","file for evaluation: ../../mimicdata/mimic3/test_full.csv\n","3372it [00:18, 183.57it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0590, 0.0848, 0.0881, 0.0864, 0.8966\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3596, 0.5525, 0.5075, 0.5290, 0.9850\n","rec_at_8: 0.3615\n","prec_at_8: 0.6899\n","rec_at_15: 0.5124\n","prec_at_15: 0.5475\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/predictions/DRCAML_mimic3_full\n","\n","TOTAL ELAPSED TIME FOR conv_attn MODEL AND 1 EPOCHS: 88.089473\n"]}],"source":["%cd /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/predictions/DRCAML_mimic3_full/\n","!python ../../learn/training.py ../../mimicdata/mimic3/train_full.csv ../../mimicdata/mimic3/vocab.csv full conv_attn 200 --filter-size 10 --num-filter-maps 50 --dropout 0.2 --patience 10 --criterion prec_at_8 --lr 0.0001 --lmbda 0.01 --public-model --test-model model.pth --gpu\n"]},{"cell_type":"markdown","source":["### DRCAML_mimic3_50\n","No starting model"],"metadata":{"id":"AE0Pv0UftUWp"}},{"cell_type":"code","source":["%cd /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/predictions/DRCAML_mimic3_50/\n","!python ../../learn/training.py ../../mimicdata/mimic3/train_50.csv ../../mimicdata/mimic3/vocab.csv 50 conv_attn 200 --filter-size 10 --num-filter-maps 50 --dropout 0.2 --patience 10 --criterion prec_at_8 --lr 0.0001 --lmbda 10 --embed-file ../../mimicdata/mimic3/processed_full.embed --gpu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"NaTpF2eCtYiM","executionInfo":{"status":"ok","timestamp":1651549708081,"user_tz":420,"elapsed":5372332,"user":{"displayName":"Chris Rock","userId":"13280703149428069160"}},"outputId":"2a1ea82e-a1b3-4bfe-ba2e-2815492c3a3f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","0.3877, 0.6364, 0.4717, 0.5418, 0.8746\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4319, 0.7113, 0.5238, 0.6033, 0.9057\n","rec_at_5: 0.5882\n","prec_at_5: 0.5957\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 69\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 69 [batch #0, batch_size 16, seq length 212]\tLoss: 0.177334\n","23it [00:00, 31.81it/s]Train epoch: 69 [batch #25, batch_size 16, seq length 571]\tLoss: 0.100555\n","47it [00:01, 30.75it/s]Train epoch: 69 [batch #50, batch_size 16, seq length 709]\tLoss: 0.104523\n","74it [00:02, 29.40it/s]Train epoch: 69 [batch #75, batch_size 16, seq length 806]\tLoss: 0.123269\n","99it [00:03, 28.55it/s]Train epoch: 69 [batch #100, batch_size 16, seq length 892]\tLoss: 0.106364\n","123it [00:04, 27.39it/s]Train epoch: 69 [batch #125, batch_size 16, seq length 978]\tLoss: 0.110091\n","150it [00:05, 25.56it/s]Train epoch: 69 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.132171\n","174it [00:06, 25.30it/s]Train epoch: 69 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.114117\n","198it [00:07, 25.48it/s]Train epoch: 69 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.123128\n","225it [00:08, 25.00it/s]Train epoch: 69 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.125263\n","249it [00:09, 23.76it/s]Train epoch: 69 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.135218\n","273it [00:10, 23.36it/s]Train epoch: 69 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.144369\n","300it [00:11, 22.96it/s]Train epoch: 69 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.151752\n","324it [00:12, 22.03it/s]Train epoch: 69 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.136459\n","348it [00:13, 21.66it/s]Train epoch: 69 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.145432\n","375it [00:14, 21.33it/s]Train epoch: 69 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.153512\n","399it [00:15, 21.40it/s]Train epoch: 69 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.156796\n","423it [00:17, 20.01it/s]Train epoch: 69 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.151459\n","449it [00:18, 18.61it/s]Train epoch: 69 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.154378\n","475it [00:19, 17.78it/s]Train epoch: 69 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.184773\n","499it [00:21, 16.70it/s]Train epoch: 69 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.197204\n","505it [00:21, 23.26it/s]\n","epoch loss: 0.13593116472705757\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 357.43it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3896, 0.6316, 0.4743, 0.5417, 0.8749\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4322, 0.7099, 0.5249, 0.6036, 0.9056\n","rec_at_5: 0.5880\n","prec_at_5: 0.5950\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 70\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 70 [batch #0, batch_size 16, seq length 212]\tLoss: 0.182575\n","23it [00:00, 31.52it/s]Train epoch: 70 [batch #25, batch_size 16, seq length 571]\tLoss: 0.098784\n","47it [00:01, 31.01it/s]Train epoch: 70 [batch #50, batch_size 16, seq length 709]\tLoss: 0.106492\n","75it [00:02, 28.69it/s]Train epoch: 70 [batch #75, batch_size 16, seq length 806]\tLoss: 0.120251\n","100it [00:03, 28.35it/s]Train epoch: 70 [batch #100, batch_size 16, seq length 892]\tLoss: 0.107759\n","124it [00:04, 27.16it/s]Train epoch: 70 [batch #125, batch_size 16, seq length 978]\tLoss: 0.109957\n","148it [00:05, 26.16it/s]Train epoch: 70 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.129094\n","175it [00:06, 26.25it/s]Train epoch: 70 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.114718\n","199it [00:07, 24.85it/s]Train epoch: 70 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.121445\n","223it [00:08, 24.77it/s]Train epoch: 70 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.123951\n","250it [00:09, 23.63it/s]Train epoch: 70 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.132713\n","274it [00:10, 23.41it/s]Train epoch: 70 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.140652\n","298it [00:11, 23.59it/s]Train epoch: 70 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.148962\n","325it [00:12, 22.45it/s]Train epoch: 70 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.134997\n","349it [00:13, 22.43it/s]Train epoch: 70 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.143299\n","373it [00:14, 21.58it/s]Train epoch: 70 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.150819\n","400it [00:15, 20.91it/s]Train epoch: 70 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.154194\n","424it [00:17, 20.35it/s]Train epoch: 70 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.148292\n","449it [00:18, 19.07it/s]Train epoch: 70 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.154163\n","474it [00:19, 18.37it/s]Train epoch: 70 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.181440\n","500it [00:21, 15.72it/s]Train epoch: 70 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.193074\n","505it [00:21, 23.33it/s]\n","epoch loss: 0.13404109940522968\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 355.19it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3890, 0.6361, 0.4728, 0.5424, 0.8750\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4309, 0.7089, 0.5235, 0.6023, 0.9053\n","rec_at_5: 0.5884\n","prec_at_5: 0.5959\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 71\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 71 [batch #0, batch_size 16, seq length 212]\tLoss: 0.183114\n","23it [00:00, 31.15it/s]Train epoch: 71 [batch #25, batch_size 16, seq length 571]\tLoss: 0.096540\n","47it [00:01, 31.14it/s]Train epoch: 71 [batch #50, batch_size 16, seq length 709]\tLoss: 0.103206\n","74it [00:02, 29.65it/s]Train epoch: 71 [batch #75, batch_size 16, seq length 806]\tLoss: 0.121595\n","100it [00:03, 28.05it/s]Train epoch: 71 [batch #100, batch_size 16, seq length 892]\tLoss: 0.103696\n","124it [00:04, 27.04it/s]Train epoch: 71 [batch #125, batch_size 16, seq length 978]\tLoss: 0.107428\n","148it [00:05, 25.75it/s]Train epoch: 71 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.126427\n","175it [00:06, 25.73it/s]Train epoch: 71 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.108171\n","199it [00:07, 25.71it/s]Train epoch: 71 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.120718\n","223it [00:08, 24.70it/s]Train epoch: 71 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.122055\n","250it [00:09, 23.91it/s]Train epoch: 71 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.130804\n","274it [00:10, 24.28it/s]Train epoch: 71 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.138640\n","298it [00:11, 23.52it/s]Train epoch: 71 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.147731\n","325it [00:12, 22.45it/s]Train epoch: 71 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.132192\n","349it [00:13, 21.95it/s]Train epoch: 71 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.140955\n","373it [00:14, 21.61it/s]Train epoch: 71 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.147028\n","400it [00:15, 20.48it/s]Train epoch: 71 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.151165\n","423it [00:17, 19.58it/s]Train epoch: 71 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.144621\n","450it [00:18, 19.13it/s]Train epoch: 71 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.151529\n","474it [00:19, 18.11it/s]Train epoch: 71 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.177112\n","500it [00:21, 15.35it/s]Train epoch: 71 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.191208\n","505it [00:21, 23.28it/s]\n","epoch loss: 0.13198141216641607\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 358.82it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3899, 0.6325, 0.4760, 0.5432, 0.8755\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4309, 0.7063, 0.5250, 0.6023, 0.9054\n","rec_at_5: 0.5891\n","prec_at_5: 0.5967\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 72\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 72 [batch #0, batch_size 16, seq length 212]\tLoss: 0.170839\n","23it [00:00, 31.48it/s]Train epoch: 72 [batch #25, batch_size 16, seq length 571]\tLoss: 0.092430\n","47it [00:01, 30.76it/s]Train epoch: 72 [batch #50, batch_size 16, seq length 709]\tLoss: 0.104395\n","73it [00:02, 28.32it/s]Train epoch: 72 [batch #75, batch_size 16, seq length 806]\tLoss: 0.118162\n","98it [00:03, 28.07it/s]Train epoch: 72 [batch #100, batch_size 16, seq length 892]\tLoss: 0.105761\n","125it [00:04, 27.64it/s]Train epoch: 72 [batch #125, batch_size 16, seq length 978]\tLoss: 0.105939\n","149it [00:05, 25.99it/s]Train epoch: 72 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.124622\n","173it [00:06, 26.29it/s]Train epoch: 72 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.109062\n","200it [00:07, 25.17it/s]Train epoch: 72 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.117883\n","224it [00:08, 25.60it/s]Train epoch: 72 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.122542\n","248it [00:09, 23.97it/s]Train epoch: 72 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.132187\n","275it [00:10, 22.93it/s]Train epoch: 72 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.136638\n","299it [00:11, 22.73it/s]Train epoch: 72 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.145094\n","323it [00:12, 22.41it/s]Train epoch: 72 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.129490\n","350it [00:13, 22.28it/s]Train epoch: 72 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.139566\n","374it [00:14, 21.66it/s]Train epoch: 72 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.146167\n","398it [00:15, 20.20it/s]Train epoch: 72 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.149497\n","425it [00:17, 19.14it/s]Train epoch: 72 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.143200\n","448it [00:18, 18.81it/s]Train epoch: 72 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.147713\n","475it [00:20, 17.98it/s]Train epoch: 72 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.176051\n","499it [00:21, 16.38it/s]Train epoch: 72 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.187624\n","505it [00:21, 23.10it/s]\n","epoch loss: 0.13017672020019871\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 352.90it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3887, 0.6312, 0.4736, 0.5412, 0.8756\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4294, 0.7067, 0.5225, 0.6008, 0.9048\n","rec_at_5: 0.5894\n","prec_at_5: 0.5964\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 73\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 73 [batch #0, batch_size 16, seq length 212]\tLoss: 0.160662\n","23it [00:00, 31.59it/s]Train epoch: 73 [batch #25, batch_size 16, seq length 571]\tLoss: 0.094039\n","47it [00:01, 30.01it/s]Train epoch: 73 [batch #50, batch_size 16, seq length 709]\tLoss: 0.100775\n","73it [00:02, 29.05it/s]Train epoch: 73 [batch #75, batch_size 16, seq length 806]\tLoss: 0.116238\n","100it [00:03, 28.47it/s]Train epoch: 73 [batch #100, batch_size 16, seq length 892]\tLoss: 0.102491\n","124it [00:04, 27.09it/s]Train epoch: 73 [batch #125, batch_size 16, seq length 978]\tLoss: 0.104214\n","148it [00:05, 26.37it/s]Train epoch: 73 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.124794\n","175it [00:06, 26.30it/s]Train epoch: 73 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.106436\n","199it [00:07, 25.64it/s]Train epoch: 73 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.117416\n","223it [00:08, 24.10it/s]Train epoch: 73 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.117325\n","250it [00:09, 23.55it/s]Train epoch: 73 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.128495\n","274it [00:10, 22.93it/s]Train epoch: 73 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.134503\n","298it [00:11, 22.58it/s]Train epoch: 73 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.143245\n","325it [00:12, 22.97it/s]Train epoch: 73 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.129791\n","349it [00:13, 22.34it/s]Train epoch: 73 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.135575\n","373it [00:14, 21.85it/s]Train epoch: 73 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.146772\n","400it [00:15, 21.47it/s]Train epoch: 73 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.147738\n","424it [00:17, 20.16it/s]Train epoch: 73 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.142102\n","450it [00:18, 19.35it/s]Train epoch: 73 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.145456\n","474it [00:19, 17.34it/s]Train epoch: 73 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.172962\n","500it [00:21, 15.64it/s]Train epoch: 73 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.185109\n","505it [00:21, 23.28it/s]\n","epoch loss: 0.12845322718419652\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 353.39it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3900, 0.6251, 0.4764, 0.5407, 0.8758\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4300, 0.7042, 0.5248, 0.6014, 0.9049\n","rec_at_5: 0.5885\n","prec_at_5: 0.5950\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 74\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 74 [batch #0, batch_size 16, seq length 212]\tLoss: 0.161677\n","23it [00:00, 31.44it/s]Train epoch: 74 [batch #25, batch_size 16, seq length 571]\tLoss: 0.092726\n","47it [00:01, 30.33it/s]Train epoch: 74 [batch #50, batch_size 16, seq length 709]\tLoss: 0.101043\n","75it [00:02, 28.51it/s]Train epoch: 74 [batch #75, batch_size 16, seq length 806]\tLoss: 0.117908\n","99it [00:03, 27.74it/s]Train epoch: 74 [batch #100, batch_size 16, seq length 892]\tLoss: 0.100163\n","123it [00:04, 28.13it/s]Train epoch: 74 [batch #125, batch_size 16, seq length 978]\tLoss: 0.102331\n","150it [00:05, 26.16it/s]Train epoch: 74 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.122155\n","174it [00:06, 25.79it/s]Train epoch: 74 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.106898\n","198it [00:07, 24.61it/s]Train epoch: 74 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.113421\n","225it [00:08, 24.54it/s]Train epoch: 74 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.118089\n","249it [00:09, 24.31it/s]Train epoch: 74 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.124627\n","273it [00:10, 23.18it/s]Train epoch: 74 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.131686\n","300it [00:11, 23.17it/s]Train epoch: 74 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.140484\n","324it [00:12, 22.38it/s]Train epoch: 74 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.129676\n","348it [00:13, 21.92it/s]Train epoch: 74 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.135318\n","375it [00:14, 21.69it/s]Train epoch: 74 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.143057\n","399it [00:15, 21.02it/s]Train epoch: 74 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.144914\n","423it [00:17, 19.93it/s]Train epoch: 74 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.139567\n","450it [00:18, 18.59it/s]Train epoch: 74 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.144173\n","474it [00:19, 18.13it/s]Train epoch: 74 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.169425\n","500it [00:21, 15.85it/s]Train epoch: 74 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.182774\n","505it [00:21, 23.29it/s]\n","epoch loss: 0.1264962127376901\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 353.83it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3899, 0.6256, 0.4767, 0.5411, 0.8760\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4293, 0.7017, 0.5252, 0.6007, 0.9047\n","rec_at_5: 0.5891\n","prec_at_5: 0.5959\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 75\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 75 [batch #0, batch_size 16, seq length 212]\tLoss: 0.170821\n","23it [00:00, 31.60it/s]Train epoch: 75 [batch #25, batch_size 16, seq length 571]\tLoss: 0.094559\n","47it [00:01, 30.61it/s]Train epoch: 75 [batch #50, batch_size 16, seq length 709]\tLoss: 0.099521\n","73it [00:02, 29.10it/s]Train epoch: 75 [batch #75, batch_size 16, seq length 806]\tLoss: 0.115727\n","98it [00:03, 28.74it/s]Train epoch: 75 [batch #100, batch_size 16, seq length 892]\tLoss: 0.098982\n","125it [00:04, 26.57it/s]Train epoch: 75 [batch #125, batch_size 16, seq length 978]\tLoss: 0.099951\n","149it [00:05, 26.23it/s]Train epoch: 75 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.122097\n","173it [00:06, 25.23it/s]Train epoch: 75 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.107684\n","200it [00:07, 25.42it/s]Train epoch: 75 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.111609\n","224it [00:08, 24.40it/s]Train epoch: 75 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.116921\n","248it [00:09, 24.75it/s]Train epoch: 75 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.124045\n","275it [00:10, 23.23it/s]Train epoch: 75 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.130655\n","299it [00:11, 22.39it/s]Train epoch: 75 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.140037\n","323it [00:12, 22.77it/s]Train epoch: 75 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.123518\n","350it [00:13, 22.44it/s]Train epoch: 75 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.132608\n","374it [00:14, 21.80it/s]Train epoch: 75 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.140631\n","398it [00:15, 20.35it/s]Train epoch: 75 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.142799\n","425it [00:17, 20.23it/s]Train epoch: 75 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.135123\n","450it [00:18, 18.48it/s]Train epoch: 75 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.142739\n","474it [00:19, 17.93it/s]Train epoch: 75 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.168573\n","500it [00:21, 16.15it/s]Train epoch: 75 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.178406\n","505it [00:21, 23.29it/s]\n","epoch loss: 0.12460006643639933\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 353.89it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3909, 0.6256, 0.4785, 0.5423, 0.8762\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4299, 0.7009, 0.5266, 0.6013, 0.9048\n","rec_at_5: 0.5901\n","prec_at_5: 0.5967\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 76\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 76 [batch #0, batch_size 16, seq length 212]\tLoss: 0.172506\n","23it [00:00, 31.79it/s]Train epoch: 76 [batch #25, batch_size 16, seq length 571]\tLoss: 0.090251\n","47it [00:01, 30.42it/s]Train epoch: 76 [batch #50, batch_size 16, seq length 709]\tLoss: 0.099134\n","74it [00:02, 29.55it/s]Train epoch: 76 [batch #75, batch_size 16, seq length 806]\tLoss: 0.109537\n","99it [00:03, 28.33it/s]Train epoch: 76 [batch #100, batch_size 16, seq length 892]\tLoss: 0.099366\n","124it [00:04, 28.02it/s]Train epoch: 76 [batch #125, batch_size 16, seq length 978]\tLoss: 0.100465\n","148it [00:05, 27.14it/s]Train epoch: 76 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.118480\n","175it [00:06, 26.09it/s]Train epoch: 76 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.101880\n","199it [00:07, 25.92it/s]Train epoch: 76 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.113597\n","223it [00:08, 25.51it/s]Train epoch: 76 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.114610\n","250it [00:09, 24.22it/s]Train epoch: 76 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.120250\n","274it [00:10, 23.15it/s]Train epoch: 76 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.127699\n","298it [00:11, 22.92it/s]Train epoch: 76 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.138765\n","325it [00:12, 22.65it/s]Train epoch: 76 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.126554\n","349it [00:13, 22.07it/s]Train epoch: 76 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.130319\n","373it [00:14, 21.84it/s]Train epoch: 76 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.141574\n","400it [00:15, 21.01it/s]Train epoch: 76 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.140639\n","424it [00:17, 20.38it/s]Train epoch: 76 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.133859\n","450it [00:18, 18.58it/s]Train epoch: 76 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.139719\n","475it [00:19, 17.92it/s]Train epoch: 76 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.166168\n","499it [00:21, 15.79it/s]Train epoch: 76 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.177798\n","505it [00:21, 23.35it/s]\n","epoch loss: 0.1228196356482435\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 357.01it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3903, 0.6224, 0.4785, 0.5411, 0.8763\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4298, 0.7002, 0.5268, 0.6012, 0.9043\n","rec_at_5: 0.5900\n","prec_at_5: 0.5964\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 77\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 77 [batch #0, batch_size 16, seq length 212]\tLoss: 0.158643\n","23it [00:00, 31.46it/s]Train epoch: 77 [batch #25, batch_size 16, seq length 571]\tLoss: 0.088777\n","47it [00:01, 31.31it/s]Train epoch: 77 [batch #50, batch_size 16, seq length 709]\tLoss: 0.097542\n","73it [00:02, 29.21it/s]Train epoch: 77 [batch #75, batch_size 16, seq length 806]\tLoss: 0.112128\n","100it [00:03, 27.40it/s]Train epoch: 77 [batch #100, batch_size 16, seq length 892]\tLoss: 0.099088\n","124it [00:04, 26.25it/s]Train epoch: 77 [batch #125, batch_size 16, seq length 978]\tLoss: 0.097013\n","148it [00:05, 27.14it/s]Train epoch: 77 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.115876\n","175it [00:06, 24.74it/s]Train epoch: 77 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.099279\n","199it [00:07, 25.78it/s]Train epoch: 77 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.109232\n","223it [00:08, 24.76it/s]Train epoch: 77 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.111040\n","250it [00:09, 23.90it/s]Train epoch: 77 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.122099\n","274it [00:10, 24.11it/s]Train epoch: 77 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.127817\n","298it [00:11, 22.00it/s]Train epoch: 77 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.134717\n","325it [00:12, 22.79it/s]Train epoch: 77 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.123399\n","349it [00:13, 21.54it/s]Train epoch: 77 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.128671\n","373it [00:14, 21.51it/s]Train epoch: 77 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.137525\n","400it [00:16, 20.43it/s]Train epoch: 77 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.137205\n","424it [00:17, 19.61it/s]Train epoch: 77 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.132043\n","450it [00:18, 18.82it/s]Train epoch: 77 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.137521\n","474it [00:19, 17.47it/s]Train epoch: 77 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.161999\n","500it [00:21, 15.97it/s]Train epoch: 77 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.172646\n","505it [00:21, 23.16it/s]\n","epoch loss: 0.12085982552259275\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 353.69it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3902, 0.6213, 0.4786, 0.5407, 0.8764\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4289, 0.6993, 0.5259, 0.6003, 0.9040\n","rec_at_5: 0.5901\n","prec_at_5: 0.5964\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 78\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 78 [batch #0, batch_size 16, seq length 212]\tLoss: 0.165130\n","22it [00:00, 31.31it/s]Train epoch: 78 [batch #25, batch_size 16, seq length 571]\tLoss: 0.088118\n","50it [00:01, 29.77it/s]Train epoch: 78 [batch #50, batch_size 16, seq length 709]\tLoss: 0.094177\n","75it [00:02, 28.61it/s]Train epoch: 78 [batch #75, batch_size 16, seq length 806]\tLoss: 0.108602\n","99it [00:03, 27.02it/s]Train epoch: 78 [batch #100, batch_size 16, seq length 892]\tLoss: 0.096327\n","123it [00:04, 26.34it/s]Train epoch: 78 [batch #125, batch_size 16, seq length 978]\tLoss: 0.097793\n","150it [00:05, 26.81it/s]Train epoch: 78 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.115389\n","174it [00:06, 25.31it/s]Train epoch: 78 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.099468\n","198it [00:07, 24.76it/s]Train epoch: 78 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.106518\n","225it [00:08, 24.66it/s]Train epoch: 78 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.109382\n","249it [00:09, 23.51it/s]Train epoch: 78 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.117374\n","273it [00:10, 23.34it/s]Train epoch: 78 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.126305\n","300it [00:11, 22.72it/s]Train epoch: 78 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.134396\n","324it [00:12, 21.94it/s]Train epoch: 78 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.120281\n","348it [00:13, 21.61it/s]Train epoch: 78 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.126915\n","375it [00:14, 21.07it/s]Train epoch: 78 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.135531\n","399it [00:16, 21.18it/s]Train epoch: 78 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.135345\n","423it [00:17, 19.60it/s]Train epoch: 78 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.130277\n","450it [00:18, 18.70it/s]Train epoch: 78 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.136035\n","474it [00:19, 17.76it/s]Train epoch: 78 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.159667\n","500it [00:21, 15.40it/s]Train epoch: 78 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.172052\n","505it [00:21, 23.13it/s]\n","epoch loss: 0.1187110804279547\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 356.11it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3905, 0.6195, 0.4798, 0.5408, 0.8765\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4286, 0.6968, 0.5269, 0.6000, 0.9040\n","rec_at_5: 0.5896\n","prec_at_5: 0.5954\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 79\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 79 [batch #0, batch_size 16, seq length 212]\tLoss: 0.158821\n","23it [00:00, 31.33it/s]Train epoch: 79 [batch #25, batch_size 16, seq length 571]\tLoss: 0.087914\n","47it [00:01, 29.80it/s]Train epoch: 79 [batch #50, batch_size 16, seq length 709]\tLoss: 0.090116\n","74it [00:02, 29.02it/s]Train epoch: 79 [batch #75, batch_size 16, seq length 806]\tLoss: 0.108031\n","100it [00:03, 28.25it/s]Train epoch: 79 [batch #100, batch_size 16, seq length 892]\tLoss: 0.096292\n","124it [00:04, 27.53it/s]Train epoch: 79 [batch #125, batch_size 16, seq length 978]\tLoss: 0.094900\n","148it [00:05, 26.35it/s]Train epoch: 79 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.113346\n","175it [00:06, 25.90it/s]Train epoch: 79 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.098714\n","199it [00:07, 25.12it/s]Train epoch: 79 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.104618\n","223it [00:08, 24.55it/s]Train epoch: 79 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.107824\n","250it [00:09, 24.09it/s]Train epoch: 79 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.115884\n","274it [00:10, 23.44it/s]Train epoch: 79 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.124453\n","298it [00:11, 22.38it/s]Train epoch: 79 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.130924\n","325it [00:12, 22.40it/s]Train epoch: 79 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.119013\n","349it [00:13, 21.44it/s]Train epoch: 79 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.125814\n","373it [00:14, 21.41it/s]Train epoch: 79 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.135312\n","400it [00:16, 20.35it/s]Train epoch: 79 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.135344\n","424it [00:17, 19.73it/s]Train epoch: 79 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.128045\n","450it [00:18, 18.87it/s]Train epoch: 79 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.132997\n","475it [00:19, 18.19it/s]Train epoch: 79 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.158299\n","499it [00:21, 15.81it/s]Train epoch: 79 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.167380\n","505it [00:21, 23.16it/s]\n","epoch loss: 0.11717721605950063\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 357.07it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3914, 0.6193, 0.4811, 0.5416, 0.8765\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4295, 0.6964, 0.5284, 0.6009, 0.9036\n","rec_at_5: 0.5890\n","prec_at_5: 0.5955\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 80\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 80 [batch #0, batch_size 16, seq length 212]\tLoss: 0.163236\n","23it [00:00, 31.67it/s]Train epoch: 80 [batch #25, batch_size 16, seq length 571]\tLoss: 0.084761\n","47it [00:01, 30.21it/s]Train epoch: 80 [batch #50, batch_size 16, seq length 709]\tLoss: 0.092909\n","75it [00:02, 28.87it/s]Train epoch: 80 [batch #75, batch_size 16, seq length 806]\tLoss: 0.105406\n","100it [00:03, 28.21it/s]Train epoch: 80 [batch #100, batch_size 16, seq length 892]\tLoss: 0.095375\n","124it [00:04, 27.20it/s]Train epoch: 80 [batch #125, batch_size 16, seq length 978]\tLoss: 0.094587\n","148it [00:05, 26.44it/s]Train epoch: 80 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.110098\n","175it [00:06, 25.71it/s]Train epoch: 80 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.096473\n","199it [00:07, 25.63it/s]Train epoch: 80 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.103239\n","223it [00:08, 23.73it/s]Train epoch: 80 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.108350\n","250it [00:09, 23.98it/s]Train epoch: 80 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.116495\n","274it [00:10, 22.76it/s]Train epoch: 80 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.122847\n","298it [00:11, 22.74it/s]Train epoch: 80 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.130194\n","325it [00:12, 22.75it/s]Train epoch: 80 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.116895\n","349it [00:13, 21.86it/s]Train epoch: 80 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.123489\n","373it [00:14, 21.45it/s]Train epoch: 80 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.131285\n","400it [00:16, 20.53it/s]Train epoch: 80 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.131135\n","424it [00:17, 19.14it/s]Train epoch: 80 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.122853\n","449it [00:18, 18.67it/s]Train epoch: 80 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.129361\n","475it [00:20, 18.00it/s]Train epoch: 80 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.156061\n","499it [00:21, 16.36it/s]Train epoch: 80 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.165957\n","505it [00:21, 23.10it/s]\n","epoch loss: 0.11530706155860779\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 358.18it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3893, 0.6181, 0.4780, 0.5391, 0.8765\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4278, 0.6966, 0.5257, 0.5992, 0.9032\n","rec_at_5: 0.5887\n","prec_at_5: 0.5947\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 81\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 81 [batch #0, batch_size 16, seq length 212]\tLoss: 0.161314\n","23it [00:00, 31.30it/s]Train epoch: 81 [batch #25, batch_size 16, seq length 571]\tLoss: 0.083307\n","47it [00:01, 30.08it/s]Train epoch: 81 [batch #50, batch_size 16, seq length 709]\tLoss: 0.089932\n","74it [00:02, 28.80it/s]Train epoch: 81 [batch #75, batch_size 16, seq length 806]\tLoss: 0.107535\n","99it [00:03, 28.86it/s]Train epoch: 81 [batch #100, batch_size 16, seq length 892]\tLoss: 0.093298\n","123it [00:04, 27.75it/s]Train epoch: 81 [batch #125, batch_size 16, seq length 978]\tLoss: 0.092915\n","150it [00:05, 26.45it/s]Train epoch: 81 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.109550\n","174it [00:06, 25.64it/s]Train epoch: 81 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.096354\n","198it [00:07, 25.30it/s]Train epoch: 81 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.103533\n","225it [00:08, 24.79it/s]Train epoch: 81 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.104568\n","249it [00:09, 24.28it/s]Train epoch: 81 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.112035\n","273it [00:10, 22.97it/s]Train epoch: 81 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.121037\n","300it [00:11, 23.39it/s]Train epoch: 81 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.126625\n","324it [00:12, 21.69it/s]Train epoch: 81 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.114330\n","348it [00:13, 21.41it/s]Train epoch: 81 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.122517\n","375it [00:14, 21.33it/s]Train epoch: 81 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.130737\n","399it [00:16, 20.19it/s]Train epoch: 81 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.131717\n","423it [00:17, 20.04it/s]Train epoch: 81 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.120781\n","450it [00:18, 19.11it/s]Train epoch: 81 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.126632\n","474it [00:19, 17.70it/s]Train epoch: 81 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.152706\n","500it [00:21, 15.45it/s]Train epoch: 81 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.160866\n","505it [00:21, 23.09it/s]\n","epoch loss: 0.11363689444664091\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 351.71it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3891, 0.6148, 0.4794, 0.5387, 0.8766\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4275, 0.6938, 0.5269, 0.5989, 0.9031\n","rec_at_5: 0.5864\n","prec_at_5: 0.5933\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 82\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 82 [batch #0, batch_size 16, seq length 212]\tLoss: 0.156173\n","23it [00:00, 31.04it/s]Train epoch: 82 [batch #25, batch_size 16, seq length 571]\tLoss: 0.083075\n","47it [00:01, 31.00it/s]Train epoch: 82 [batch #50, batch_size 16, seq length 709]\tLoss: 0.088881\n","75it [00:02, 28.79it/s]Train epoch: 82 [batch #75, batch_size 16, seq length 806]\tLoss: 0.106474\n","100it [00:03, 28.07it/s]Train epoch: 82 [batch #100, batch_size 16, seq length 892]\tLoss: 0.091153\n","125it [00:04, 27.50it/s]Train epoch: 82 [batch #125, batch_size 16, seq length 978]\tLoss: 0.091249\n","149it [00:05, 27.34it/s]Train epoch: 82 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.105513\n","173it [00:06, 25.52it/s]Train epoch: 82 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.093587\n","200it [00:07, 25.02it/s]Train epoch: 82 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.099245\n","224it [00:08, 24.83it/s]Train epoch: 82 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.104729\n","248it [00:09, 24.04it/s]Train epoch: 82 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.111658\n","275it [00:10, 23.51it/s]Train epoch: 82 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.116417\n","299it [00:11, 23.74it/s]Train epoch: 82 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.126167\n","323it [00:12, 22.09it/s]Train epoch: 82 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.114014\n","350it [00:13, 22.00it/s]Train epoch: 82 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.119790\n","374it [00:14, 21.68it/s]Train epoch: 82 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.127855\n","398it [00:15, 20.93it/s]Train epoch: 82 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.126316\n","425it [00:17, 19.61it/s]Train epoch: 82 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.120661\n","450it [00:18, 19.15it/s]Train epoch: 82 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.124759\n","474it [00:19, 17.93it/s]Train epoch: 82 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.149677\n","500it [00:21, 15.84it/s]Train epoch: 82 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.161257\n","505it [00:21, 23.14it/s]\n","epoch loss: 0.11158331879368513\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 354.27it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3899, 0.6153, 0.4815, 0.5402, 0.8767\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4280, 0.6920, 0.5287, 0.5995, 0.9029\n","rec_at_5: 0.5853\n","prec_at_5: 0.5928\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 83\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 83 [batch #0, batch_size 16, seq length 212]\tLoss: 0.168355\n","22it [00:00, 29.85it/s]Train epoch: 83 [batch #25, batch_size 16, seq length 571]\tLoss: 0.081360\n","49it [00:01, 29.05it/s]Train epoch: 83 [batch #50, batch_size 16, seq length 709]\tLoss: 0.087519\n","75it [00:02, 29.23it/s]Train epoch: 83 [batch #75, batch_size 16, seq length 806]\tLoss: 0.101965\n","100it [00:03, 27.97it/s]Train epoch: 83 [batch #100, batch_size 16, seq length 892]\tLoss: 0.091717\n","124it [00:04, 27.72it/s]Train epoch: 83 [batch #125, batch_size 16, seq length 978]\tLoss: 0.089622\n","148it [00:05, 26.22it/s]Train epoch: 83 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.106643\n","175it [00:06, 26.34it/s]Train epoch: 83 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.094356\n","199it [00:07, 25.95it/s]Train epoch: 83 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.101838\n","223it [00:08, 25.23it/s]Train epoch: 83 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.102474\n","250it [00:09, 24.69it/s]Train epoch: 83 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.109680\n","274it [00:10, 23.89it/s]Train epoch: 83 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.113488\n","298it [00:11, 23.06it/s]Train epoch: 83 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.122265\n","325it [00:12, 22.51it/s]Train epoch: 83 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.111553\n","349it [00:13, 22.35it/s]Train epoch: 83 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.114990\n","373it [00:14, 22.20it/s]Train epoch: 83 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.127618\n","400it [00:16, 19.89it/s]Train epoch: 83 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.123440\n","424it [00:17, 19.67it/s]Train epoch: 83 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.117832\n","450it [00:18, 19.27it/s]Train epoch: 83 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.123815\n","474it [00:19, 17.79it/s]Train epoch: 83 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.147397\n","500it [00:21, 16.26it/s]Train epoch: 83 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.154641\n","505it [00:21, 23.24it/s]\n","epoch loss: 0.10983613870005206\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 353.91it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3899, 0.6166, 0.4808, 0.5403, 0.8765\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4276, 0.6934, 0.5273, 0.5991, 0.9025\n","rec_at_5: 0.5853\n","prec_at_5: 0.5933\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 84\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 84 [batch #0, batch_size 16, seq length 212]\tLoss: 0.157071\n","24it [00:00, 29.23it/s]Train epoch: 84 [batch #25, batch_size 16, seq length 571]\tLoss: 0.079389\n","47it [00:01, 28.99it/s]Train epoch: 84 [batch #50, batch_size 16, seq length 709]\tLoss: 0.088030\n","74it [00:02, 28.31it/s]Train epoch: 84 [batch #75, batch_size 16, seq length 806]\tLoss: 0.101590\n","100it [00:03, 28.23it/s]Train epoch: 84 [batch #100, batch_size 16, seq length 892]\tLoss: 0.089348\n","124it [00:04, 27.67it/s]Train epoch: 84 [batch #125, batch_size 16, seq length 978]\tLoss: 0.089507\n","148it [00:05, 26.44it/s]Train epoch: 84 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.103977\n","175it [00:06, 25.36it/s]Train epoch: 84 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.092211\n","199it [00:07, 25.76it/s]Train epoch: 84 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.097290\n","223it [00:08, 24.56it/s]Train epoch: 84 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.104092\n","250it [00:09, 24.39it/s]Train epoch: 84 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.107743\n","274it [00:10, 23.37it/s]Train epoch: 84 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.112530\n","298it [00:11, 23.00it/s]Train epoch: 84 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.120283\n","325it [00:12, 22.76it/s]Train epoch: 84 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.110315\n","349it [00:13, 22.01it/s]Train epoch: 84 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.115084\n","373it [00:14, 22.01it/s]Train epoch: 84 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.123899\n","400it [00:16, 20.27it/s]Train epoch: 84 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.121224\n","425it [00:17, 19.40it/s]Train epoch: 84 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.116248\n","449it [00:18, 19.20it/s]Train epoch: 84 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.123260\n","474it [00:20, 17.39it/s]Train epoch: 84 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.144046\n","500it [00:21, 15.94it/s]Train epoch: 84 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.153481\n","505it [00:21, 22.99it/s]\n","epoch loss: 0.10812687675035236\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 358.70it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3892, 0.6142, 0.4810, 0.5395, 0.8765\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4269, 0.6906, 0.5278, 0.5984, 0.9023\n","rec_at_5: 0.5848\n","prec_at_5: 0.5926\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 85\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 85 [batch #0, batch_size 16, seq length 212]\tLoss: 0.143663\n","23it [00:00, 30.47it/s]Train epoch: 85 [batch #25, batch_size 16, seq length 571]\tLoss: 0.077765\n","50it [00:01, 29.47it/s]Train epoch: 85 [batch #50, batch_size 16, seq length 709]\tLoss: 0.085013\n","75it [00:02, 28.24it/s]Train epoch: 85 [batch #75, batch_size 16, seq length 806]\tLoss: 0.098746\n","99it [00:03, 29.01it/s]Train epoch: 85 [batch #100, batch_size 16, seq length 892]\tLoss: 0.087711\n","123it [00:04, 27.32it/s]Train epoch: 85 [batch #125, batch_size 16, seq length 978]\tLoss: 0.086896\n","150it [00:05, 25.55it/s]Train epoch: 85 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.100961\n","174it [00:06, 25.41it/s]Train epoch: 85 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.087732\n","198it [00:07, 25.76it/s]Train epoch: 85 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.096124\n","225it [00:08, 25.01it/s]Train epoch: 85 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.099906\n","249it [00:09, 23.65it/s]Train epoch: 85 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.107454\n","273it [00:10, 23.64it/s]Train epoch: 85 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.112398\n","300it [00:11, 23.47it/s]Train epoch: 85 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.118361\n","324it [00:12, 22.89it/s]Train epoch: 85 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.107040\n","348it [00:13, 22.35it/s]Train epoch: 85 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.113002\n","375it [00:14, 21.34it/s]Train epoch: 85 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.121182\n","399it [00:15, 20.23it/s]Train epoch: 85 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.122382\n","424it [00:17, 19.92it/s]Train epoch: 85 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.114693\n","449it [00:18, 18.81it/s]Train epoch: 85 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.120454\n","475it [00:19, 17.89it/s]Train epoch: 85 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.142026\n","499it [00:21, 15.78it/s]Train epoch: 85 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.150642\n","505it [00:21, 23.21it/s]\n","epoch loss: 0.10600644600863504\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 353.48it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3907, 0.6132, 0.4835, 0.5407, 0.8764\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4279, 0.6894, 0.5301, 0.5994, 0.9022\n","rec_at_5: 0.5856\n","prec_at_5: 0.5925\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 86\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 86 [batch #0, batch_size 16, seq length 212]\tLoss: 0.143007\n","25it [00:00, 29.98it/s]Train epoch: 86 [batch #25, batch_size 16, seq length 571]\tLoss: 0.077492\n","49it [00:01, 30.34it/s]Train epoch: 86 [batch #50, batch_size 16, seq length 709]\tLoss: 0.081943\n","74it [00:02, 28.04it/s]Train epoch: 86 [batch #75, batch_size 16, seq length 806]\tLoss: 0.097157\n","99it [00:03, 27.19it/s]Train epoch: 86 [batch #100, batch_size 16, seq length 892]\tLoss: 0.085481\n","124it [00:04, 27.53it/s]Train epoch: 86 [batch #125, batch_size 16, seq length 978]\tLoss: 0.086661\n","148it [00:05, 26.68it/s]Train epoch: 86 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.100571\n","175it [00:06, 25.73it/s]Train epoch: 86 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.088951\n","199it [00:07, 25.30it/s]Train epoch: 86 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.092516\n","223it [00:08, 24.98it/s]Train epoch: 86 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.096888\n","250it [00:09, 24.45it/s]Train epoch: 86 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.104167\n","274it [00:10, 23.37it/s]Train epoch: 86 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.108967\n","298it [00:11, 23.46it/s]Train epoch: 86 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.114969\n","325it [00:12, 22.99it/s]Train epoch: 86 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.107446\n","349it [00:13, 22.07it/s]Train epoch: 86 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.111368\n","373it [00:14, 21.41it/s]Train epoch: 86 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.121639\n","400it [00:16, 20.34it/s]Train epoch: 86 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.119481\n","424it [00:17, 19.47it/s]Train epoch: 86 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.112413\n","450it [00:18, 19.15it/s]Train epoch: 86 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.118347\n","474it [00:19, 17.90it/s]Train epoch: 86 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.139064\n","500it [00:21, 15.73it/s]Train epoch: 86 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.147361\n","505it [00:21, 23.13it/s]\n","epoch loss: 0.10416890976600128\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 357.47it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3884, 0.6097, 0.4820, 0.5384, 0.8763\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4260, 0.6864, 0.5290, 0.5975, 0.9017\n","rec_at_5: 0.5854\n","prec_at_5: 0.5921\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 87\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 87 [batch #0, batch_size 16, seq length 212]\tLoss: 0.142309\n","23it [00:00, 30.67it/s]Train epoch: 87 [batch #25, batch_size 16, seq length 571]\tLoss: 0.073497\n","47it [00:01, 30.89it/s]Train epoch: 87 [batch #50, batch_size 16, seq length 709]\tLoss: 0.081123\n","74it [00:02, 29.13it/s]Train epoch: 87 [batch #75, batch_size 16, seq length 806]\tLoss: 0.097400\n","100it [00:03, 28.30it/s]Train epoch: 87 [batch #100, batch_size 16, seq length 892]\tLoss: 0.083298\n","124it [00:04, 27.95it/s]Train epoch: 87 [batch #125, batch_size 16, seq length 978]\tLoss: 0.084188\n","148it [00:05, 26.90it/s]Train epoch: 87 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.098211\n","175it [00:06, 26.23it/s]Train epoch: 87 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.086037\n","199it [00:07, 24.42it/s]Train epoch: 87 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.092156\n","223it [00:08, 25.12it/s]Train epoch: 87 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.094799\n","250it [00:09, 24.63it/s]Train epoch: 87 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.102493\n","274it [00:10, 23.30it/s]Train epoch: 87 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.107747\n","298it [00:11, 23.55it/s]Train epoch: 87 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.115985\n","325it [00:12, 22.66it/s]Train epoch: 87 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.104783\n","349it [00:13, 21.86it/s]Train epoch: 87 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.109533\n","373it [00:14, 21.59it/s]Train epoch: 87 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.118937\n","400it [00:15, 21.04it/s]Train epoch: 87 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.118726\n","424it [00:17, 19.82it/s]Train epoch: 87 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.109660\n","450it [00:18, 19.44it/s]Train epoch: 87 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.115793\n","474it [00:19, 17.95it/s]Train epoch: 87 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.135747\n","500it [00:21, 16.08it/s]Train epoch: 87 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.147015\n","505it [00:21, 23.29it/s]\n","epoch loss: 0.10266482899758485\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 353.38it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3883, 0.6117, 0.4818, 0.5390, 0.8762\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4251, 0.6865, 0.5275, 0.5966, 0.9016\n","rec_at_5: 0.5827\n","prec_at_5: 0.5903\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 88\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 88 [batch #0, batch_size 16, seq length 212]\tLoss: 0.155271\n","23it [00:00, 31.72it/s]Train epoch: 88 [batch #25, batch_size 16, seq length 571]\tLoss: 0.074829\n","50it [00:01, 29.51it/s]Train epoch: 88 [batch #50, batch_size 16, seq length 709]\tLoss: 0.082263\n","74it [00:02, 28.16it/s]Train epoch: 88 [batch #75, batch_size 16, seq length 806]\tLoss: 0.094774\n","98it [00:03, 28.90it/s]Train epoch: 88 [batch #100, batch_size 16, seq length 892]\tLoss: 0.083625\n","123it [00:04, 27.99it/s]Train epoch: 88 [batch #125, batch_size 16, seq length 978]\tLoss: 0.082722\n","150it [00:05, 27.32it/s]Train epoch: 88 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.096739\n","174it [00:06, 25.87it/s]Train epoch: 88 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.083915\n","198it [00:07, 25.06it/s]Train epoch: 88 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.090594\n","225it [00:08, 24.21it/s]Train epoch: 88 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.094411\n","249it [00:09, 24.89it/s]Train epoch: 88 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.101611\n","273it [00:10, 23.60it/s]Train epoch: 88 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.105933\n","300it [00:11, 22.83it/s]Train epoch: 88 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.114789\n","324it [00:12, 22.25it/s]Train epoch: 88 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.103698\n","348it [00:13, 21.81it/s]Train epoch: 88 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.105387\n","375it [00:14, 21.37it/s]Train epoch: 88 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.116472\n","399it [00:15, 20.50it/s]Train epoch: 88 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.113973\n","423it [00:17, 20.10it/s]Train epoch: 88 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.106341\n","449it [00:18, 18.70it/s]Train epoch: 88 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.112591\n","474it [00:19, 17.16it/s]Train epoch: 88 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.132631\n","500it [00:21, 15.68it/s]Train epoch: 88 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.141554\n","505it [00:21, 23.17it/s]\n","epoch loss: 0.1005268254123702\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 358.24it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3888, 0.6106, 0.4826, 0.5391, 0.8761\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4254, 0.6852, 0.5287, 0.5969, 0.9013\n","rec_at_5: 0.5826\n","prec_at_5: 0.5900\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 89\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 89 [batch #0, batch_size 16, seq length 212]\tLoss: 0.150938\n","23it [00:00, 30.78it/s]Train epoch: 89 [batch #25, batch_size 16, seq length 571]\tLoss: 0.075400\n","47it [00:01, 30.80it/s]Train epoch: 89 [batch #50, batch_size 16, seq length 709]\tLoss: 0.079725\n","75it [00:02, 29.08it/s]Train epoch: 89 [batch #75, batch_size 16, seq length 806]\tLoss: 0.094622\n","98it [00:03, 28.36it/s]Train epoch: 89 [batch #100, batch_size 16, seq length 892]\tLoss: 0.081002\n","125it [00:04, 27.53it/s]Train epoch: 89 [batch #125, batch_size 16, seq length 978]\tLoss: 0.079313\n","149it [00:05, 26.04it/s]Train epoch: 89 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.096622\n","173it [00:06, 25.70it/s]Train epoch: 89 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.084104\n","200it [00:07, 25.56it/s]Train epoch: 89 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.086436\n","224it [00:08, 24.97it/s]Train epoch: 89 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.093351\n","248it [00:09, 24.59it/s]Train epoch: 89 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.101432\n","275it [00:10, 24.19it/s]Train epoch: 89 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.101741\n","299it [00:11, 23.11it/s]Train epoch: 89 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.111734\n","323it [00:12, 22.41it/s]Train epoch: 89 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.102954\n","350it [00:13, 21.34it/s]Train epoch: 89 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.107565\n","374it [00:14, 20.75it/s]Train epoch: 89 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.115750\n","398it [00:15, 20.20it/s]Train epoch: 89 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.113075\n","425it [00:17, 19.87it/s]Train epoch: 89 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.105896\n","449it [00:18, 18.87it/s]Train epoch: 89 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.111754\n","475it [00:19, 17.67it/s]Train epoch: 89 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.131966\n","499it [00:21, 16.44it/s]Train epoch: 89 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.139712\n","505it [00:21, 23.19it/s]\n","epoch loss: 0.09934751462228228\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 355.59it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3881, 0.6105, 0.4817, 0.5385, 0.8759\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4247, 0.6852, 0.5276, 0.5962, 0.9008\n","rec_at_5: 0.5822\n","prec_at_5: 0.5897\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 90\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 90 [batch #0, batch_size 16, seq length 212]\tLoss: 0.136257\n","25it [00:00, 31.30it/s]Train epoch: 90 [batch #25, batch_size 16, seq length 571]\tLoss: 0.075628\n","49it [00:01, 30.54it/s]Train epoch: 90 [batch #50, batch_size 16, seq length 709]\tLoss: 0.078387\n","74it [00:02, 28.46it/s]Train epoch: 90 [batch #75, batch_size 16, seq length 806]\tLoss: 0.092477\n","99it [00:03, 27.22it/s]Train epoch: 90 [batch #100, batch_size 16, seq length 892]\tLoss: 0.081700\n","123it [00:04, 26.89it/s]Train epoch: 90 [batch #125, batch_size 16, seq length 978]\tLoss: 0.080332\n","150it [00:05, 26.82it/s]Train epoch: 90 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.093875\n","174it [00:06, 26.16it/s]Train epoch: 90 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.082756\n","198it [00:07, 25.28it/s]Train epoch: 90 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.087276\n","225it [00:08, 24.59it/s]Train epoch: 90 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.090006\n","249it [00:09, 24.21it/s]Train epoch: 90 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.098267\n","273it [00:10, 23.74it/s]Train epoch: 90 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.099536\n","300it [00:11, 23.17it/s]Train epoch: 90 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.107564\n","324it [00:12, 22.39it/s]Train epoch: 90 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.100957\n","348it [00:13, 22.20it/s]Train epoch: 90 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.104740\n","375it [00:14, 21.37it/s]Train epoch: 90 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.115220\n","399it [00:16, 20.63it/s]Train epoch: 90 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.109792\n","423it [00:17, 20.17it/s]Train epoch: 90 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.100588\n","450it [00:18, 18.69it/s]Train epoch: 90 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.106512\n","474it [00:20, 17.69it/s]Train epoch: 90 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.128623\n","500it [00:21, 15.98it/s]Train epoch: 90 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.137301\n","505it [00:21, 23.06it/s]\n","epoch loss: 0.09731611641732478\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 353.96it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3877, 0.6084, 0.4828, 0.5384, 0.8757\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4235, 0.6815, 0.5281, 0.5950, 0.9005\n","rec_at_5: 0.5813\n","prec_at_5: 0.5882\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 91\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 91 [batch #0, batch_size 16, seq length 212]\tLoss: 0.129249\n","23it [00:00, 30.81it/s]Train epoch: 91 [batch #25, batch_size 16, seq length 571]\tLoss: 0.071801\n","47it [00:01, 30.60it/s]Train epoch: 91 [batch #50, batch_size 16, seq length 709]\tLoss: 0.077348\n","75it [00:02, 30.09it/s]Train epoch: 91 [batch #75, batch_size 16, seq length 806]\tLoss: 0.089494\n","98it [00:03, 28.27it/s]Train epoch: 91 [batch #100, batch_size 16, seq length 892]\tLoss: 0.078021\n","125it [00:04, 26.23it/s]Train epoch: 91 [batch #125, batch_size 16, seq length 978]\tLoss: 0.079816\n","149it [00:05, 26.95it/s]Train epoch: 91 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.089750\n","173it [00:06, 25.08it/s]Train epoch: 91 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.080810\n","200it [00:07, 25.19it/s]Train epoch: 91 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.086434\n","224it [00:08, 24.52it/s]Train epoch: 91 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.089307\n","248it [00:09, 23.48it/s]Train epoch: 91 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.096058\n","275it [00:10, 23.34it/s]Train epoch: 91 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.100631\n","299it [00:11, 22.20it/s]Train epoch: 91 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.107818\n","323it [00:12, 20.86it/s]Train epoch: 91 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.098888\n","350it [00:13, 21.81it/s]Train epoch: 91 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.104826\n","374it [00:14, 21.43it/s]Train epoch: 91 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.109789\n","398it [00:16, 21.00it/s]Train epoch: 91 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.106481\n","425it [00:17, 19.77it/s]Train epoch: 91 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.100978\n","449it [00:18, 18.27it/s]Train epoch: 91 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.106907\n","475it [00:20, 17.72it/s]Train epoch: 91 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.124575\n","499it [00:21, 16.43it/s]Train epoch: 91 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.131419\n","505it [00:21, 23.04it/s]\n","epoch loss: 0.09540471865030209\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 353.76it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3881, 0.6108, 0.4829, 0.5393, 0.8756\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4251, 0.6837, 0.5291, 0.5966, 0.9002\n","rec_at_5: 0.5822\n","prec_at_5: 0.5888\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 92\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 92 [batch #0, batch_size 16, seq length 212]\tLoss: 0.146671\n","22it [00:00, 31.30it/s]Train epoch: 92 [batch #25, batch_size 16, seq length 571]\tLoss: 0.070407\n","50it [00:01, 29.38it/s]Train epoch: 92 [batch #50, batch_size 16, seq length 709]\tLoss: 0.076642\n","74it [00:02, 28.74it/s]Train epoch: 92 [batch #75, batch_size 16, seq length 806]\tLoss: 0.089993\n","98it [00:03, 28.42it/s]Train epoch: 92 [batch #100, batch_size 16, seq length 892]\tLoss: 0.077201\n","125it [00:04, 27.18it/s]Train epoch: 92 [batch #125, batch_size 16, seq length 978]\tLoss: 0.076669\n","149it [00:05, 26.07it/s]Train epoch: 92 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.091184\n","173it [00:06, 25.90it/s]Train epoch: 92 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.080710\n","200it [00:07, 25.23it/s]Train epoch: 92 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.083809\n","224it [00:08, 24.88it/s]Train epoch: 92 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.089069\n","248it [00:09, 24.18it/s]Train epoch: 92 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.094145\n","275it [00:10, 22.86it/s]Train epoch: 92 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.096739\n","299it [00:11, 22.76it/s]Train epoch: 92 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.104977\n","323it [00:12, 20.02it/s]Train epoch: 92 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.095881\n","350it [00:13, 22.25it/s]Train epoch: 92 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.098755\n","374it [00:14, 20.06it/s]Train epoch: 92 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.108051\n","400it [00:16, 19.66it/s]Train epoch: 92 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.106966\n","425it [00:17, 19.56it/s]Train epoch: 92 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.099728\n","450it [00:18, 18.71it/s]Train epoch: 92 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.104310\n","474it [00:20, 16.95it/s]Train epoch: 92 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.121474\n","500it [00:21, 15.61it/s]Train epoch: 92 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.128990\n","505it [00:22, 22.77it/s]\n","epoch loss: 0.09415435614709806\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 349.64it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3865, 0.6099, 0.4804, 0.5374, 0.8754\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4234, 0.6827, 0.5271, 0.5949, 0.8997\n","rec_at_5: 0.5816\n","prec_at_5: 0.5888\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 93\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 93 [batch #0, batch_size 16, seq length 212]\tLoss: 0.149827\n","22it [00:00, 31.12it/s]Train epoch: 93 [batch #25, batch_size 16, seq length 571]\tLoss: 0.068504\n","50it [00:01, 29.51it/s]Train epoch: 93 [batch #50, batch_size 16, seq length 709]\tLoss: 0.074486\n","75it [00:02, 27.40it/s]Train epoch: 93 [batch #75, batch_size 16, seq length 806]\tLoss: 0.087860\n","99it [00:03, 27.02it/s]Train epoch: 93 [batch #100, batch_size 16, seq length 892]\tLoss: 0.075987\n","123it [00:04, 27.30it/s]Train epoch: 93 [batch #125, batch_size 16, seq length 978]\tLoss: 0.077528\n","150it [00:05, 26.25it/s]Train epoch: 93 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.088604\n","174it [00:06, 25.79it/s]Train epoch: 93 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.078130\n","198it [00:07, 25.08it/s]Train epoch: 93 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.081440\n","225it [00:08, 24.28it/s]Train epoch: 93 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.084975\n","249it [00:09, 24.39it/s]Train epoch: 93 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.093695\n","273it [00:10, 23.82it/s]Train epoch: 93 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.096578\n","300it [00:11, 22.96it/s]Train epoch: 93 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.103361\n","324it [00:12, 22.89it/s]Train epoch: 93 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.095470\n","348it [00:13, 21.16it/s]Train epoch: 93 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.099339\n","375it [00:14, 21.30it/s]Train epoch: 93 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.107858\n","399it [00:16, 20.62it/s]Train epoch: 93 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.104003\n","424it [00:17, 19.80it/s]Train epoch: 93 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.095946\n","449it [00:18, 18.88it/s]Train epoch: 93 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.104648\n","475it [00:20, 18.04it/s]Train epoch: 93 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.122253\n","499it [00:21, 16.31it/s]Train epoch: 93 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.126328\n","505it [00:21, 22.99it/s]\n","epoch loss: 0.09217556396702138\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 355.71it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3874, 0.6068, 0.4827, 0.5377, 0.8752\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4231, 0.6798, 0.5284, 0.5946, 0.8996\n","rec_at_5: 0.5782\n","prec_at_5: 0.5859\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 94\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 94 [batch #0, batch_size 16, seq length 212]\tLoss: 0.137499\n","22it [00:00, 30.48it/s]Train epoch: 94 [batch #25, batch_size 16, seq length 571]\tLoss: 0.070387\n","50it [00:01, 30.01it/s]Train epoch: 94 [batch #50, batch_size 16, seq length 709]\tLoss: 0.073106\n","74it [00:02, 28.38it/s]Train epoch: 94 [batch #75, batch_size 16, seq length 806]\tLoss: 0.086497\n","98it [00:03, 27.98it/s]Train epoch: 94 [batch #100, batch_size 16, seq length 892]\tLoss: 0.073363\n","125it [00:04, 27.02it/s]Train epoch: 94 [batch #125, batch_size 16, seq length 978]\tLoss: 0.075279\n","149it [00:05, 25.93it/s]Train epoch: 94 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.083845\n","173it [00:06, 26.35it/s]Train epoch: 94 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.076870\n","200it [00:07, 24.79it/s]Train epoch: 94 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.079013\n","224it [00:08, 25.05it/s]Train epoch: 94 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.083258\n","248it [00:09, 24.03it/s]Train epoch: 94 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.091165\n","275it [00:10, 23.75it/s]Train epoch: 94 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.095110\n","299it [00:11, 23.07it/s]Train epoch: 94 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.101243\n","323it [00:12, 22.05it/s]Train epoch: 94 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.095093\n","350it [00:13, 21.50it/s]Train epoch: 94 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.097560\n","374it [00:14, 21.39it/s]Train epoch: 94 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.104977\n","398it [00:16, 20.91it/s]Train epoch: 94 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.103964\n","425it [00:17, 19.80it/s]Train epoch: 94 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.099317\n","449it [00:18, 17.89it/s]Train epoch: 94 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.101466\n","474it [00:20, 17.66it/s]Train epoch: 94 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.118702\n","500it [00:21, 15.53it/s]Train epoch: 94 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.126435\n","505it [00:21, 23.01it/s]\n","epoch loss: 0.09045587424742113\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 353.04it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3890, 0.6080, 0.4851, 0.5397, 0.8750\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4246, 0.6793, 0.5311, 0.5961, 0.8993\n","rec_at_5: 0.5780\n","prec_at_5: 0.5855\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 95\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 95 [batch #0, batch_size 16, seq length 212]\tLoss: 0.140824\n","23it [00:00, 31.18it/s]Train epoch: 95 [batch #25, batch_size 16, seq length 571]\tLoss: 0.067261\n","47it [00:01, 29.91it/s]Train epoch: 95 [batch #50, batch_size 16, seq length 709]\tLoss: 0.070927\n","72it [00:02, 29.35it/s]Train epoch: 95 [batch #75, batch_size 16, seq length 806]\tLoss: 0.086220\n","98it [00:03, 28.32it/s]Train epoch: 95 [batch #100, batch_size 16, seq length 892]\tLoss: 0.072265\n","125it [00:04, 27.12it/s]Train epoch: 95 [batch #125, batch_size 16, seq length 978]\tLoss: 0.071950\n","149it [00:05, 26.33it/s]Train epoch: 95 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.086801\n","173it [00:06, 25.87it/s]Train epoch: 95 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.075080\n","200it [00:07, 25.19it/s]Train epoch: 95 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.079961\n","224it [00:08, 24.31it/s]Train epoch: 95 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.080963\n","248it [00:09, 23.53it/s]Train epoch: 95 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.090010\n","275it [00:10, 23.54it/s]Train epoch: 95 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.091943\n","299it [00:11, 23.20it/s]Train epoch: 95 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.098142\n","323it [00:12, 22.02it/s]Train epoch: 95 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.090330\n","350it [00:13, 21.91it/s]Train epoch: 95 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.094644\n","374it [00:14, 20.82it/s]Train epoch: 95 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.102528\n","398it [00:15, 20.59it/s]Train epoch: 95 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.100159\n","424it [00:17, 19.96it/s]Train epoch: 95 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.094214\n","449it [00:18, 18.19it/s]Train epoch: 95 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.097872\n","474it [00:20, 18.11it/s]Train epoch: 95 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.117544\n","500it [00:21, 15.99it/s]Train epoch: 95 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.125097\n","505it [00:21, 23.04it/s]\n","epoch loss: 0.0884511760777176\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 352.47it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3868, 0.6046, 0.4845, 0.5379, 0.8746\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4226, 0.6768, 0.5295, 0.5941, 0.8988\n","rec_at_5: 0.5765\n","prec_at_5: 0.5839\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 96\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 96 [batch #0, batch_size 16, seq length 212]\tLoss: 0.130741\n","25it [00:00, 30.72it/s]Train epoch: 96 [batch #25, batch_size 16, seq length 571]\tLoss: 0.067271\n","49it [00:01, 30.71it/s]Train epoch: 96 [batch #50, batch_size 16, seq length 709]\tLoss: 0.068146\n","74it [00:02, 29.10it/s]Train epoch: 96 [batch #75, batch_size 16, seq length 806]\tLoss: 0.083817\n","99it [00:03, 28.46it/s]Train epoch: 96 [batch #100, batch_size 16, seq length 892]\tLoss: 0.072978\n","123it [00:04, 27.33it/s]Train epoch: 96 [batch #125, batch_size 16, seq length 978]\tLoss: 0.070813\n","150it [00:05, 26.26it/s]Train epoch: 96 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.083534\n","174it [00:06, 26.14it/s]Train epoch: 96 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.072455\n","198it [00:07, 25.58it/s]Train epoch: 96 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.077178\n","225it [00:08, 24.33it/s]Train epoch: 96 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.079104\n","249it [00:09, 24.47it/s]Train epoch: 96 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.086620\n","273it [00:10, 23.58it/s]Train epoch: 96 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.090883\n","300it [00:11, 23.02it/s]Train epoch: 96 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.097341\n","324it [00:12, 22.66it/s]Train epoch: 96 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.090890\n","348it [00:13, 21.36it/s]Train epoch: 96 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.092314\n","375it [00:14, 20.64it/s]Train epoch: 96 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.103926\n","399it [00:16, 20.87it/s]Train epoch: 96 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.098674\n","423it [00:17, 19.99it/s]Train epoch: 96 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.091428\n","449it [00:18, 18.33it/s]Train epoch: 96 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.097706\n","475it [00:20, 18.15it/s]Train epoch: 96 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.114558\n","499it [00:21, 15.73it/s]Train epoch: 96 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.120298\n","505it [00:21, 23.04it/s]\n","epoch loss: 0.08727454816977871\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 354.23it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3854, 0.6052, 0.4809, 0.5359, 0.8745\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4213, 0.6774, 0.5270, 0.5928, 0.8983\n","rec_at_5: 0.5774\n","prec_at_5: 0.5842\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 97\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 97 [batch #0, batch_size 16, seq length 212]\tLoss: 0.134143\n","23it [00:00, 30.08it/s]Train epoch: 97 [batch #25, batch_size 16, seq length 571]\tLoss: 0.062130\n","47it [00:01, 30.31it/s]Train epoch: 97 [batch #50, batch_size 16, seq length 709]\tLoss: 0.067144\n","74it [00:02, 29.31it/s]Train epoch: 97 [batch #75, batch_size 16, seq length 806]\tLoss: 0.082186\n","100it [00:03, 28.67it/s]Train epoch: 97 [batch #100, batch_size 16, seq length 892]\tLoss: 0.072277\n","124it [00:04, 27.69it/s]Train epoch: 97 [batch #125, batch_size 16, seq length 978]\tLoss: 0.069624\n","148it [00:05, 27.18it/s]Train epoch: 97 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.081497\n","175it [00:06, 25.60it/s]Train epoch: 97 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.069282\n","199it [00:07, 25.69it/s]Train epoch: 97 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.076475\n","223it [00:08, 24.18it/s]Train epoch: 97 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.078758\n","250it [00:09, 23.34it/s]Train epoch: 97 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.086815\n","274it [00:10, 23.50it/s]Train epoch: 97 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.089279\n","298it [00:11, 22.61it/s]Train epoch: 97 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.096117\n","325it [00:12, 22.00it/s]Train epoch: 97 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.089142\n","349it [00:13, 21.67it/s]Train epoch: 97 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.088709\n","373it [00:14, 20.64it/s]Train epoch: 97 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.099545\n","400it [00:16, 20.29it/s]Train epoch: 97 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.095821\n","423it [00:17, 19.52it/s]Train epoch: 97 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.089142\n","449it [00:18, 18.89it/s]Train epoch: 97 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.093862\n","474it [00:20, 17.46it/s]Train epoch: 97 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.113336\n","500it [00:21, 15.96it/s]Train epoch: 97 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.116682\n","505it [00:21, 23.00it/s]\n","epoch loss: 0.08499957823959908\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 351.00it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3854, 0.6004, 0.4830, 0.5353, 0.8742\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4204, 0.6738, 0.5278, 0.5920, 0.8978\n","rec_at_5: 0.5759\n","prec_at_5: 0.5830\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 98\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 98 [batch #0, batch_size 16, seq length 212]\tLoss: 0.125360\n","23it [00:00, 30.96it/s]Train epoch: 98 [batch #25, batch_size 16, seq length 571]\tLoss: 0.064389\n","47it [00:01, 30.68it/s]Train epoch: 98 [batch #50, batch_size 16, seq length 709]\tLoss: 0.070865\n","73it [00:02, 29.30it/s]Train epoch: 98 [batch #75, batch_size 16, seq length 806]\tLoss: 0.080957\n","100it [00:03, 27.93it/s]Train epoch: 98 [batch #100, batch_size 16, seq length 892]\tLoss: 0.070869\n","124it [00:04, 27.21it/s]Train epoch: 98 [batch #125, batch_size 16, seq length 978]\tLoss: 0.068652\n","148it [00:05, 26.79it/s]Train epoch: 98 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.078588\n","175it [00:06, 25.52it/s]Train epoch: 98 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.072890\n","199it [00:07, 24.99it/s]Train epoch: 98 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.076156\n","223it [00:08, 25.05it/s]Train epoch: 98 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.078554\n","250it [00:09, 24.43it/s]Train epoch: 98 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.084421\n","274it [00:10, 23.84it/s]Train epoch: 98 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.085573\n","298it [00:11, 22.96it/s]Train epoch: 98 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.092901\n","325it [00:12, 22.53it/s]Train epoch: 98 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.086935\n","349it [00:13, 21.77it/s]Train epoch: 98 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.089334\n","373it [00:14, 20.81it/s]Train epoch: 98 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.099965\n","400it [00:16, 20.89it/s]Train epoch: 98 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.092878\n","424it [00:17, 19.84it/s]Train epoch: 98 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.086479\n","450it [00:18, 18.97it/s]Train epoch: 98 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.092081\n","474it [00:19, 16.87it/s]Train epoch: 98 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.109415\n","500it [00:21, 15.77it/s]Train epoch: 98 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.116124\n","505it [00:21, 23.00it/s]\n","epoch loss: 0.08372955337941351\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 346.20it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3861, 0.6028, 0.4828, 0.5362, 0.8740\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4212, 0.6756, 0.5281, 0.5928, 0.8975\n","rec_at_5: 0.5756\n","prec_at_5: 0.5827\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 99\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 99 [batch #0, batch_size 16, seq length 212]\tLoss: 0.136296\n","23it [00:00, 30.84it/s]Train epoch: 99 [batch #25, batch_size 16, seq length 571]\tLoss: 0.062157\n","47it [00:01, 30.54it/s]Train epoch: 99 [batch #50, batch_size 16, seq length 709]\tLoss: 0.064446\n","74it [00:02, 27.85it/s]Train epoch: 99 [batch #75, batch_size 16, seq length 806]\tLoss: 0.079336\n","100it [00:03, 27.27it/s]Train epoch: 99 [batch #100, batch_size 16, seq length 892]\tLoss: 0.068823\n","124it [00:04, 26.68it/s]Train epoch: 99 [batch #125, batch_size 16, seq length 978]\tLoss: 0.068483\n","148it [00:05, 26.03it/s]Train epoch: 99 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.078927\n","175it [00:06, 25.07it/s]Train epoch: 99 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.067032\n","199it [00:07, 25.42it/s]Train epoch: 99 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.074945\n","223it [00:08, 24.27it/s]Train epoch: 99 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.076201\n","250it [00:09, 23.32it/s]Train epoch: 99 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.083924\n","274it [00:10, 23.00it/s]Train epoch: 99 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.085670\n","298it [00:11, 22.66it/s]Train epoch: 99 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.092094\n","325it [00:12, 22.30it/s]Train epoch: 99 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.085712\n","349it [00:13, 21.67it/s]Train epoch: 99 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.085946\n","373it [00:14, 20.96it/s]Train epoch: 99 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.096312\n","400it [00:16, 20.58it/s]Train epoch: 99 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.092294\n","424it [00:17, 19.07it/s]Train epoch: 99 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.087679\n","450it [00:18, 18.92it/s]Train epoch: 99 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.091641\n","474it [00:20, 17.98it/s]Train epoch: 99 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.108638\n","500it [00:21, 15.40it/s]Train epoch: 99 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.115028\n","505it [00:22, 22.94it/s]\n","epoch loss: 0.08202249450553761\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 353.04it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3862, 0.6048, 0.4829, 0.5370, 0.8738\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4216, 0.6758, 0.5285, 0.5931, 0.8973\n","rec_at_5: 0.5756\n","prec_at_5: 0.5828\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 100\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 100 [batch #0, batch_size 16, seq length 212]\tLoss: 0.123130\n","24it [00:00, 30.22it/s]Train epoch: 100 [batch #25, batch_size 16, seq length 571]\tLoss: 0.061112\n","48it [00:01, 30.27it/s]Train epoch: 100 [batch #50, batch_size 16, seq length 709]\tLoss: 0.064812\n","74it [00:02, 28.95it/s]Train epoch: 100 [batch #75, batch_size 16, seq length 806]\tLoss: 0.078313\n","99it [00:03, 28.43it/s]Train epoch: 100 [batch #100, batch_size 16, seq length 892]\tLoss: 0.068920\n","123it [00:04, 26.51it/s]Train epoch: 100 [batch #125, batch_size 16, seq length 978]\tLoss: 0.066827\n","150it [00:05, 27.05it/s]Train epoch: 100 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.076795\n","174it [00:06, 26.64it/s]Train epoch: 100 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.069910\n","198it [00:07, 25.11it/s]Train epoch: 100 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.072527\n","225it [00:08, 25.22it/s]Train epoch: 100 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.077622\n","249it [00:09, 24.47it/s]Train epoch: 100 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.082325\n","273it [00:10, 23.93it/s]Train epoch: 100 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.084820\n","300it [00:11, 22.79it/s]Train epoch: 100 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.088220\n","324it [00:12, 22.12it/s]Train epoch: 100 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.083662\n","348it [00:13, 21.69it/s]Train epoch: 100 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.085531\n","375it [00:14, 22.07it/s]Train epoch: 100 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.096082\n","399it [00:16, 20.03it/s]Train epoch: 100 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.089920\n","425it [00:17, 19.56it/s]Train epoch: 100 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.082556\n","449it [00:18, 19.24it/s]Train epoch: 100 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.090951\n","475it [00:19, 18.08it/s]Train epoch: 100 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.106325\n","499it [00:21, 16.13it/s]Train epoch: 100 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.109973\n","505it [00:21, 23.15it/s]\n","epoch loss: 0.08050297670951574\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 353.39it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3863, 0.6055, 0.4829, 0.5373, 0.8735\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4207, 0.6755, 0.5272, 0.5922, 0.8968\n","rec_at_5: 0.5739\n","prec_at_5: 0.5814\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 101\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 101 [batch #0, batch_size 16, seq length 212]\tLoss: 0.131715\n","23it [00:00, 30.76it/s]Train epoch: 101 [batch #25, batch_size 16, seq length 571]\tLoss: 0.057694\n","47it [00:01, 30.76it/s]Train epoch: 101 [batch #50, batch_size 16, seq length 709]\tLoss: 0.064911\n","73it [00:02, 29.00it/s]Train epoch: 101 [batch #75, batch_size 16, seq length 806]\tLoss: 0.075985\n","100it [00:03, 27.18it/s]Train epoch: 101 [batch #100, batch_size 16, seq length 892]\tLoss: 0.066145\n","124it [00:04, 27.38it/s]Train epoch: 101 [batch #125, batch_size 16, seq length 978]\tLoss: 0.069741\n","148it [00:05, 26.81it/s]Train epoch: 101 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.074179\n","175it [00:06, 25.96it/s]Train epoch: 101 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.067461\n","199it [00:07, 24.76it/s]Train epoch: 101 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.074864\n","223it [00:08, 24.95it/s]Train epoch: 101 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.073573\n","250it [00:09, 24.42it/s]Train epoch: 101 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.079832\n","274it [00:10, 23.57it/s]Train epoch: 101 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.081936\n","298it [00:11, 23.48it/s]Train epoch: 101 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.087170\n","325it [00:12, 22.89it/s]Train epoch: 101 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.083421\n","349it [00:13, 22.53it/s]Train epoch: 101 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.086044\n","373it [00:14, 20.35it/s]Train epoch: 101 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.091025\n","400it [00:16, 20.06it/s]Train epoch: 101 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.087641\n","424it [00:17, 19.75it/s]Train epoch: 101 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.081824\n","448it [00:18, 18.97it/s]Train epoch: 101 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.085333\n","475it [00:20, 17.95it/s]Train epoch: 101 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.103918\n","499it [00:21, 16.11it/s]Train epoch: 101 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.108392\n","505it [00:21, 23.06it/s]\n","epoch loss: 0.0791572745723447\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 347.97it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3861, 0.6004, 0.4852, 0.5367, 0.8733\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4205, 0.6715, 0.5294, 0.5920, 0.8965\n","rec_at_5: 0.5725\n","prec_at_5: 0.5809\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 102\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 102 [batch #0, batch_size 16, seq length 212]\tLoss: 0.119281\n","23it [00:00, 29.55it/s]Train epoch: 102 [batch #25, batch_size 16, seq length 571]\tLoss: 0.061552\n","47it [00:01, 30.44it/s]Train epoch: 102 [batch #50, batch_size 16, seq length 709]\tLoss: 0.065918\n","72it [00:02, 28.08it/s]Train epoch: 102 [batch #75, batch_size 16, seq length 806]\tLoss: 0.076371\n","99it [00:03, 28.03it/s]Train epoch: 102 [batch #100, batch_size 16, seq length 892]\tLoss: 0.066443\n","123it [00:04, 27.90it/s]Train epoch: 102 [batch #125, batch_size 16, seq length 978]\tLoss: 0.064633\n","150it [00:05, 26.52it/s]Train epoch: 102 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.076731\n","174it [00:06, 26.33it/s]Train epoch: 102 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.065417\n","198it [00:07, 25.98it/s]Train epoch: 102 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.068866\n","225it [00:08, 23.20it/s]Train epoch: 102 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.071475\n","249it [00:09, 24.14it/s]Train epoch: 102 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.080339\n","273it [00:10, 22.92it/s]Train epoch: 102 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.079849\n","300it [00:11, 22.90it/s]Train epoch: 102 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.086435\n","324it [00:12, 22.82it/s]Train epoch: 102 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.080764\n","348it [00:13, 20.68it/s]Train epoch: 102 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.082651\n","375it [00:14, 21.30it/s]Train epoch: 102 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.089437\n","399it [00:16, 20.28it/s]Train epoch: 102 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.088253\n","425it [00:17, 19.82it/s]Train epoch: 102 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.081583\n","449it [00:18, 18.51it/s]Train epoch: 102 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.086664\n","475it [00:20, 18.17it/s]Train epoch: 102 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.099010\n","499it [00:21, 16.12it/s]Train epoch: 102 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.104498\n","505it [00:22, 22.93it/s]\n","epoch loss: 0.07768485461647558\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 353.67it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3862, 0.6027, 0.4844, 0.5371, 0.8731\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4212, 0.6730, 0.5296, 0.5927, 0.8961\n","rec_at_5: 0.5716\n","prec_at_5: 0.5811\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 103\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 103 [batch #0, batch_size 16, seq length 212]\tLoss: 0.119445\n","23it [00:00, 31.70it/s]Train epoch: 103 [batch #25, batch_size 16, seq length 571]\tLoss: 0.055649\n","50it [00:01, 29.56it/s]Train epoch: 103 [batch #50, batch_size 16, seq length 709]\tLoss: 0.060321\n","73it [00:02, 28.63it/s]Train epoch: 103 [batch #75, batch_size 16, seq length 806]\tLoss: 0.073455\n","99it [00:03, 27.61it/s]Train epoch: 103 [batch #100, batch_size 16, seq length 892]\tLoss: 0.064233\n","123it [00:04, 26.28it/s]Train epoch: 103 [batch #125, batch_size 16, seq length 978]\tLoss: 0.063681\n","150it [00:05, 25.61it/s]Train epoch: 103 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.073142\n","174it [00:06, 25.71it/s]Train epoch: 103 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.064930\n","198it [00:07, 25.40it/s]Train epoch: 103 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.069195\n","225it [00:08, 23.93it/s]Train epoch: 103 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.073089\n","249it [00:09, 24.13it/s]Train epoch: 103 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.074216\n","273it [00:10, 22.96it/s]Train epoch: 103 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.078811\n","300it [00:11, 22.69it/s]Train epoch: 103 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.082884\n","324it [00:12, 22.11it/s]Train epoch: 103 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.078729\n","348it [00:13, 21.27it/s]Train epoch: 103 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.079967\n","375it [00:14, 21.52it/s]Train epoch: 103 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.090237\n","399it [00:16, 20.22it/s]Train epoch: 103 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.085074\n","423it [00:17, 19.65it/s]Train epoch: 103 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.077203\n","449it [00:18, 18.65it/s]Train epoch: 103 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.083259\n","474it [00:20, 17.43it/s]Train epoch: 103 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.099438\n","500it [00:21, 15.35it/s]Train epoch: 103 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.101469\n","505it [00:22, 22.94it/s]\n","epoch loss: 0.07569412501104693\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 353.64it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3852, 0.5992, 0.4844, 0.5357, 0.8729\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4195, 0.6695, 0.5290, 0.5910, 0.8959\n","rec_at_5: 0.5703\n","prec_at_5: 0.5789\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 104\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 104 [batch #0, batch_size 16, seq length 212]\tLoss: 0.116710\n","23it [00:00, 30.65it/s]Train epoch: 104 [batch #25, batch_size 16, seq length 571]\tLoss: 0.055973\n","47it [00:01, 29.93it/s]Train epoch: 104 [batch #50, batch_size 16, seq length 709]\tLoss: 0.060716\n","74it [00:02, 28.93it/s]Train epoch: 104 [batch #75, batch_size 16, seq length 806]\tLoss: 0.072446\n","100it [00:03, 28.21it/s]Train epoch: 104 [batch #100, batch_size 16, seq length 892]\tLoss: 0.064162\n","124it [00:04, 27.27it/s]Train epoch: 104 [batch #125, batch_size 16, seq length 978]\tLoss: 0.064615\n","148it [00:05, 26.17it/s]Train epoch: 104 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.073082\n","175it [00:06, 26.23it/s]Train epoch: 104 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.064052\n","199it [00:07, 25.13it/s]Train epoch: 104 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.067224\n","223it [00:08, 24.09it/s]Train epoch: 104 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.070770\n","250it [00:09, 23.70it/s]Train epoch: 104 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.076658\n","274it [00:10, 23.66it/s]Train epoch: 104 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.075649\n","298it [00:11, 22.95it/s]Train epoch: 104 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.082461\n","325it [00:12, 22.07it/s]Train epoch: 104 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.076949\n","349it [00:13, 21.64it/s]Train epoch: 104 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.078642\n","373it [00:14, 20.85it/s]Train epoch: 104 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.083566\n","400it [00:16, 20.05it/s]Train epoch: 104 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.080960\n","424it [00:17, 19.03it/s]Train epoch: 104 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.078290\n","450it [00:18, 17.87it/s]Train epoch: 104 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.082921\n","475it [00:20, 17.23it/s]Train epoch: 104 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.096499\n","499it [00:21, 15.53it/s]Train epoch: 104 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.101932\n","505it [00:22, 22.88it/s]\n","epoch loss: 0.07436138926774706\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 351.47it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3847, 0.6003, 0.4822, 0.5348, 0.8728\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4191, 0.6712, 0.5274, 0.5907, 0.8955\n","rec_at_5: 0.5722\n","prec_at_5: 0.5800\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 105\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 105 [batch #0, batch_size 16, seq length 212]\tLoss: 0.106514\n","23it [00:00, 31.33it/s]Train epoch: 105 [batch #25, batch_size 16, seq length 571]\tLoss: 0.054309\n","47it [00:01, 30.51it/s]Train epoch: 105 [batch #50, batch_size 16, seq length 709]\tLoss: 0.059965\n","73it [00:02, 27.25it/s]Train epoch: 105 [batch #75, batch_size 16, seq length 806]\tLoss: 0.069935\n","100it [00:03, 27.72it/s]Train epoch: 105 [batch #100, batch_size 16, seq length 892]\tLoss: 0.062845\n","124it [00:04, 28.08it/s]Train epoch: 105 [batch #125, batch_size 16, seq length 978]\tLoss: 0.059111\n","148it [00:05, 26.38it/s]Train epoch: 105 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.068734\n","175it [00:06, 25.71it/s]Train epoch: 105 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.063762\n","199it [00:07, 24.45it/s]Train epoch: 105 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.064680\n","223it [00:08, 23.61it/s]Train epoch: 105 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.068596\n","250it [00:09, 23.64it/s]Train epoch: 105 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.071203\n","274it [00:10, 23.37it/s]Train epoch: 105 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.075953\n","298it [00:11, 22.93it/s]Train epoch: 105 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.080882\n","325it [00:12, 22.97it/s]Train epoch: 105 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.076623\n","349it [00:13, 21.00it/s]Train epoch: 105 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.077508\n","373it [00:14, 21.25it/s]Train epoch: 105 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.086262\n","400it [00:16, 20.28it/s]Train epoch: 105 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.083147\n","424it [00:17, 19.50it/s]Train epoch: 105 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.075547\n","450it [00:18, 17.49it/s]Train epoch: 105 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.082257\n","474it [00:20, 16.97it/s]Train epoch: 105 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.092226\n","500it [00:21, 15.14it/s]Train epoch: 105 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.099069\n","505it [00:22, 22.72it/s]\n","epoch loss: 0.07280503968975627\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 344.81it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3866, 0.6009, 0.4858, 0.5373, 0.8724\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4191, 0.6675, 0.5297, 0.5907, 0.8953\n","rec_at_5: 0.5712\n","prec_at_5: 0.5788\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 106\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 106 [batch #0, batch_size 16, seq length 212]\tLoss: 0.116445\n","23it [00:00, 31.22it/s]Train epoch: 106 [batch #25, batch_size 16, seq length 571]\tLoss: 0.054901\n","47it [00:01, 30.45it/s]Train epoch: 106 [batch #50, batch_size 16, seq length 709]\tLoss: 0.056397\n","73it [00:02, 28.08it/s]Train epoch: 106 [batch #75, batch_size 16, seq length 806]\tLoss: 0.069119\n","98it [00:03, 28.02it/s]Train epoch: 106 [batch #100, batch_size 16, seq length 892]\tLoss: 0.063961\n","125it [00:04, 27.54it/s]Train epoch: 106 [batch #125, batch_size 16, seq length 978]\tLoss: 0.060565\n","149it [00:05, 25.75it/s]Train epoch: 106 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.067667\n","173it [00:06, 25.62it/s]Train epoch: 106 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.060482\n","200it [00:07, 25.37it/s]Train epoch: 106 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.063214\n","224it [00:08, 24.80it/s]Train epoch: 106 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.065580\n","248it [00:09, 23.80it/s]Train epoch: 106 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.073334\n","275it [00:10, 23.02it/s]Train epoch: 106 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.074345\n","299it [00:11, 22.55it/s]Train epoch: 106 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.078048\n","323it [00:12, 21.56it/s]Train epoch: 106 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.075976\n","350it [00:13, 21.69it/s]Train epoch: 106 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.075508\n","374it [00:14, 21.36it/s]Train epoch: 106 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.084301\n","398it [00:16, 20.03it/s]Train epoch: 106 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.079685\n","425it [00:17, 18.84it/s]Train epoch: 106 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.076160\n","449it [00:18, 18.41it/s]Train epoch: 106 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.078805\n","475it [00:20, 16.58it/s]Train epoch: 106 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.093926\n","499it [00:21, 15.39it/s]Train epoch: 106 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.095816\n","505it [00:22, 22.80it/s]\n","epoch loss: 0.07151277656620829\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 347.20it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3842, 0.5987, 0.4841, 0.5353, 0.8721\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4167, 0.6660, 0.5268, 0.5883, 0.8946\n","rec_at_5: 0.5686\n","prec_at_5: 0.5766\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 107\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 107 [batch #0, batch_size 16, seq length 212]\tLoss: 0.108041\n","23it [00:00, 30.83it/s]Train epoch: 107 [batch #25, batch_size 16, seq length 571]\tLoss: 0.052752\n","50it [00:01, 29.76it/s]Train epoch: 107 [batch #50, batch_size 16, seq length 709]\tLoss: 0.055442\n","73it [00:02, 28.16it/s]Train epoch: 107 [batch #75, batch_size 16, seq length 806]\tLoss: 0.067621\n","100it [00:03, 27.50it/s]Train epoch: 107 [batch #100, batch_size 16, seq length 892]\tLoss: 0.062653\n","124it [00:04, 27.39it/s]Train epoch: 107 [batch #125, batch_size 16, seq length 978]\tLoss: 0.057177\n","148it [00:05, 26.49it/s]Train epoch: 107 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.068085\n","175it [00:06, 25.36it/s]Train epoch: 107 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.062289\n","199it [00:07, 24.40it/s]Train epoch: 107 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.061629\n","223it [00:08, 24.26it/s]Train epoch: 107 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.066930\n","250it [00:09, 23.53it/s]Train epoch: 107 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.069794\n","274it [00:10, 23.57it/s]Train epoch: 107 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.071759\n","298it [00:11, 22.11it/s]Train epoch: 107 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.077497\n","325it [00:12, 21.54it/s]Train epoch: 107 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.072191\n","349it [00:13, 21.33it/s]Train epoch: 107 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.071939\n","373it [00:15, 20.29it/s]Train epoch: 107 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.081274\n","400it [00:16, 20.29it/s]Train epoch: 107 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.078722\n","425it [00:17, 19.44it/s]Train epoch: 107 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.073439\n","449it [00:18, 19.02it/s]Train epoch: 107 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.077166\n","475it [00:20, 16.58it/s]Train epoch: 107 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.089765\n","499it [00:21, 15.87it/s]Train epoch: 107 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.093871\n","505it [00:22, 22.70it/s]\n","epoch loss: 0.06981733494650315\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 354.02it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3849, 0.5968, 0.4856, 0.5355, 0.8719\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4172, 0.6645, 0.5286, 0.5888, 0.8943\n","rec_at_5: 0.5688\n","prec_at_5: 0.5764\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 108\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 108 [batch #0, batch_size 16, seq length 212]\tLoss: 0.103741\n","23it [00:00, 30.82it/s]Train epoch: 108 [batch #25, batch_size 16, seq length 571]\tLoss: 0.051878\n","50it [00:01, 28.87it/s]Train epoch: 108 [batch #50, batch_size 16, seq length 709]\tLoss: 0.054817\n","74it [00:02, 28.36it/s]Train epoch: 108 [batch #75, batch_size 16, seq length 806]\tLoss: 0.066004\n","99it [00:03, 27.69it/s]Train epoch: 108 [batch #100, batch_size 16, seq length 892]\tLoss: 0.057097\n","123it [00:04, 27.42it/s]Train epoch: 108 [batch #125, batch_size 16, seq length 978]\tLoss: 0.056880\n","150it [00:05, 26.56it/s]Train epoch: 108 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.065809\n","174it [00:06, 24.94it/s]Train epoch: 108 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.062885\n","198it [00:07, 24.54it/s]Train epoch: 108 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.059244\n","225it [00:08, 24.43it/s]Train epoch: 108 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.066840\n","249it [00:09, 24.04it/s]Train epoch: 108 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.065382\n","273it [00:10, 23.06it/s]Train epoch: 108 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.071693\n","300it [00:11, 23.23it/s]Train epoch: 108 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.077154\n","324it [00:12, 22.28it/s]Train epoch: 108 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.073192\n","348it [00:13, 21.13it/s]Train epoch: 108 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.072659\n","375it [00:15, 20.54it/s]Train epoch: 108 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.078705\n","399it [00:16, 19.83it/s]Train epoch: 108 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.076151\n","424it [00:17, 19.20it/s]Train epoch: 108 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.070771\n","449it [00:18, 18.07it/s]Train epoch: 108 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.074964\n","475it [00:20, 16.33it/s]Train epoch: 108 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.087392\n","499it [00:21, 15.93it/s]Train epoch: 108 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.091787\n","505it [00:22, 22.59it/s]\n","epoch loss: 0.0685074664910536\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 345.45it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3855, 0.5960, 0.4878, 0.5365, 0.8716\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4174, 0.6625, 0.5301, 0.5890, 0.8942\n","rec_at_5: 0.5688\n","prec_at_5: 0.5770\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 109\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 109 [batch #0, batch_size 16, seq length 212]\tLoss: 0.114754\n","23it [00:00, 31.17it/s]Train epoch: 109 [batch #25, batch_size 16, seq length 571]\tLoss: 0.054039\n","47it [00:01, 30.66it/s]Train epoch: 109 [batch #50, batch_size 16, seq length 709]\tLoss: 0.054423\n","74it [00:02, 28.37it/s]Train epoch: 109 [batch #75, batch_size 16, seq length 806]\tLoss: 0.065948\n","98it [00:03, 27.46it/s]Train epoch: 109 [batch #100, batch_size 16, seq length 892]\tLoss: 0.057774\n","125it [00:04, 26.43it/s]Train epoch: 109 [batch #125, batch_size 16, seq length 978]\tLoss: 0.055247\n","149it [00:05, 25.89it/s]Train epoch: 109 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.066846\n","173it [00:06, 24.81it/s]Train epoch: 109 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.059566\n","200it [00:07, 23.76it/s]Train epoch: 109 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.061219\n","224it [00:08, 24.19it/s]Train epoch: 109 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.064599\n","248it [00:09, 24.12it/s]Train epoch: 109 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.066976\n","275it [00:10, 23.56it/s]Train epoch: 109 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.071591\n","299it [00:11, 22.93it/s]Train epoch: 109 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.075672\n","323it [00:12, 22.32it/s]Train epoch: 109 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.070819\n","350it [00:13, 21.42it/s]Train epoch: 109 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.073716\n","374it [00:14, 21.13it/s]Train epoch: 109 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.077656\n","398it [00:16, 20.60it/s]Train epoch: 109 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.073926\n","424it [00:17, 19.75it/s]Train epoch: 109 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.070074\n","450it [00:18, 18.74it/s]Train epoch: 109 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.076626\n","474it [00:20, 17.60it/s]Train epoch: 109 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.084482\n","500it [00:21, 14.94it/s]Train epoch: 109 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.090116\n","505it [00:22, 22.79it/s]\n","epoch loss: 0.06748225453060748\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 347.17it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3845, 0.5967, 0.4850, 0.5351, 0.8715\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4168, 0.6638, 0.5283, 0.5883, 0.8940\n","rec_at_5: 0.5660\n","prec_at_5: 0.5743\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 110\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 110 [batch #0, batch_size 16, seq length 212]\tLoss: 0.117759\n","22it [00:00, 30.36it/s]Train epoch: 110 [batch #25, batch_size 16, seq length 571]\tLoss: 0.050712\n","50it [00:01, 30.03it/s]Train epoch: 110 [batch #50, batch_size 16, seq length 709]\tLoss: 0.051308\n","73it [00:02, 29.55it/s]Train epoch: 110 [batch #75, batch_size 16, seq length 806]\tLoss: 0.066048\n","99it [00:03, 27.50it/s]Train epoch: 110 [batch #100, batch_size 16, seq length 892]\tLoss: 0.058595\n","123it [00:04, 27.20it/s]Train epoch: 110 [batch #125, batch_size 16, seq length 978]\tLoss: 0.056311\n","150it [00:05, 26.01it/s]Train epoch: 110 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.065612\n","174it [00:06, 26.58it/s]Train epoch: 110 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.058387\n","198it [00:07, 24.94it/s]Train epoch: 110 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.059386\n","225it [00:08, 23.72it/s]Train epoch: 110 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.061940\n","249it [00:09, 23.64it/s]Train epoch: 110 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.065861\n","273it [00:10, 23.69it/s]Train epoch: 110 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.066725\n","300it [00:11, 23.05it/s]Train epoch: 110 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.072933\n","324it [00:12, 21.84it/s]Train epoch: 110 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.069097\n","348it [00:13, 21.70it/s]Train epoch: 110 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.069255\n","375it [00:14, 20.62it/s]Train epoch: 110 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.077147\n","399it [00:16, 20.79it/s]Train epoch: 110 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.071642\n","424it [00:17, 19.36it/s]Train epoch: 110 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.066282\n","448it [00:18, 18.96it/s]Train epoch: 110 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.072225\n","475it [00:20, 17.76it/s]Train epoch: 110 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.084376\n","499it [00:21, 16.16it/s]Train epoch: 110 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.090886\n","505it [00:22, 22.92it/s]\n","epoch loss: 0.06583113666660714\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 347.84it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3834, 0.5928, 0.4854, 0.5338, 0.8711\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4145, 0.6603, 0.5269, 0.5861, 0.8933\n","rec_at_5: 0.5679\n","prec_at_5: 0.5760\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 111\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 111 [batch #0, batch_size 16, seq length 212]\tLoss: 0.107068\n","25it [00:00, 30.05it/s]Train epoch: 111 [batch #25, batch_size 16, seq length 571]\tLoss: 0.052793\n","49it [00:01, 29.74it/s]Train epoch: 111 [batch #50, batch_size 16, seq length 709]\tLoss: 0.051536\n","75it [00:02, 28.25it/s]Train epoch: 111 [batch #75, batch_size 16, seq length 806]\tLoss: 0.062758\n","99it [00:03, 27.31it/s]Train epoch: 111 [batch #100, batch_size 16, seq length 892]\tLoss: 0.056456\n","123it [00:04, 26.84it/s]Train epoch: 111 [batch #125, batch_size 16, seq length 978]\tLoss: 0.053536\n","150it [00:05, 24.87it/s]Train epoch: 111 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.062294\n","174it [00:06, 24.87it/s]Train epoch: 111 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.057159\n","198it [00:07, 25.33it/s]Train epoch: 111 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.057490\n","225it [00:08, 23.65it/s]Train epoch: 111 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.062051\n","249it [00:09, 23.81it/s]Train epoch: 111 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.064846\n","273it [00:10, 22.65it/s]Train epoch: 111 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.065982\n","300it [00:11, 22.33it/s]Train epoch: 111 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.072172\n","324it [00:12, 21.82it/s]Train epoch: 111 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.067146\n","348it [00:13, 21.58it/s]Train epoch: 111 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.065498\n","375it [00:15, 20.79it/s]Train epoch: 111 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.075316\n","399it [00:16, 20.57it/s]Train epoch: 111 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.073552\n","425it [00:17, 19.01it/s]Train epoch: 111 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.067249\n","449it [00:18, 18.47it/s]Train epoch: 111 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.068705\n","475it [00:20, 17.37it/s]Train epoch: 111 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.082421\n","499it [00:21, 16.04it/s]Train epoch: 111 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.085937\n","505it [00:22, 22.83it/s]\n","epoch loss: 0.06469125383911599\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 348.46it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3830, 0.5913, 0.4848, 0.5328, 0.8708\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4154, 0.6605, 0.5282, 0.5870, 0.8931\n","rec_at_5: 0.5671\n","prec_at_5: 0.5766\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 112\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 112 [batch #0, batch_size 16, seq length 212]\tLoss: 0.107506\n","23it [00:00, 30.46it/s]Train epoch: 112 [batch #25, batch_size 16, seq length 571]\tLoss: 0.048473\n","47it [00:01, 29.67it/s]Train epoch: 112 [batch #50, batch_size 16, seq length 709]\tLoss: 0.052984\n","74it [00:02, 28.64it/s]Train epoch: 112 [batch #75, batch_size 16, seq length 806]\tLoss: 0.060508\n","99it [00:03, 27.83it/s]Train epoch: 112 [batch #100, batch_size 16, seq length 892]\tLoss: 0.056592\n","123it [00:04, 27.19it/s]Train epoch: 112 [batch #125, batch_size 16, seq length 978]\tLoss: 0.054778\n","150it [00:05, 26.54it/s]Train epoch: 112 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.060533\n","174it [00:06, 26.12it/s]Train epoch: 112 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.054726\n","198it [00:07, 26.10it/s]Train epoch: 112 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.054922\n","225it [00:08, 24.55it/s]Train epoch: 112 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.057965\n","249it [00:09, 24.26it/s]Train epoch: 112 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.063497\n","273it [00:10, 23.60it/s]Train epoch: 112 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.068409\n","300it [00:11, 22.12it/s]Train epoch: 112 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.068080\n","324it [00:12, 22.72it/s]Train epoch: 112 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.065347\n","348it [00:13, 21.43it/s]Train epoch: 112 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.067336\n","375it [00:14, 20.68it/s]Train epoch: 112 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.075442\n","399it [00:16, 20.43it/s]Train epoch: 112 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.071769\n","423it [00:17, 20.39it/s]Train epoch: 112 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.065306\n","450it [00:18, 18.47it/s]Train epoch: 112 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.069758\n","474it [00:20, 17.91it/s]Train epoch: 112 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.079019\n","500it [00:21, 15.62it/s]Train epoch: 112 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.082069\n","505it [00:21, 23.00it/s]\n","epoch loss: 0.06326788895957099\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 354.20it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3834, 0.5914, 0.4851, 0.5330, 0.8705\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4148, 0.6590, 0.5282, 0.5864, 0.8927\n","rec_at_5: 0.5675\n","prec_at_5: 0.5765\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 113\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 113 [batch #0, batch_size 16, seq length 212]\tLoss: 0.100043\n","22it [00:00, 30.87it/s]Train epoch: 113 [batch #25, batch_size 16, seq length 571]\tLoss: 0.046375\n","50it [00:01, 29.76it/s]Train epoch: 113 [batch #50, batch_size 16, seq length 709]\tLoss: 0.048170\n","74it [00:02, 28.05it/s]Train epoch: 113 [batch #75, batch_size 16, seq length 806]\tLoss: 0.059733\n","100it [00:03, 28.25it/s]Train epoch: 113 [batch #100, batch_size 16, seq length 892]\tLoss: 0.055151\n","124it [00:04, 26.92it/s]Train epoch: 113 [batch #125, batch_size 16, seq length 978]\tLoss: 0.054533\n","148it [00:05, 26.65it/s]Train epoch: 113 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.058701\n","175it [00:06, 25.77it/s]Train epoch: 113 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.054746\n","199it [00:07, 25.98it/s]Train epoch: 113 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.055659\n","223it [00:08, 24.42it/s]Train epoch: 113 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.057458\n","250it [00:09, 24.47it/s]Train epoch: 113 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.063730\n","274it [00:10, 23.53it/s]Train epoch: 113 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.065439\n","298it [00:11, 23.28it/s]Train epoch: 113 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.069270\n","325it [00:12, 22.38it/s]Train epoch: 113 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.065907\n","349it [00:13, 21.84it/s]Train epoch: 113 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.064956\n","373it [00:14, 20.24it/s]Train epoch: 113 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.072130\n","400it [00:16, 20.68it/s]Train epoch: 113 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.069550\n","424it [00:17, 19.85it/s]Train epoch: 113 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.065158\n","450it [00:18, 18.84it/s]Train epoch: 113 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.068553\n","474it [00:20, 17.91it/s]Train epoch: 113 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.075918\n","500it [00:21, 15.88it/s]Train epoch: 113 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.080803\n","505it [00:22, 22.94it/s]\n","epoch loss: 0.06189133233141781\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 350.48it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3832, 0.5924, 0.4833, 0.5323, 0.8703\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4147, 0.6599, 0.5274, 0.5863, 0.8923\n","rec_at_5: 0.5671\n","prec_at_5: 0.5758\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 114\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 114 [batch #0, batch_size 16, seq length 212]\tLoss: 0.101581\n","23it [00:00, 31.32it/s]Train epoch: 114 [batch #25, batch_size 16, seq length 571]\tLoss: 0.046799\n","47it [00:01, 29.91it/s]Train epoch: 114 [batch #50, batch_size 16, seq length 709]\tLoss: 0.048507\n","75it [00:02, 28.88it/s]Train epoch: 114 [batch #75, batch_size 16, seq length 806]\tLoss: 0.061026\n","98it [00:03, 28.01it/s]Train epoch: 114 [batch #100, batch_size 16, seq length 892]\tLoss: 0.053612\n","125it [00:04, 27.53it/s]Train epoch: 114 [batch #125, batch_size 16, seq length 978]\tLoss: 0.050895\n","149it [00:05, 26.26it/s]Train epoch: 114 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.058930\n","173it [00:06, 26.22it/s]Train epoch: 114 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.052572\n","200it [00:07, 25.01it/s]Train epoch: 114 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.054889\n","224it [00:08, 24.49it/s]Train epoch: 114 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.057438\n","248it [00:09, 23.99it/s]Train epoch: 114 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.059586\n","275it [00:10, 22.99it/s]Train epoch: 114 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.063042\n","299it [00:11, 21.96it/s]Train epoch: 114 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.066341\n","323it [00:12, 22.17it/s]Train epoch: 114 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.065666\n","350it [00:13, 22.26it/s]Train epoch: 114 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.063412\n","374it [00:14, 20.68it/s]Train epoch: 114 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.070615\n","398it [00:16, 20.62it/s]Train epoch: 114 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.066902\n","425it [00:17, 19.02it/s]Train epoch: 114 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.062465\n","449it [00:18, 18.95it/s]Train epoch: 114 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.068210\n","475it [00:20, 18.15it/s]Train epoch: 114 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.077461\n","499it [00:21, 15.61it/s]Train epoch: 114 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.077979\n","505it [00:21, 23.00it/s]\n","epoch loss: 0.060200008028878434\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 351.81it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3840, 0.5915, 0.4862, 0.5337, 0.8702\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4156, 0.6586, 0.5297, 0.5871, 0.8924\n","rec_at_5: 0.5667\n","prec_at_5: 0.5760\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 115\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 115 [batch #0, batch_size 16, seq length 212]\tLoss: 0.086609\n","22it [00:00, 31.69it/s]Train epoch: 115 [batch #25, batch_size 16, seq length 571]\tLoss: 0.044967\n","50it [00:01, 29.47it/s]Train epoch: 115 [batch #50, batch_size 16, seq length 709]\tLoss: 0.047296\n","74it [00:02, 29.24it/s]Train epoch: 115 [batch #75, batch_size 16, seq length 806]\tLoss: 0.057021\n","98it [00:03, 27.09it/s]Train epoch: 115 [batch #100, batch_size 16, seq length 892]\tLoss: 0.052151\n","125it [00:04, 26.27it/s]Train epoch: 115 [batch #125, batch_size 16, seq length 978]\tLoss: 0.053696\n","149it [00:05, 25.63it/s]Train epoch: 115 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.055799\n","173it [00:06, 25.86it/s]Train epoch: 115 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.053467\n","200it [00:07, 24.76it/s]Train epoch: 115 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.053100\n","224it [00:08, 24.04it/s]Train epoch: 115 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.055034\n","248it [00:09, 23.77it/s]Train epoch: 115 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.059551\n","275it [00:10, 23.52it/s]Train epoch: 115 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.059861\n","299it [00:11, 22.70it/s]Train epoch: 115 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.066445\n","323it [00:12, 22.59it/s]Train epoch: 115 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.060980\n","350it [00:13, 21.71it/s]Train epoch: 115 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.063408\n","374it [00:14, 20.86it/s]Train epoch: 115 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.067730\n","398it [00:16, 20.70it/s]Train epoch: 115 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.063577\n","423it [00:17, 19.05it/s]Train epoch: 115 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.060619\n","450it [00:18, 19.09it/s]Train epoch: 115 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.067244\n","474it [00:20, 17.02it/s]Train epoch: 115 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.074643\n","500it [00:21, 15.39it/s]Train epoch: 115 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.077524\n","505it [00:22, 22.90it/s]\n","epoch loss: 0.05895918798679025\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 354.22it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3835, 0.5882, 0.4882, 0.5335, 0.8697\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4149, 0.6554, 0.5308, 0.5865, 0.8920\n","rec_at_5: 0.5647\n","prec_at_5: 0.5747\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 116\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 116 [batch #0, batch_size 16, seq length 212]\tLoss: 0.103244\n","22it [00:00, 30.47it/s]Train epoch: 116 [batch #25, batch_size 16, seq length 571]\tLoss: 0.042510\n","50it [00:01, 29.42it/s]Train epoch: 116 [batch #50, batch_size 16, seq length 709]\tLoss: 0.047150\n","73it [00:02, 28.48it/s]Train epoch: 116 [batch #75, batch_size 16, seq length 806]\tLoss: 0.055819\n","98it [00:03, 27.57it/s]Train epoch: 116 [batch #100, batch_size 16, seq length 892]\tLoss: 0.050872\n","125it [00:04, 27.55it/s]Train epoch: 116 [batch #125, batch_size 16, seq length 978]\tLoss: 0.050045\n","149it [00:05, 25.87it/s]Train epoch: 116 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.056648\n","173it [00:06, 25.69it/s]Train epoch: 116 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.051783\n","200it [00:07, 25.69it/s]Train epoch: 116 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.052255\n","224it [00:08, 24.60it/s]Train epoch: 116 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.053364\n","248it [00:09, 23.48it/s]Train epoch: 116 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.056572\n","275it [00:10, 23.50it/s]Train epoch: 116 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.059593\n","299it [00:11, 22.81it/s]Train epoch: 116 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.066301\n","323it [00:12, 23.40it/s]Train epoch: 116 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.061010\n","350it [00:13, 21.03it/s]Train epoch: 116 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.061480\n","374it [00:14, 20.68it/s]Train epoch: 116 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.070514\n","398it [00:16, 20.02it/s]Train epoch: 116 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.062855\n","425it [00:17, 19.69it/s]Train epoch: 116 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.059519\n","449it [00:18, 18.51it/s]Train epoch: 116 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.063362\n","475it [00:20, 17.73it/s]Train epoch: 116 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.072177\n","499it [00:21, 16.18it/s]Train epoch: 116 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.073769\n","505it [00:22, 22.90it/s]\n","epoch loss: 0.05773614423146638\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 353.23it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3812, 0.5894, 0.4835, 0.5312, 0.8696\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4127, 0.6570, 0.5260, 0.5843, 0.8913\n","rec_at_5: 0.5630\n","prec_at_5: 0.5732\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 117\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 117 [batch #0, batch_size 16, seq length 212]\tLoss: 0.100033\n","23it [00:00, 31.63it/s]Train epoch: 117 [batch #25, batch_size 16, seq length 571]\tLoss: 0.043846\n","47it [00:01, 30.29it/s]Train epoch: 117 [batch #50, batch_size 16, seq length 709]\tLoss: 0.043448\n","73it [00:02, 28.86it/s]Train epoch: 117 [batch #75, batch_size 16, seq length 806]\tLoss: 0.054051\n","98it [00:03, 27.45it/s]Train epoch: 117 [batch #100, batch_size 16, seq length 892]\tLoss: 0.050243\n","123it [00:04, 26.97it/s]Train epoch: 117 [batch #125, batch_size 16, seq length 978]\tLoss: 0.050930\n","150it [00:05, 25.46it/s]Train epoch: 117 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.055880\n","174it [00:06, 25.33it/s]Train epoch: 117 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.050087\n","198it [00:07, 25.54it/s]Train epoch: 117 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.049643\n","225it [00:08, 24.96it/s]Train epoch: 117 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.054379\n","249it [00:09, 24.07it/s]Train epoch: 117 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.055549\n","273it [00:10, 23.05it/s]Train epoch: 117 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.058587\n","300it [00:11, 22.09it/s]Train epoch: 117 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.065242\n","324it [00:12, 22.89it/s]Train epoch: 117 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.057644\n","348it [00:13, 21.36it/s]Train epoch: 117 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.060807\n","375it [00:14, 20.99it/s]Train epoch: 117 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.067190\n","399it [00:16, 20.06it/s]Train epoch: 117 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.060504\n","424it [00:17, 19.65it/s]Train epoch: 117 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.058551\n","449it [00:18, 18.39it/s]Train epoch: 117 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.061756\n","474it [00:20, 17.18it/s]Train epoch: 117 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.071473\n","500it [00:21, 15.66it/s]Train epoch: 117 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.072644\n","505it [00:22, 22.87it/s]\n","epoch loss: 0.05625459654994383\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 352.51it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3810, 0.5871, 0.4849, 0.5311, 0.8692\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4135, 0.6547, 0.5289, 0.5851, 0.8912\n","rec_at_5: 0.5616\n","prec_at_5: 0.5722\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 118\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 118 [batch #0, batch_size 16, seq length 212]\tLoss: 0.094731\n","23it [00:00, 31.61it/s]Train epoch: 118 [batch #25, batch_size 16, seq length 571]\tLoss: 0.042542\n","47it [00:01, 30.25it/s]Train epoch: 118 [batch #50, batch_size 16, seq length 709]\tLoss: 0.044999\n","74it [00:02, 28.47it/s]Train epoch: 118 [batch #75, batch_size 16, seq length 806]\tLoss: 0.055301\n","100it [00:03, 28.73it/s]Train epoch: 118 [batch #100, batch_size 16, seq length 892]\tLoss: 0.050060\n","124it [00:04, 27.81it/s]Train epoch: 118 [batch #125, batch_size 16, seq length 978]\tLoss: 0.046984\n","148it [00:05, 26.97it/s]Train epoch: 118 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.052690\n","175it [00:06, 26.58it/s]Train epoch: 118 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.049147\n","199it [00:07, 25.36it/s]Train epoch: 118 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.048806\n","223it [00:08, 24.02it/s]Train epoch: 118 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.052060\n","250it [00:09, 23.71it/s]Train epoch: 118 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.056072\n","274it [00:10, 23.68it/s]Train epoch: 118 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.056456\n","298it [00:11, 22.55it/s]Train epoch: 118 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.061789\n","325it [00:12, 22.97it/s]Train epoch: 118 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.058964\n","349it [00:13, 20.93it/s]Train epoch: 118 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.059752\n","373it [00:14, 21.12it/s]Train epoch: 118 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.064935\n","400it [00:16, 20.86it/s]Train epoch: 118 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.060849\n","425it [00:17, 18.68it/s]Train epoch: 118 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.055979\n","449it [00:18, 18.59it/s]Train epoch: 118 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.061655\n","474it [00:20, 17.97it/s]Train epoch: 118 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.068433\n","500it [00:21, 14.89it/s]Train epoch: 118 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.070856\n","505it [00:21, 22.96it/s]\n","epoch loss: 0.05538614634608868\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 351.50it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3807, 0.5854, 0.4847, 0.5303, 0.8689\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4122, 0.6529, 0.5278, 0.5838, 0.8907\n","rec_at_5: 0.5622\n","prec_at_5: 0.5730\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 119\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 119 [batch #0, batch_size 16, seq length 212]\tLoss: 0.092498\n","23it [00:00, 31.37it/s]Train epoch: 119 [batch #25, batch_size 16, seq length 571]\tLoss: 0.043595\n","49it [00:01, 28.47it/s]Train epoch: 119 [batch #50, batch_size 16, seq length 709]\tLoss: 0.043282\n","73it [00:02, 29.48it/s]Train epoch: 119 [batch #75, batch_size 16, seq length 806]\tLoss: 0.054159\n","98it [00:03, 27.81it/s]Train epoch: 119 [batch #100, batch_size 16, seq length 892]\tLoss: 0.046057\n","125it [00:04, 27.36it/s]Train epoch: 119 [batch #125, batch_size 16, seq length 978]\tLoss: 0.045651\n","149it [00:05, 26.38it/s]Train epoch: 119 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.053057\n","173it [00:06, 26.27it/s]Train epoch: 119 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.045966\n","200it [00:07, 24.69it/s]Train epoch: 119 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.047772\n","224it [00:08, 24.04it/s]Train epoch: 119 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.050741\n","248it [00:09, 24.43it/s]Train epoch: 119 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.053777\n","275it [00:10, 23.30it/s]Train epoch: 119 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.054514\n","299it [00:11, 23.28it/s]Train epoch: 119 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.061405\n","323it [00:12, 22.33it/s]Train epoch: 119 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.057010\n","350it [00:13, 21.64it/s]Train epoch: 119 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.058945\n","374it [00:14, 20.24it/s]Train epoch: 119 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.064745\n","398it [00:16, 19.78it/s]Train epoch: 119 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.060336\n","424it [00:17, 19.12it/s]Train epoch: 119 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.053232\n","449it [00:18, 18.37it/s]Train epoch: 119 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.059931\n","475it [00:20, 17.80it/s]Train epoch: 119 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.066754\n","499it [00:21, 16.09it/s]Train epoch: 119 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.067636\n","505it [00:22, 22.91it/s]\n","epoch loss: 0.05409488849858246\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 346.85it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3791, 0.5843, 0.4825, 0.5285, 0.8686\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4106, 0.6530, 0.5252, 0.5822, 0.8903\n","rec_at_5: 0.5627\n","prec_at_5: 0.5733\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 120\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 120 [batch #0, batch_size 16, seq length 212]\tLoss: 0.089426\n","25it [00:00, 31.45it/s]Train epoch: 120 [batch #25, batch_size 16, seq length 571]\tLoss: 0.039814\n","49it [00:01, 30.34it/s]Train epoch: 120 [batch #50, batch_size 16, seq length 709]\tLoss: 0.042615\n","75it [00:02, 28.26it/s]Train epoch: 120 [batch #75, batch_size 16, seq length 806]\tLoss: 0.053951\n","98it [00:03, 27.74it/s]Train epoch: 120 [batch #100, batch_size 16, seq length 892]\tLoss: 0.046322\n","125it [00:04, 26.29it/s]Train epoch: 120 [batch #125, batch_size 16, seq length 978]\tLoss: 0.044664\n","149it [00:05, 25.60it/s]Train epoch: 120 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.051286\n","173it [00:06, 26.08it/s]Train epoch: 120 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.047356\n","200it [00:07, 25.52it/s]Train epoch: 120 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.046665\n","224it [00:08, 24.01it/s]Train epoch: 120 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.047210\n","248it [00:09, 23.14it/s]Train epoch: 120 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.053299\n","275it [00:10, 22.63it/s]Train epoch: 120 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.055813\n","299it [00:11, 21.97it/s]Train epoch: 120 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.059417\n","323it [00:12, 21.79it/s]Train epoch: 120 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.053821\n","350it [00:13, 21.28it/s]Train epoch: 120 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.056997\n","374it [00:15, 21.36it/s]Train epoch: 120 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.059360\n","398it [00:16, 20.61it/s]Train epoch: 120 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.059987\n","424it [00:17, 19.87it/s]Train epoch: 120 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.054748\n","449it [00:18, 18.95it/s]Train epoch: 120 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.057553\n","475it [00:20, 17.75it/s]Train epoch: 120 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.064636\n","499it [00:21, 15.20it/s]Train epoch: 120 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.066971\n","505it [00:22, 22.80it/s]\n","epoch loss: 0.05284058396142013\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 353.57it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3803, 0.5843, 0.4855, 0.5303, 0.8683\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4120, 0.6500, 0.5295, 0.5836, 0.8902\n","rec_at_5: 0.5608\n","prec_at_5: 0.5716\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 121\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 121 [batch #0, batch_size 16, seq length 212]\tLoss: 0.088743\n","23it [00:00, 31.74it/s]Train epoch: 121 [batch #25, batch_size 16, seq length 571]\tLoss: 0.041244\n","47it [00:01, 30.80it/s]Train epoch: 121 [batch #50, batch_size 16, seq length 709]\tLoss: 0.042186\n","73it [00:02, 28.84it/s]Train epoch: 121 [batch #75, batch_size 16, seq length 806]\tLoss: 0.051103\n","99it [00:03, 27.52it/s]Train epoch: 121 [batch #100, batch_size 16, seq length 892]\tLoss: 0.045495\n","123it [00:04, 26.64it/s]Train epoch: 121 [batch #125, batch_size 16, seq length 978]\tLoss: 0.044788\n","150it [00:05, 25.79it/s]Train epoch: 121 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.052086\n","174it [00:06, 26.30it/s]Train epoch: 121 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.047622\n","198it [00:07, 24.50it/s]Train epoch: 121 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.044192\n","225it [00:08, 24.76it/s]Train epoch: 121 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.047077\n","249it [00:09, 23.61it/s]Train epoch: 121 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.050682\n","273it [00:10, 23.31it/s]Train epoch: 121 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.054122\n","300it [00:11, 22.32it/s]Train epoch: 121 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.058121\n","324it [00:12, 22.14it/s]Train epoch: 121 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.054297\n","348it [00:13, 21.76it/s]Train epoch: 121 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.055797\n","375it [00:14, 21.08it/s]Train epoch: 121 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.062079\n","399it [00:16, 20.95it/s]Train epoch: 121 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.056609\n","423it [00:17, 19.52it/s]Train epoch: 121 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.052936\n","449it [00:18, 19.09it/s]Train epoch: 121 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.056407\n","475it [00:20, 17.36it/s]Train epoch: 121 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.063763\n","499it [00:21, 16.04it/s]Train epoch: 121 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.065074\n","505it [00:21, 23.00it/s]\n","epoch loss: 0.05203200773806265\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 347.64it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3797, 0.5816, 0.4856, 0.5293, 0.8681\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4099, 0.6472, 0.5278, 0.5815, 0.8900\n","rec_at_5: 0.5599\n","prec_at_5: 0.5713\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 122\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 122 [batch #0, batch_size 16, seq length 212]\tLoss: 0.085330\n","23it [00:00, 30.46it/s]Train epoch: 122 [batch #25, batch_size 16, seq length 571]\tLoss: 0.037845\n","47it [00:01, 29.72it/s]Train epoch: 122 [batch #50, batch_size 16, seq length 709]\tLoss: 0.038664\n","74it [00:02, 28.93it/s]Train epoch: 122 [batch #75, batch_size 16, seq length 806]\tLoss: 0.051749\n","98it [00:03, 27.74it/s]Train epoch: 122 [batch #100, batch_size 16, seq length 892]\tLoss: 0.045041\n","125it [00:04, 26.81it/s]Train epoch: 122 [batch #125, batch_size 16, seq length 978]\tLoss: 0.046384\n","149it [00:05, 26.39it/s]Train epoch: 122 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.049719\n","173it [00:06, 25.22it/s]Train epoch: 122 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.043168\n","200it [00:07, 24.16it/s]Train epoch: 122 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.045151\n","224it [00:08, 24.69it/s]Train epoch: 122 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.046692\n","248it [00:09, 24.22it/s]Train epoch: 122 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.051284\n","275it [00:10, 23.24it/s]Train epoch: 122 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.051125\n","299it [00:11, 22.70it/s]Train epoch: 122 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.055139\n","323it [00:12, 21.87it/s]Train epoch: 122 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.052409\n","350it [00:13, 21.63it/s]Train epoch: 122 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.055011\n","374it [00:14, 21.38it/s]Train epoch: 122 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.057400\n","398it [00:16, 20.12it/s]Train epoch: 122 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.055336\n","425it [00:17, 19.90it/s]Train epoch: 122 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.051297\n","450it [00:18, 18.58it/s]Train epoch: 122 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.053923\n","475it [00:20, 18.15it/s]Train epoch: 122 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.060068\n","499it [00:21, 16.25it/s]Train epoch: 122 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.062842\n","505it [00:22, 22.85it/s]\n","epoch loss: 0.05020490376795135\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 339.88it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3789, 0.5810, 0.4849, 0.5286, 0.8679\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4104, 0.6482, 0.5281, 0.5820, 0.8897\n","rec_at_5: 0.5597\n","prec_at_5: 0.5713\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 123\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 123 [batch #0, batch_size 16, seq length 212]\tLoss: 0.085749\n","23it [00:00, 31.38it/s]Train epoch: 123 [batch #25, batch_size 16, seq length 571]\tLoss: 0.041180\n","47it [00:01, 30.76it/s]Train epoch: 123 [batch #50, batch_size 16, seq length 709]\tLoss: 0.040331\n","73it [00:02, 27.99it/s]Train epoch: 123 [batch #75, batch_size 16, seq length 806]\tLoss: 0.051347\n","100it [00:03, 28.15it/s]Train epoch: 123 [batch #100, batch_size 16, seq length 892]\tLoss: 0.046677\n","124it [00:04, 26.77it/s]Train epoch: 123 [batch #125, batch_size 16, seq length 978]\tLoss: 0.043609\n","148it [00:05, 25.63it/s]Train epoch: 123 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.047078\n","175it [00:06, 25.55it/s]Train epoch: 123 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.044985\n","199it [00:07, 24.82it/s]Train epoch: 123 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.046095\n","223it [00:08, 23.78it/s]Train epoch: 123 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.043953\n","250it [00:09, 23.76it/s]Train epoch: 123 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.048906\n","274it [00:10, 23.85it/s]Train epoch: 123 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.051785\n","298it [00:11, 22.06it/s]Train epoch: 123 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.053631\n","325it [00:12, 22.34it/s]Train epoch: 123 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.051395\n","349it [00:13, 21.71it/s]Train epoch: 123 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.050850\n","373it [00:14, 21.25it/s]Train epoch: 123 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.056635\n","400it [00:16, 20.17it/s]Train epoch: 123 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.055136\n","425it [00:17, 19.18it/s]Train epoch: 123 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.051200\n","449it [00:18, 18.64it/s]Train epoch: 123 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.051438\n","475it [00:20, 17.57it/s]Train epoch: 123 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.062188\n","499it [00:21, 15.71it/s]Train epoch: 123 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.061361\n","505it [00:22, 22.79it/s]\n","epoch loss: 0.04959687176852091\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 341.30it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3791, 0.5810, 0.4848, 0.5286, 0.8677\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4109, 0.6478, 0.5291, 0.5825, 0.8893\n","rec_at_5: 0.5597\n","prec_at_5: 0.5708\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 124\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 124 [batch #0, batch_size 16, seq length 212]\tLoss: 0.078989\n","22it [00:00, 30.03it/s]Train epoch: 124 [batch #25, batch_size 16, seq length 571]\tLoss: 0.034474\n","50it [00:01, 29.33it/s]Train epoch: 124 [batch #50, batch_size 16, seq length 709]\tLoss: 0.038315\n","74it [00:02, 27.80it/s]Train epoch: 124 [batch #75, batch_size 16, seq length 806]\tLoss: 0.047400\n","98it [00:03, 26.56it/s]Train epoch: 124 [batch #100, batch_size 16, seq length 892]\tLoss: 0.044709\n","125it [00:04, 26.50it/s]Train epoch: 124 [batch #125, batch_size 16, seq length 978]\tLoss: 0.039800\n","149it [00:05, 25.56it/s]Train epoch: 124 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.045476\n","173it [00:06, 24.76it/s]Train epoch: 124 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.043066\n","200it [00:07, 24.55it/s]Train epoch: 124 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.042815\n","224it [00:08, 24.80it/s]Train epoch: 124 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.044282\n","248it [00:09, 23.81it/s]Train epoch: 124 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.049235\n","275it [00:10, 23.43it/s]Train epoch: 124 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.051488\n","299it [00:11, 22.02it/s]Train epoch: 124 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.053787\n","323it [00:12, 21.37it/s]Train epoch: 124 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.052047\n","350it [00:13, 21.21it/s]Train epoch: 124 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.051689\n","374it [00:15, 20.80it/s]Train epoch: 124 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.058749\n","398it [00:16, 20.10it/s]Train epoch: 124 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.053454\n","425it [00:17, 19.73it/s]Train epoch: 124 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.046861\n","449it [00:18, 18.29it/s]Train epoch: 124 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.052828\n","475it [00:20, 17.65it/s]Train epoch: 124 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.060534\n","499it [00:21, 16.05it/s]Train epoch: 124 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.060899\n","505it [00:22, 22.68it/s]\n","epoch loss: 0.048218749710681415\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 348.33it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3786, 0.5814, 0.4833, 0.5278, 0.8676\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4104, 0.6490, 0.5275, 0.5820, 0.8892\n","rec_at_5: 0.5580\n","prec_at_5: 0.5691\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 125\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 125 [batch #0, batch_size 16, seq length 212]\tLoss: 0.077199\n","23it [00:00, 31.35it/s]Train epoch: 125 [batch #25, batch_size 16, seq length 571]\tLoss: 0.035030\n","47it [00:01, 30.19it/s]Train epoch: 125 [batch #50, batch_size 16, seq length 709]\tLoss: 0.037925\n","75it [00:02, 29.20it/s]Train epoch: 125 [batch #75, batch_size 16, seq length 806]\tLoss: 0.047862\n","100it [00:03, 28.01it/s]Train epoch: 125 [batch #100, batch_size 16, seq length 892]\tLoss: 0.043522\n","124it [00:04, 27.52it/s]Train epoch: 125 [batch #125, batch_size 16, seq length 978]\tLoss: 0.043217\n","148it [00:05, 27.01it/s]Train epoch: 125 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.046001\n","175it [00:06, 26.01it/s]Train epoch: 125 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.041377\n","199it [00:07, 24.50it/s]Train epoch: 125 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.041886\n","223it [00:08, 24.19it/s]Train epoch: 125 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.046387\n","250it [00:09, 22.90it/s]Train epoch: 125 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.047550\n","274it [00:10, 23.32it/s]Train epoch: 125 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.048057\n","298it [00:11, 22.46it/s]Train epoch: 125 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.052678\n","325it [00:12, 22.54it/s]Train epoch: 125 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.047930\n","349it [00:13, 21.45it/s]Train epoch: 125 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.050735\n","373it [00:14, 21.09it/s]Train epoch: 125 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.056198\n","400it [00:16, 20.03it/s]Train epoch: 125 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.050656\n","424it [00:17, 19.05it/s]Train epoch: 125 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.050585\n","450it [00:18, 17.96it/s]Train epoch: 125 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.050662\n","474it [00:20, 16.73it/s]Train epoch: 125 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.059518\n","500it [00:21, 15.58it/s]Train epoch: 125 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.058232\n","505it [00:22, 22.75it/s]\n","epoch loss: 0.04746204791221731\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 342.38it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3792, 0.5792, 0.4858, 0.5284, 0.8674\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4103, 0.6455, 0.5297, 0.5819, 0.8889\n","rec_at_5: 0.5584\n","prec_at_5: 0.5700\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 126\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 126 [batch #0, batch_size 16, seq length 212]\tLoss: 0.089164\n","24it [00:00, 30.51it/s]Train epoch: 126 [batch #25, batch_size 16, seq length 571]\tLoss: 0.035615\n","48it [00:01, 29.85it/s]Train epoch: 126 [batch #50, batch_size 16, seq length 709]\tLoss: 0.036505\n","73it [00:02, 29.05it/s]Train epoch: 126 [batch #75, batch_size 16, seq length 806]\tLoss: 0.048029\n","99it [00:03, 26.83it/s]Train epoch: 126 [batch #100, batch_size 16, seq length 892]\tLoss: 0.042629\n","123it [00:04, 26.05it/s]Train epoch: 126 [batch #125, batch_size 16, seq length 978]\tLoss: 0.041395\n","150it [00:05, 25.98it/s]Train epoch: 126 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.041821\n","174it [00:06, 25.85it/s]Train epoch: 126 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.040638\n","198it [00:07, 24.48it/s]Train epoch: 126 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.042994\n","225it [00:08, 23.63it/s]Train epoch: 126 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.043822\n","249it [00:09, 22.80it/s]Train epoch: 126 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.044499\n","273it [00:10, 23.23it/s]Train epoch: 126 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.048663\n","300it [00:11, 21.91it/s]Train epoch: 126 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.052488\n","324it [00:12, 20.88it/s]Train epoch: 126 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.048543\n","348it [00:13, 21.59it/s]Train epoch: 126 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.048480\n","375it [00:15, 20.90it/s]Train epoch: 126 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.055723\n","399it [00:16, 20.07it/s]Train epoch: 126 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.050990\n","424it [00:17, 18.82it/s]Train epoch: 126 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.047345\n","450it [00:19, 19.14it/s]Train epoch: 126 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.050976\n","474it [00:20, 17.40it/s]Train epoch: 126 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.057036\n","500it [00:22, 15.63it/s]Train epoch: 126 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.057300\n","505it [00:22, 22.56it/s]\n","epoch loss: 0.04614878498976774\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 349.33it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3776, 0.5796, 0.4824, 0.5266, 0.8671\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4103, 0.6481, 0.5278, 0.5818, 0.8885\n","rec_at_5: 0.5575\n","prec_at_5: 0.5695\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 127\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 127 [batch #0, batch_size 16, seq length 212]\tLoss: 0.086116\n","25it [00:00, 29.41it/s]Train epoch: 127 [batch #25, batch_size 16, seq length 571]\tLoss: 0.035041\n","48it [00:01, 29.76it/s]Train epoch: 127 [batch #50, batch_size 16, seq length 709]\tLoss: 0.037839\n","73it [00:02, 28.66it/s]Train epoch: 127 [batch #75, batch_size 16, seq length 806]\tLoss: 0.047995\n","99it [00:03, 27.28it/s]Train epoch: 127 [batch #100, batch_size 16, seq length 892]\tLoss: 0.041679\n","123it [00:04, 26.52it/s]Train epoch: 127 [batch #125, batch_size 16, seq length 978]\tLoss: 0.039027\n","150it [00:05, 26.73it/s]Train epoch: 127 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.046727\n","174it [00:06, 26.18it/s]Train epoch: 127 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.042211\n","198it [00:07, 24.51it/s]Train epoch: 127 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.041138\n","225it [00:08, 23.48it/s]Train epoch: 127 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.043072\n","249it [00:09, 23.52it/s]Train epoch: 127 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.045311\n","273it [00:10, 23.72it/s]Train epoch: 127 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.044645\n","300it [00:11, 23.43it/s]Train epoch: 127 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.049674\n","324it [00:12, 22.10it/s]Train epoch: 127 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.045367\n","348it [00:13, 22.22it/s]Train epoch: 127 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.049696\n","375it [00:15, 20.99it/s]Train epoch: 127 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.052068\n","399it [00:16, 19.74it/s]Train epoch: 127 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.050497\n","425it [00:17, 19.84it/s]Train epoch: 127 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.046499\n","449it [00:18, 18.47it/s]Train epoch: 127 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.047682\n","475it [00:20, 17.31it/s]Train epoch: 127 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.054427\n","499it [00:21, 15.67it/s]Train epoch: 127 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.054438\n","505it [00:22, 22.75it/s]\n","epoch loss: 0.04510662430934239\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 347.17it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3776, 0.5784, 0.4825, 0.5261, 0.8668\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4092, 0.6466, 0.5271, 0.5808, 0.8883\n","rec_at_5: 0.5560\n","prec_at_5: 0.5676\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 128\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 128 [batch #0, batch_size 16, seq length 212]\tLoss: 0.084827\n","23it [00:00, 31.01it/s]Train epoch: 128 [batch #25, batch_size 16, seq length 571]\tLoss: 0.033035\n","50it [00:01, 29.91it/s]Train epoch: 128 [batch #50, batch_size 16, seq length 709]\tLoss: 0.035567\n","74it [00:02, 28.59it/s]Train epoch: 128 [batch #75, batch_size 16, seq length 806]\tLoss: 0.045499\n","99it [00:03, 27.73it/s]Train epoch: 128 [batch #100, batch_size 16, seq length 892]\tLoss: 0.042026\n","124it [00:04, 26.53it/s]Train epoch: 128 [batch #125, batch_size 16, seq length 978]\tLoss: 0.039370\n","148it [00:05, 25.99it/s]Train epoch: 128 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.044570\n","175it [00:06, 25.28it/s]Train epoch: 128 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.040886\n","199it [00:07, 25.28it/s]Train epoch: 128 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.036668\n","223it [00:08, 24.42it/s]Train epoch: 128 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.040931\n","250it [00:09, 23.20it/s]Train epoch: 128 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.043089\n","274it [00:10, 22.31it/s]Train epoch: 128 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.044824\n","298it [00:11, 21.77it/s]Train epoch: 128 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.045912\n","325it [00:12, 22.18it/s]Train epoch: 128 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.046710\n","349it [00:13, 21.97it/s]Train epoch: 128 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.048669\n","373it [00:15, 21.15it/s]Train epoch: 128 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.050652\n","400it [00:16, 20.37it/s]Train epoch: 128 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.049061\n","424it [00:17, 19.14it/s]Train epoch: 128 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.045436\n","449it [00:18, 18.88it/s]Train epoch: 128 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.048271\n","475it [00:20, 17.32it/s]Train epoch: 128 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.053643\n","499it [00:21, 15.51it/s]Train epoch: 128 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.053738\n","505it [00:22, 22.72it/s]\n","epoch loss: 0.04402648763786448\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 346.81it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3800, 0.5783, 0.4866, 0.5285, 0.8668\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4099, 0.6444, 0.5297, 0.5814, 0.8884\n","rec_at_5: 0.5559\n","prec_at_5: 0.5686\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 129\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 129 [batch #0, batch_size 16, seq length 212]\tLoss: 0.077168\n","22it [00:00, 29.98it/s]Train epoch: 129 [batch #25, batch_size 16, seq length 571]\tLoss: 0.034337\n","50it [00:01, 29.04it/s]Train epoch: 129 [batch #50, batch_size 16, seq length 709]\tLoss: 0.035475\n","73it [00:02, 28.94it/s]Train epoch: 129 [batch #75, batch_size 16, seq length 806]\tLoss: 0.044196\n","100it [00:03, 27.91it/s]Train epoch: 129 [batch #100, batch_size 16, seq length 892]\tLoss: 0.037926\n","124it [00:04, 26.60it/s]Train epoch: 129 [batch #125, batch_size 16, seq length 978]\tLoss: 0.038358\n","148it [00:05, 26.43it/s]Train epoch: 129 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.043564\n","175it [00:06, 25.01it/s]Train epoch: 129 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.038205\n","199it [00:07, 25.20it/s]Train epoch: 129 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.037966\n","223it [00:08, 24.92it/s]Train epoch: 129 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.039629\n","250it [00:09, 23.36it/s]Train epoch: 129 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.042410\n","274it [00:10, 22.76it/s]Train epoch: 129 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.046332\n","298it [00:11, 22.57it/s]Train epoch: 129 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.046768\n","325it [00:12, 21.31it/s]Train epoch: 129 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.044076\n","349it [00:13, 21.40it/s]Train epoch: 129 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.048701\n","373it [00:15, 21.37it/s]Train epoch: 129 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.050317\n","400it [00:16, 20.42it/s]Train epoch: 129 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.049219\n","424it [00:17, 19.79it/s]Train epoch: 129 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.043335\n","450it [00:19, 17.56it/s]Train epoch: 129 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.048356\n","474it [00:20, 17.76it/s]Train epoch: 129 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.052723\n","500it [00:21, 15.06it/s]Train epoch: 129 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.050925\n","505it [00:22, 22.65it/s]\n","epoch loss: 0.04328286554526058\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 344.66it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3789, 0.5772, 0.4863, 0.5279, 0.8665\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4095, 0.6428, 0.5301, 0.5811, 0.8880\n","rec_at_5: 0.5527\n","prec_at_5: 0.5655\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 130\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 130 [batch #0, batch_size 16, seq length 212]\tLoss: 0.077515\n","23it [00:00, 30.55it/s]Train epoch: 130 [batch #25, batch_size 16, seq length 571]\tLoss: 0.030875\n","47it [00:01, 30.42it/s]Train epoch: 130 [batch #50, batch_size 16, seq length 709]\tLoss: 0.036596\n","73it [00:02, 29.22it/s]Train epoch: 130 [batch #75, batch_size 16, seq length 806]\tLoss: 0.043602\n","98it [00:03, 27.09it/s]Train epoch: 130 [batch #100, batch_size 16, seq length 892]\tLoss: 0.037540\n","125it [00:04, 27.89it/s]Train epoch: 130 [batch #125, batch_size 16, seq length 978]\tLoss: 0.037563\n","149it [00:05, 26.28it/s]Train epoch: 130 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.042819\n","173it [00:06, 26.32it/s]Train epoch: 130 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.040319\n","200it [00:07, 25.15it/s]Train epoch: 130 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.036159\n","224it [00:08, 24.31it/s]Train epoch: 130 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.038598\n","248it [00:09, 24.23it/s]Train epoch: 130 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.041358\n","275it [00:10, 23.87it/s]Train epoch: 130 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.042234\n","299it [00:11, 22.07it/s]Train epoch: 130 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.045871\n","323it [00:12, 21.75it/s]Train epoch: 130 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.045588\n","350it [00:13, 20.99it/s]Train epoch: 130 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.044038\n","374it [00:15, 20.76it/s]Train epoch: 130 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.050235\n","398it [00:16, 19.32it/s]Train epoch: 130 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.043727\n","424it [00:17, 19.04it/s]Train epoch: 130 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.043279\n","450it [00:18, 18.59it/s]Train epoch: 130 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.046153\n","474it [00:20, 17.22it/s]Train epoch: 130 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.051064\n","500it [00:21, 15.46it/s]Train epoch: 130 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.050780\n","505it [00:22, 22.69it/s]\n","epoch loss: 0.04246100856378527\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 342.36it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3785, 0.5770, 0.4858, 0.5274, 0.8661\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4081, 0.6424, 0.5281, 0.5796, 0.8874\n","rec_at_5: 0.5523\n","prec_at_5: 0.5645\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 131\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 131 [batch #0, batch_size 16, seq length 212]\tLoss: 0.078158\n","22it [00:00, 30.89it/s]Train epoch: 131 [batch #25, batch_size 16, seq length 571]\tLoss: 0.032479\n","50it [00:01, 29.37it/s]Train epoch: 131 [batch #50, batch_size 16, seq length 709]\tLoss: 0.032795\n","73it [00:02, 28.56it/s]Train epoch: 131 [batch #75, batch_size 16, seq length 806]\tLoss: 0.038715\n","98it [00:03, 28.13it/s]Train epoch: 131 [batch #100, batch_size 16, seq length 892]\tLoss: 0.038409\n","125it [00:04, 26.69it/s]Train epoch: 131 [batch #125, batch_size 16, seq length 978]\tLoss: 0.035705\n","149it [00:05, 25.26it/s]Train epoch: 131 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.040443\n","173it [00:06, 25.65it/s]Train epoch: 131 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.040658\n","200it [00:07, 24.81it/s]Train epoch: 131 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.034691\n","224it [00:08, 25.03it/s]Train epoch: 131 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.037328\n","248it [00:09, 23.86it/s]Train epoch: 131 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.040278\n","275it [00:10, 23.26it/s]Train epoch: 131 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.041995\n","299it [00:11, 21.87it/s]Train epoch: 131 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.044028\n","323it [00:12, 21.99it/s]Train epoch: 131 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.040796\n","350it [00:13, 21.06it/s]Train epoch: 131 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.042553\n","374it [00:14, 21.25it/s]Train epoch: 131 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.048587\n","398it [00:16, 20.31it/s]Train epoch: 131 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.045066\n","425it [00:17, 19.98it/s]Train epoch: 131 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.041457\n","449it [00:18, 18.27it/s]Train epoch: 131 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.045395\n","474it [00:20, 16.97it/s]Train epoch: 131 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.052165\n","500it [00:21, 15.91it/s]Train epoch: 131 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.049762\n","505it [00:22, 22.85it/s]\n","epoch loss: 0.040907850594600975\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 344.94it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3768, 0.5738, 0.4841, 0.5252, 0.8659\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4081, 0.6412, 0.5289, 0.5797, 0.8872\n","rec_at_5: 0.5526\n","prec_at_5: 0.5652\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 132\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 132 [batch #0, batch_size 16, seq length 212]\tLoss: 0.086865\n","23it [00:00, 30.86it/s]Train epoch: 132 [batch #25, batch_size 16, seq length 571]\tLoss: 0.029971\n","47it [00:01, 30.98it/s]Train epoch: 132 [batch #50, batch_size 16, seq length 709]\tLoss: 0.031657\n","73it [00:02, 28.36it/s]Train epoch: 132 [batch #75, batch_size 16, seq length 806]\tLoss: 0.043834\n","100it [00:03, 27.70it/s]Train epoch: 132 [batch #100, batch_size 16, seq length 892]\tLoss: 0.036213\n","124it [00:04, 27.06it/s]Train epoch: 132 [batch #125, batch_size 16, seq length 978]\tLoss: 0.036751\n","148it [00:05, 25.78it/s]Train epoch: 132 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.039189\n","175it [00:06, 25.45it/s]Train epoch: 132 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.035331\n","199it [00:07, 24.53it/s]Train epoch: 132 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.035953\n","223it [00:08, 24.25it/s]Train epoch: 132 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.038379\n","250it [00:09, 24.13it/s]Train epoch: 132 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.041021\n","274it [00:10, 23.11it/s]Train epoch: 132 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.039335\n","298it [00:11, 22.71it/s]Train epoch: 132 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.043825\n","325it [00:12, 22.77it/s]Train epoch: 132 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.039271\n","349it [00:13, 21.90it/s]Train epoch: 132 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.041143\n","373it [00:14, 21.45it/s]Train epoch: 132 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.044892\n","400it [00:16, 20.51it/s]Train epoch: 132 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.045220\n","424it [00:17, 19.49it/s]Train epoch: 132 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.037725\n","449it [00:18, 18.44it/s]Train epoch: 132 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.041947\n","475it [00:20, 17.35it/s]Train epoch: 132 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.048912\n","499it [00:21, 16.35it/s]Train epoch: 132 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.047925\n","505it [00:22, 22.83it/s]\n","epoch loss: 0.03991297357162405\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 352.08it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3772, 0.5729, 0.4860, 0.5259, 0.8654\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4069, 0.6392, 0.5283, 0.5785, 0.8867\n","rec_at_5: 0.5524\n","prec_at_5: 0.5643\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 133\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 133 [batch #0, batch_size 16, seq length 212]\tLoss: 0.068756\n","22it [00:00, 30.12it/s]Train epoch: 133 [batch #25, batch_size 16, seq length 571]\tLoss: 0.030792\n","50it [00:01, 29.25it/s]Train epoch: 133 [batch #50, batch_size 16, seq length 709]\tLoss: 0.029415\n","73it [00:02, 28.50it/s]Train epoch: 133 [batch #75, batch_size 16, seq length 806]\tLoss: 0.037527\n","98it [00:03, 28.49it/s]Train epoch: 133 [batch #100, batch_size 16, seq length 892]\tLoss: 0.036581\n","125it [00:04, 27.37it/s]Train epoch: 133 [batch #125, batch_size 16, seq length 978]\tLoss: 0.035593\n","149it [00:05, 25.67it/s]Train epoch: 133 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.040137\n","173it [00:06, 25.41it/s]Train epoch: 133 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.033818\n","200it [00:07, 25.11it/s]Train epoch: 133 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.032025\n","224it [00:08, 23.84it/s]Train epoch: 133 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.036545\n","248it [00:09, 23.39it/s]Train epoch: 133 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.038767\n","275it [00:10, 22.26it/s]Train epoch: 133 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.039884\n","299it [00:11, 22.09it/s]Train epoch: 133 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.040443\n","323it [00:12, 21.71it/s]Train epoch: 133 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.039496\n","350it [00:13, 22.00it/s]Train epoch: 133 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.040543\n","374it [00:15, 21.21it/s]Train epoch: 133 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.045403\n","398it [00:16, 20.59it/s]Train epoch: 133 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.043919\n","425it [00:17, 19.40it/s]Train epoch: 133 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.037620\n","450it [00:18, 18.42it/s]Train epoch: 133 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.041845\n","474it [00:20, 17.76it/s]Train epoch: 133 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.048192\n","500it [00:21, 15.23it/s]Train epoch: 133 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.047590\n","505it [00:22, 22.71it/s]\n","epoch loss: 0.03892393719570914\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 349.36it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3775, 0.5727, 0.4862, 0.5260, 0.8653\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4071, 0.6385, 0.5290, 0.5786, 0.8869\n","rec_at_5: 0.5525\n","prec_at_5: 0.5644\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 134\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 134 [batch #0, batch_size 16, seq length 212]\tLoss: 0.081930\n","23it [00:00, 30.39it/s]Train epoch: 134 [batch #25, batch_size 16, seq length 571]\tLoss: 0.030720\n","47it [00:01, 30.04it/s]Train epoch: 134 [batch #50, batch_size 16, seq length 709]\tLoss: 0.028405\n","72it [00:02, 28.58it/s]Train epoch: 134 [batch #75, batch_size 16, seq length 806]\tLoss: 0.036448\n","100it [00:03, 27.96it/s]Train epoch: 134 [batch #100, batch_size 16, seq length 892]\tLoss: 0.033262\n","124it [00:04, 27.73it/s]Train epoch: 134 [batch #125, batch_size 16, seq length 978]\tLoss: 0.038602\n","148it [00:05, 26.66it/s]Train epoch: 134 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.038129\n","175it [00:06, 24.81it/s]Train epoch: 134 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.038961\n","199it [00:07, 24.90it/s]Train epoch: 134 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.033285\n","223it [00:08, 23.85it/s]Train epoch: 134 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.035287\n","250it [00:09, 23.29it/s]Train epoch: 134 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.039432\n","274it [00:10, 23.26it/s]Train epoch: 134 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.038230\n","298it [00:11, 22.66it/s]Train epoch: 134 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.039132\n","325it [00:12, 22.04it/s]Train epoch: 134 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.040528\n","349it [00:13, 21.53it/s]Train epoch: 134 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.038754\n","373it [00:14, 20.60it/s]Train epoch: 134 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.043984\n","400it [00:16, 20.57it/s]Train epoch: 134 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.041757\n","425it [00:17, 19.83it/s]Train epoch: 134 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.040207\n","449it [00:18, 18.70it/s]Train epoch: 134 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.042329\n","475it [00:20, 17.44it/s]Train epoch: 134 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.045675\n","499it [00:21, 15.68it/s]Train epoch: 134 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.044644\n","505it [00:22, 22.73it/s]\n","epoch loss: 0.03855714957312782\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 349.97it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3765, 0.5712, 0.4866, 0.5255, 0.8652\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4066, 0.6367, 0.5295, 0.5782, 0.8870\n","rec_at_5: 0.5522\n","prec_at_5: 0.5643\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 135\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 135 [batch #0, batch_size 16, seq length 212]\tLoss: 0.075085\n","23it [00:00, 30.84it/s]Train epoch: 135 [batch #25, batch_size 16, seq length 571]\tLoss: 0.029484\n","47it [00:01, 30.50it/s]Train epoch: 135 [batch #50, batch_size 16, seq length 709]\tLoss: 0.031339\n","74it [00:02, 29.57it/s]Train epoch: 135 [batch #75, batch_size 16, seq length 806]\tLoss: 0.037949\n","100it [00:03, 28.23it/s]Train epoch: 135 [batch #100, batch_size 16, seq length 892]\tLoss: 0.035636\n","124it [00:04, 27.12it/s]Train epoch: 135 [batch #125, batch_size 16, seq length 978]\tLoss: 0.032119\n","148it [00:05, 26.92it/s]Train epoch: 135 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.037499\n","175it [00:06, 25.35it/s]Train epoch: 135 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.033372\n","199it [00:07, 25.77it/s]Train epoch: 135 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.032388\n","223it [00:08, 24.52it/s]Train epoch: 135 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.035030\n","250it [00:09, 23.03it/s]Train epoch: 135 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.037711\n","274it [00:10, 22.82it/s]Train epoch: 135 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.038379\n","298it [00:11, 22.45it/s]Train epoch: 135 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.040384\n","325it [00:12, 21.98it/s]Train epoch: 135 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.039533\n","349it [00:13, 20.64it/s]Train epoch: 135 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.039415\n","373it [00:15, 20.72it/s]Train epoch: 135 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.043769\n","400it [00:16, 20.41it/s]Train epoch: 135 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.042470\n","425it [00:17, 19.34it/s]Train epoch: 135 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.036828\n","450it [00:18, 18.93it/s]Train epoch: 135 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.039830\n","474it [00:20, 17.49it/s]Train epoch: 135 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.044559\n","500it [00:21, 15.67it/s]Train epoch: 135 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.046767\n","505it [00:22, 22.76it/s]\n","epoch loss: 0.03720899491759921\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 346.88it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3774, 0.5710, 0.4870, 0.5257, 0.8651\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4064, 0.6366, 0.5292, 0.5780, 0.8867\n","rec_at_5: 0.5526\n","prec_at_5: 0.5653\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 136\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 136 [batch #0, batch_size 16, seq length 212]\tLoss: 0.074927\n","23it [00:00, 30.59it/s]Train epoch: 136 [batch #25, batch_size 16, seq length 571]\tLoss: 0.027164\n","47it [00:01, 30.85it/s]Train epoch: 136 [batch #50, batch_size 16, seq length 709]\tLoss: 0.028918\n","74it [00:02, 28.61it/s]Train epoch: 136 [batch #75, batch_size 16, seq length 806]\tLoss: 0.038328\n","100it [00:03, 27.93it/s]Train epoch: 136 [batch #100, batch_size 16, seq length 892]\tLoss: 0.032311\n","124it [00:04, 26.54it/s]Train epoch: 136 [batch #125, batch_size 16, seq length 978]\tLoss: 0.034333\n","148it [00:05, 26.42it/s]Train epoch: 136 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.035846\n","175it [00:06, 24.38it/s]Train epoch: 136 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.034596\n","199it [00:07, 25.02it/s]Train epoch: 136 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.033275\n","223it [00:08, 25.12it/s]Train epoch: 136 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.033874\n","250it [00:09, 24.31it/s]Train epoch: 136 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.036335\n","274it [00:10, 22.73it/s]Train epoch: 136 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.038824\n","298it [00:11, 21.73it/s]Train epoch: 136 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.038533\n","325it [00:12, 22.27it/s]Train epoch: 136 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.038799\n","349it [00:13, 21.08it/s]Train epoch: 136 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.038033\n","373it [00:14, 20.97it/s]Train epoch: 136 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.042381\n","400it [00:16, 19.68it/s]Train epoch: 136 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.039406\n","425it [00:17, 19.46it/s]Train epoch: 136 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.037445\n","450it [00:18, 18.25it/s]Train epoch: 136 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.039118\n","474it [00:20, 17.47it/s]Train epoch: 136 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.044744\n","500it [00:21, 16.01it/s]Train epoch: 136 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.042447\n","505it [00:22, 22.76it/s]\n","epoch loss: 0.03665224920755418\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 347.30it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3763, 0.5720, 0.4846, 0.5247, 0.8648\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4055, 0.6365, 0.5276, 0.5770, 0.8861\n","rec_at_5: 0.5497\n","prec_at_5: 0.5635\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 137\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 137 [batch #0, batch_size 16, seq length 212]\tLoss: 0.066185\n","23it [00:00, 31.03it/s]Train epoch: 137 [batch #25, batch_size 16, seq length 571]\tLoss: 0.027884\n","47it [00:01, 30.27it/s]Train epoch: 137 [batch #50, batch_size 16, seq length 709]\tLoss: 0.027720\n","74it [00:02, 28.29it/s]Train epoch: 137 [batch #75, batch_size 16, seq length 806]\tLoss: 0.038657\n","99it [00:03, 28.56it/s]Train epoch: 137 [batch #100, batch_size 16, seq length 892]\tLoss: 0.032886\n","123it [00:04, 26.67it/s]Train epoch: 137 [batch #125, batch_size 16, seq length 978]\tLoss: 0.032194\n","150it [00:05, 25.28it/s]Train epoch: 137 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.036066\n","174it [00:06, 25.50it/s]Train epoch: 137 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.035397\n","198it [00:07, 24.07it/s]Train epoch: 137 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.033738\n","225it [00:08, 23.76it/s]Train epoch: 137 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.034204\n","249it [00:09, 23.23it/s]Train epoch: 137 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.036621\n","273it [00:10, 23.44it/s]Train epoch: 137 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.037915\n","300it [00:11, 22.45it/s]Train epoch: 137 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.036263\n","324it [00:12, 22.62it/s]Train epoch: 137 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.036336\n","348it [00:13, 21.63it/s]Train epoch: 137 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.037361\n","375it [00:15, 20.40it/s]Train epoch: 137 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.042869\n","399it [00:16, 19.81it/s]Train epoch: 137 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.037211\n","424it [00:17, 19.99it/s]Train epoch: 137 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.034497\n","448it [00:18, 18.08it/s]Train epoch: 137 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.037300\n","474it [00:20, 17.61it/s]Train epoch: 137 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.043754\n","500it [00:21, 16.05it/s]Train epoch: 137 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.041116\n","505it [00:22, 22.79it/s]\n","epoch loss: 0.0354657904060539\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 350.68it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3765, 0.5689, 0.4866, 0.5246, 0.8646\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4060, 0.6355, 0.5292, 0.5775, 0.8862\n","rec_at_5: 0.5519\n","prec_at_5: 0.5643\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 138\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 138 [batch #0, batch_size 16, seq length 212]\tLoss: 0.062581\n","23it [00:00, 30.38it/s]Train epoch: 138 [batch #25, batch_size 16, seq length 571]\tLoss: 0.029224\n","50it [00:01, 28.16it/s]Train epoch: 138 [batch #50, batch_size 16, seq length 709]\tLoss: 0.029484\n","75it [00:02, 27.32it/s]Train epoch: 138 [batch #75, batch_size 16, seq length 806]\tLoss: 0.033761\n","100it [00:03, 27.58it/s]Train epoch: 138 [batch #100, batch_size 16, seq length 892]\tLoss: 0.030031\n","124it [00:04, 27.30it/s]Train epoch: 138 [batch #125, batch_size 16, seq length 978]\tLoss: 0.030516\n","148it [00:05, 26.27it/s]Train epoch: 138 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.033959\n","175it [00:06, 25.69it/s]Train epoch: 138 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.030623\n","199it [00:07, 25.00it/s]Train epoch: 138 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.030086\n","223it [00:08, 24.30it/s]Train epoch: 138 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.032176\n","250it [00:09, 24.19it/s]Train epoch: 138 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.036857\n","274it [00:10, 23.08it/s]Train epoch: 138 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.035417\n","298it [00:11, 22.37it/s]Train epoch: 138 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.036777\n","325it [00:12, 22.19it/s]Train epoch: 138 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.039826\n","349it [00:13, 21.64it/s]Train epoch: 138 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.033554\n","373it [00:15, 20.86it/s]Train epoch: 138 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.041220\n","400it [00:16, 20.07it/s]Train epoch: 138 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.036824\n","425it [00:17, 19.60it/s]Train epoch: 138 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.033827\n","450it [00:19, 17.78it/s]Train epoch: 138 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.034846\n","474it [00:20, 17.34it/s]Train epoch: 138 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.042515\n","500it [00:21, 15.34it/s]Train epoch: 138 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.040761\n","505it [00:22, 22.66it/s]\n","epoch loss: 0.03439538328548764\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 348.88it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3761, 0.5666, 0.4877, 0.5242, 0.8644\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4059, 0.6333, 0.5306, 0.5775, 0.8860\n","rec_at_5: 0.5500\n","prec_at_5: 0.5625\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 139\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 139 [batch #0, batch_size 16, seq length 212]\tLoss: 0.069192\n","23it [00:00, 30.54it/s]Train epoch: 139 [batch #25, batch_size 16, seq length 571]\tLoss: 0.025734\n","49it [00:01, 29.05it/s]Train epoch: 139 [batch #50, batch_size 16, seq length 709]\tLoss: 0.029400\n","74it [00:02, 28.33it/s]Train epoch: 139 [batch #75, batch_size 16, seq length 806]\tLoss: 0.035449\n","99it [00:03, 28.01it/s]Train epoch: 139 [batch #100, batch_size 16, seq length 892]\tLoss: 0.031362\n","123it [00:04, 26.66it/s]Train epoch: 139 [batch #125, batch_size 16, seq length 978]\tLoss: 0.032558\n","150it [00:05, 25.77it/s]Train epoch: 139 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.031849\n","174it [00:06, 24.74it/s]Train epoch: 139 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.029386\n","198it [00:07, 24.32it/s]Train epoch: 139 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.028191\n","225it [00:08, 25.04it/s]Train epoch: 139 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.030517\n","249it [00:09, 23.60it/s]Train epoch: 139 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.036487\n","273it [00:10, 22.80it/s]Train epoch: 139 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.035938\n","300it [00:11, 22.58it/s]Train epoch: 139 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.036822\n","324it [00:12, 22.49it/s]Train epoch: 139 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.033951\n","348it [00:13, 21.90it/s]Train epoch: 139 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.035687\n","375it [00:14, 20.95it/s]Train epoch: 139 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.040596\n","399it [00:16, 20.26it/s]Train epoch: 139 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.035742\n","423it [00:17, 19.37it/s]Train epoch: 139 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.033950\n","448it [00:18, 18.11it/s]Train epoch: 139 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.034914\n","475it [00:20, 17.14it/s]Train epoch: 139 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.040336\n","499it [00:21, 16.18it/s]Train epoch: 139 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.039413\n","505it [00:22, 22.83it/s]\n","epoch loss: 0.03362851123035065\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 347.56it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3756, 0.5690, 0.4850, 0.5237, 0.8641\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4047, 0.6353, 0.5272, 0.5762, 0.8856\n","rec_at_5: 0.5496\n","prec_at_5: 0.5625\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 140\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 140 [batch #0, batch_size 16, seq length 212]\tLoss: 0.065528\n","23it [00:00, 30.83it/s]Train epoch: 140 [batch #25, batch_size 16, seq length 571]\tLoss: 0.027174\n","48it [00:01, 28.56it/s]Train epoch: 140 [batch #50, batch_size 16, seq length 709]\tLoss: 0.028394\n","73it [00:02, 28.12it/s]Train epoch: 140 [batch #75, batch_size 16, seq length 806]\tLoss: 0.034769\n","99it [00:03, 26.55it/s]Train epoch: 140 [batch #100, batch_size 16, seq length 892]\tLoss: 0.030508\n","123it [00:04, 27.68it/s]Train epoch: 140 [batch #125, batch_size 16, seq length 978]\tLoss: 0.032129\n","150it [00:05, 25.59it/s]Train epoch: 140 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.034358\n","174it [00:06, 24.77it/s]Train epoch: 140 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.031073\n","198it [00:07, 25.80it/s]Train epoch: 140 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.030436\n","225it [00:08, 24.45it/s]Train epoch: 140 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.027580\n","249it [00:09, 23.38it/s]Train epoch: 140 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.030996\n","273it [00:10, 23.35it/s]Train epoch: 140 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.033772\n","300it [00:11, 23.30it/s]Train epoch: 140 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.035956\n","324it [00:12, 22.41it/s]Train epoch: 140 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.035735\n","348it [00:13, 21.63it/s]Train epoch: 140 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.033068\n","375it [00:15, 21.21it/s]Train epoch: 140 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.037466\n","399it [00:16, 19.77it/s]Train epoch: 140 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.038301\n","424it [00:17, 19.83it/s]Train epoch: 140 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.030787\n","450it [00:18, 18.51it/s]Train epoch: 140 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.035700\n","474it [00:20, 17.16it/s]Train epoch: 140 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.043243\n","500it [00:21, 15.75it/s]Train epoch: 140 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.037416\n","505it [00:22, 22.62it/s]\n","epoch loss: 0.0330868979920587\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 346.32it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3744, 0.5694, 0.4834, 0.5229, 0.8642\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4054, 0.6354, 0.5283, 0.5769, 0.8856\n","rec_at_5: 0.5505\n","prec_at_5: 0.5631\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 141\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 141 [batch #0, batch_size 16, seq length 212]\tLoss: 0.072292\n","22it [00:00, 30.63it/s]Train epoch: 141 [batch #25, batch_size 16, seq length 571]\tLoss: 0.026175\n","50it [00:01, 29.19it/s]Train epoch: 141 [batch #50, batch_size 16, seq length 709]\tLoss: 0.028057\n","73it [00:02, 28.15it/s]Train epoch: 141 [batch #75, batch_size 16, seq length 806]\tLoss: 0.036144\n","97it [00:03, 27.16it/s]Train epoch: 141 [batch #100, batch_size 16, seq length 892]\tLoss: 0.027540\n","125it [00:04, 26.43it/s]Train epoch: 141 [batch #125, batch_size 16, seq length 978]\tLoss: 0.031243\n","149it [00:05, 25.89it/s]Train epoch: 141 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.032927\n","173it [00:06, 25.76it/s]Train epoch: 141 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.030085\n","200it [00:07, 25.75it/s]Train epoch: 141 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.030109\n","224it [00:08, 24.18it/s]Train epoch: 141 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.030871\n","248it [00:09, 23.48it/s]Train epoch: 141 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.033735\n","275it [00:10, 22.73it/s]Train epoch: 141 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.034289\n","299it [00:11, 23.07it/s]Train epoch: 141 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.033550\n","323it [00:12, 21.94it/s]Train epoch: 141 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.034617\n","350it [00:13, 21.75it/s]Train epoch: 141 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.035003\n","374it [00:15, 19.93it/s]Train epoch: 141 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.037813\n","398it [00:16, 20.30it/s]Train epoch: 141 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.036791\n","425it [00:17, 19.75it/s]Train epoch: 141 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.031683\n","450it [00:19, 18.65it/s]Train epoch: 141 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.036545\n","474it [00:20, 17.27it/s]Train epoch: 141 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.040174\n","500it [00:22, 15.34it/s]Train epoch: 141 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.037073\n","505it [00:22, 22.65it/s]\n","epoch loss: 0.03245455689894238\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 345.21it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3744, 0.5694, 0.4828, 0.5225, 0.8638\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4044, 0.6349, 0.5269, 0.5759, 0.8854\n","rec_at_5: 0.5500\n","prec_at_5: 0.5629\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 142\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 142 [batch #0, batch_size 16, seq length 212]\tLoss: 0.057143\n","23it [00:00, 30.08it/s]Train epoch: 142 [batch #25, batch_size 16, seq length 571]\tLoss: 0.024260\n","48it [00:01, 28.52it/s]Train epoch: 142 [batch #50, batch_size 16, seq length 709]\tLoss: 0.025677\n","73it [00:02, 28.28it/s]Train epoch: 142 [batch #75, batch_size 16, seq length 806]\tLoss: 0.033134\n","98it [00:03, 27.30it/s]Train epoch: 142 [batch #100, batch_size 16, seq length 892]\tLoss: 0.027915\n","123it [00:04, 27.63it/s]Train epoch: 142 [batch #125, batch_size 16, seq length 978]\tLoss: 0.029521\n","150it [00:05, 26.26it/s]Train epoch: 142 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.029601\n","174it [00:06, 25.29it/s]Train epoch: 142 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.029537\n","198it [00:07, 24.45it/s]Train epoch: 142 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.029628\n","225it [00:08, 22.77it/s]Train epoch: 142 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.030546\n","249it [00:09, 23.35it/s]Train epoch: 142 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.030691\n","273it [00:10, 23.40it/s]Train epoch: 142 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.032610\n","300it [00:11, 22.35it/s]Train epoch: 142 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.034074\n","324it [00:12, 22.12it/s]Train epoch: 142 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.033740\n","348it [00:13, 21.40it/s]Train epoch: 142 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.032181\n","375it [00:15, 20.34it/s]Train epoch: 142 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.036527\n","399it [00:16, 20.68it/s]Train epoch: 142 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.034464\n","423it [00:17, 20.11it/s]Train epoch: 142 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.029100\n","449it [00:18, 17.88it/s]Train epoch: 142 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.033243\n","475it [00:20, 17.94it/s]Train epoch: 142 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.038390\n","499it [00:21, 15.43it/s]Train epoch: 142 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.036834\n","505it [00:22, 22.71it/s]\n","epoch loss: 0.03133469552613131\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 339.41it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3751, 0.5658, 0.4859, 0.5228, 0.8636\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4039, 0.6312, 0.5286, 0.5754, 0.8851\n","rec_at_5: 0.5495\n","prec_at_5: 0.5622\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 143\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 143 [batch #0, batch_size 16, seq length 212]\tLoss: 0.052512\n","22it [00:00, 29.24it/s]Train epoch: 143 [batch #25, batch_size 16, seq length 571]\tLoss: 0.024589\n","49it [00:01, 27.74it/s]Train epoch: 143 [batch #50, batch_size 16, seq length 709]\tLoss: 0.027081\n","75it [00:02, 27.96it/s]Train epoch: 143 [batch #75, batch_size 16, seq length 806]\tLoss: 0.033463\n","99it [00:03, 27.63it/s]Train epoch: 143 [batch #100, batch_size 16, seq length 892]\tLoss: 0.027475\n","123it [00:04, 26.97it/s]Train epoch: 143 [batch #125, batch_size 16, seq length 978]\tLoss: 0.027955\n","150it [00:05, 25.56it/s]Train epoch: 143 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.029965\n","174it [00:06, 24.72it/s]Train epoch: 143 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.028584\n","198it [00:07, 25.53it/s]Train epoch: 143 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.026174\n","225it [00:08, 24.46it/s]Train epoch: 143 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.028548\n","249it [00:09, 23.30it/s]Train epoch: 143 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.031642\n","273it [00:10, 23.76it/s]Train epoch: 143 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.032183\n","300it [00:11, 22.07it/s]Train epoch: 143 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.032481\n","324it [00:12, 21.68it/s]Train epoch: 143 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.030436\n","348it [00:13, 20.94it/s]Train epoch: 143 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.031578\n","375it [00:15, 21.20it/s]Train epoch: 143 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.035461\n","399it [00:16, 19.90it/s]Train epoch: 143 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.032314\n","423it [00:17, 20.21it/s]Train epoch: 143 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.030514\n","449it [00:18, 17.99it/s]Train epoch: 143 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.031530\n","475it [00:20, 17.85it/s]Train epoch: 143 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.036498\n","499it [00:21, 15.50it/s]Train epoch: 143 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.032618\n","505it [00:22, 22.65it/s]\n","epoch loss: 0.030793103205021655\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 346.32it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3738, 0.5671, 0.4833, 0.5218, 0.8635\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4022, 0.6328, 0.5247, 0.5737, 0.8849\n","rec_at_5: 0.5493\n","prec_at_5: 0.5615\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 144\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 144 [batch #0, batch_size 16, seq length 212]\tLoss: 0.059365\n","25it [00:00, 30.15it/s]Train epoch: 144 [batch #25, batch_size 16, seq length 571]\tLoss: 0.024004\n","49it [00:01, 30.08it/s]Train epoch: 144 [batch #50, batch_size 16, seq length 709]\tLoss: 0.023631\n","73it [00:02, 29.11it/s]Train epoch: 144 [batch #75, batch_size 16, seq length 806]\tLoss: 0.033126\n","98it [00:03, 26.23it/s]Train epoch: 144 [batch #100, batch_size 16, seq length 892]\tLoss: 0.028745\n","125it [00:04, 27.28it/s]Train epoch: 144 [batch #125, batch_size 16, seq length 978]\tLoss: 0.028546\n","149it [00:05, 26.68it/s]Train epoch: 144 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.029957\n","173it [00:06, 25.49it/s]Train epoch: 144 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.030479\n","200it [00:07, 25.02it/s]Train epoch: 144 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.027127\n","224it [00:08, 23.67it/s]Train epoch: 144 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.027346\n","248it [00:09, 23.10it/s]Train epoch: 144 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.029755\n","275it [00:10, 23.78it/s]Train epoch: 144 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.030406\n","299it [00:11, 22.18it/s]Train epoch: 144 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.030929\n","323it [00:12, 22.33it/s]Train epoch: 144 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.031389\n","350it [00:13, 22.23it/s]Train epoch: 144 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.031458\n","374it [00:14, 21.19it/s]Train epoch: 144 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.032233\n","398it [00:16, 20.50it/s]Train epoch: 144 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.031286\n","424it [00:17, 19.64it/s]Train epoch: 144 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.028736\n","450it [00:18, 18.57it/s]Train epoch: 144 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.031891\n","474it [00:20, 17.53it/s]Train epoch: 144 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.036871\n","500it [00:21, 15.71it/s]Train epoch: 144 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.034849\n","505it [00:22, 22.79it/s]\n","epoch loss: 0.029604872466424608\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 350.85it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3734, 0.5646, 0.4839, 0.5211, 0.8633\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4033, 0.6306, 0.5281, 0.5748, 0.8849\n","rec_at_5: 0.5499\n","prec_at_5: 0.5627\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 145\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 145 [batch #0, batch_size 16, seq length 212]\tLoss: 0.069467\n","23it [00:00, 30.38it/s]Train epoch: 145 [batch #25, batch_size 16, seq length 571]\tLoss: 0.023478\n","48it [00:01, 28.52it/s]Train epoch: 145 [batch #50, batch_size 16, seq length 709]\tLoss: 0.021578\n","74it [00:02, 28.65it/s]Train epoch: 145 [batch #75, batch_size 16, seq length 806]\tLoss: 0.032496\n","100it [00:03, 28.04it/s]Train epoch: 145 [batch #100, batch_size 16, seq length 892]\tLoss: 0.026319\n","124it [00:04, 26.56it/s]Train epoch: 145 [batch #125, batch_size 16, seq length 978]\tLoss: 0.025248\n","148it [00:05, 26.38it/s]Train epoch: 145 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.029083\n","175it [00:06, 25.30it/s]Train epoch: 145 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.029453\n","199it [00:07, 25.16it/s]Train epoch: 145 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.028557\n","223it [00:08, 24.10it/s]Train epoch: 145 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.026255\n","250it [00:09, 24.23it/s]Train epoch: 145 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.027134\n","274it [00:10, 23.04it/s]Train epoch: 145 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.030652\n","298it [00:11, 22.33it/s]Train epoch: 145 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.031826\n","325it [00:12, 22.20it/s]Train epoch: 145 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.031053\n","349it [00:13, 21.47it/s]Train epoch: 145 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.031077\n","373it [00:14, 20.13it/s]Train epoch: 145 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.034828\n","400it [00:16, 20.56it/s]Train epoch: 145 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.033079\n","424it [00:17, 20.01it/s]Train epoch: 145 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.028908\n","449it [00:18, 18.72it/s]Train epoch: 145 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.029938\n","474it [00:20, 17.98it/s]Train epoch: 145 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.035522\n","500it [00:21, 16.15it/s]Train epoch: 145 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.032313\n","505it [00:22, 22.81it/s]\n","epoch loss: 0.02897034440798187\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 351.14it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3724, 0.5646, 0.4809, 0.5194, 0.8630\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4020, 0.6325, 0.5246, 0.5735, 0.8845\n","rec_at_5: 0.5478\n","prec_at_5: 0.5612\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 146\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 146 [batch #0, batch_size 16, seq length 212]\tLoss: 0.068312\n","25it [00:00, 30.59it/s]Train epoch: 146 [batch #25, batch_size 16, seq length 571]\tLoss: 0.021871\n","49it [00:01, 30.10it/s]Train epoch: 146 [batch #50, batch_size 16, seq length 709]\tLoss: 0.023691\n","75it [00:02, 27.29it/s]Train epoch: 146 [batch #75, batch_size 16, seq length 806]\tLoss: 0.029804\n","98it [00:03, 27.10it/s]Train epoch: 146 [batch #100, batch_size 16, seq length 892]\tLoss: 0.024775\n","125it [00:04, 27.06it/s]Train epoch: 146 [batch #125, batch_size 16, seq length 978]\tLoss: 0.027732\n","149it [00:05, 26.13it/s]Train epoch: 146 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.028336\n","173it [00:06, 25.68it/s]Train epoch: 146 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.026536\n","200it [00:07, 24.06it/s]Train epoch: 146 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.028416\n","224it [00:08, 23.83it/s]Train epoch: 146 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.026337\n","248it [00:09, 24.50it/s]Train epoch: 146 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.029410\n","275it [00:10, 23.28it/s]Train epoch: 146 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.029212\n","299it [00:11, 22.53it/s]Train epoch: 146 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.028405\n","323it [00:12, 22.05it/s]Train epoch: 146 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.029315\n","350it [00:13, 22.09it/s]Train epoch: 146 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.028819\n","374it [00:15, 20.09it/s]Train epoch: 146 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.031332\n","398it [00:16, 20.45it/s]Train epoch: 146 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.031936\n","424it [00:17, 19.20it/s]Train epoch: 146 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.026940\n","449it [00:18, 18.25it/s]Train epoch: 146 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.029257\n","475it [00:20, 17.33it/s]Train epoch: 146 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.034865\n","499it [00:21, 15.97it/s]Train epoch: 146 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.033287\n","505it [00:22, 22.70it/s]\n","epoch loss: 0.028554911489626116\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 349.73it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3710, 0.5631, 0.4806, 0.5186, 0.8627\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4012, 0.6299, 0.5249, 0.5727, 0.8845\n","rec_at_5: 0.5473\n","prec_at_5: 0.5603\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 147\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 147 [batch #0, batch_size 16, seq length 212]\tLoss: 0.056783\n","22it [00:00, 30.46it/s]Train epoch: 147 [batch #25, batch_size 16, seq length 571]\tLoss: 0.020154\n","50it [00:01, 28.68it/s]Train epoch: 147 [batch #50, batch_size 16, seq length 709]\tLoss: 0.022268\n","73it [00:02, 28.76it/s]Train epoch: 147 [batch #75, batch_size 16, seq length 806]\tLoss: 0.028641\n","98it [00:03, 27.70it/s]Train epoch: 147 [batch #100, batch_size 16, seq length 892]\tLoss: 0.025828\n","125it [00:04, 27.31it/s]Train epoch: 147 [batch #125, batch_size 16, seq length 978]\tLoss: 0.024022\n","149it [00:05, 26.09it/s]Train epoch: 147 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.029966\n","173it [00:06, 25.14it/s]Train epoch: 147 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.027761\n","200it [00:07, 24.92it/s]Train epoch: 147 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.026114\n","224it [00:08, 24.09it/s]Train epoch: 147 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.026078\n","248it [00:09, 23.97it/s]Train epoch: 147 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.027152\n","275it [00:10, 23.95it/s]Train epoch: 147 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.027981\n","299it [00:11, 22.55it/s]Train epoch: 147 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.029389\n","323it [00:12, 22.36it/s]Train epoch: 147 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.028068\n","350it [00:13, 21.34it/s]Train epoch: 147 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.030915\n","374it [00:15, 20.30it/s]Train epoch: 147 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.033500\n","398it [00:16, 19.78it/s]Train epoch: 147 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.028237\n","425it [00:17, 19.99it/s]Train epoch: 147 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.026594\n","449it [00:18, 18.49it/s]Train epoch: 147 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.028593\n","474it [00:20, 16.83it/s]Train epoch: 147 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.032059\n","500it [00:21, 15.25it/s]Train epoch: 147 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.031695\n","505it [00:22, 22.70it/s]\n","epoch loss: 0.0276662260533707\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 349.09it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3708, 0.5601, 0.4815, 0.5178, 0.8625\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3998, 0.6258, 0.5255, 0.5713, 0.8841\n","rec_at_5: 0.5483\n","prec_at_5: 0.5612\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 148\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 148 [batch #0, batch_size 16, seq length 212]\tLoss: 0.050638\n","22it [00:00, 30.79it/s]Train epoch: 148 [batch #25, batch_size 16, seq length 571]\tLoss: 0.022317\n","49it [00:01, 29.16it/s]Train epoch: 148 [batch #50, batch_size 16, seq length 709]\tLoss: 0.023521\n","72it [00:02, 27.80it/s]Train epoch: 148 [batch #75, batch_size 16, seq length 806]\tLoss: 0.027341\n","98it [00:03, 27.48it/s]Train epoch: 148 [batch #100, batch_size 16, seq length 892]\tLoss: 0.024224\n","123it [00:04, 26.35it/s]Train epoch: 148 [batch #125, batch_size 16, seq length 978]\tLoss: 0.027340\n","150it [00:05, 25.53it/s]Train epoch: 148 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.026495\n","174it [00:06, 24.93it/s]Train epoch: 148 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.026722\n","198it [00:07, 25.12it/s]Train epoch: 148 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.024886\n","225it [00:08, 24.04it/s]Train epoch: 148 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.025746\n","249it [00:09, 24.08it/s]Train epoch: 148 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.028716\n","273it [00:10, 23.37it/s]Train epoch: 148 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.026083\n","300it [00:11, 22.65it/s]Train epoch: 148 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.028962\n","324it [00:12, 21.69it/s]Train epoch: 148 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.028232\n","348it [00:13, 21.23it/s]Train epoch: 148 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.027239\n","375it [00:15, 20.51it/s]Train epoch: 148 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.031848\n","398it [00:16, 19.91it/s]Train epoch: 148 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.029335\n","423it [00:17, 19.42it/s]Train epoch: 148 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.026281\n","450it [00:18, 17.90it/s]Train epoch: 148 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.026478\n","474it [00:20, 17.61it/s]Train epoch: 148 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.033222\n","500it [00:22, 14.90it/s]Train epoch: 148 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.029904\n","505it [00:22, 22.64it/s]\n","epoch loss: 0.02691896873650759\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 345.24it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3705, 0.5627, 0.4786, 0.5173, 0.8624\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4000, 0.6294, 0.5233, 0.5715, 0.8842\n","rec_at_5: 0.5476\n","prec_at_5: 0.5606\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 149\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 149 [batch #0, batch_size 16, seq length 212]\tLoss: 0.043778\n","22it [00:00, 31.03it/s]Train epoch: 149 [batch #25, batch_size 16, seq length 571]\tLoss: 0.022477\n","50it [00:01, 29.24it/s]Train epoch: 149 [batch #50, batch_size 16, seq length 709]\tLoss: 0.021175\n","74it [00:02, 27.89it/s]Train epoch: 149 [batch #75, batch_size 16, seq length 806]\tLoss: 0.029849\n","98it [00:03, 28.89it/s]Train epoch: 149 [batch #100, batch_size 16, seq length 892]\tLoss: 0.022905\n","125it [00:04, 26.90it/s]Train epoch: 149 [batch #125, batch_size 16, seq length 978]\tLoss: 0.027195\n","149it [00:05, 25.81it/s]Train epoch: 149 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.027792\n","173it [00:06, 25.97it/s]Train epoch: 149 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.022786\n","200it [00:07, 24.46it/s]Train epoch: 149 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.021388\n","224it [00:08, 24.79it/s]Train epoch: 149 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.023423\n","248it [00:09, 23.49it/s]Train epoch: 149 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.028152\n","275it [00:10, 23.91it/s]Train epoch: 149 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.029266\n","299it [00:11, 22.71it/s]Train epoch: 149 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.028138\n","323it [00:12, 22.09it/s]Train epoch: 149 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.030125\n","350it [00:13, 21.00it/s]Train epoch: 149 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.027186\n","374it [00:14, 21.22it/s]Train epoch: 149 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.030437\n","398it [00:16, 20.10it/s]Train epoch: 149 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.027178\n","424it [00:17, 19.68it/s]Train epoch: 149 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.025435\n","450it [00:18, 17.93it/s]Train epoch: 149 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.028977\n","474it [00:20, 17.87it/s]Train epoch: 149 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.030161\n","500it [00:21, 15.75it/s]Train epoch: 149 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.028299\n","505it [00:22, 22.82it/s]\n","epoch loss: 0.02646979457758699\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 346.37it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3701, 0.5596, 0.4804, 0.5170, 0.8619\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3990, 0.6244, 0.5249, 0.5704, 0.8840\n","rec_at_5: 0.5444\n","prec_at_5: 0.5579\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 150\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 150 [batch #0, batch_size 16, seq length 212]\tLoss: 0.043401\n","23it [00:00, 31.28it/s]Train epoch: 150 [batch #25, batch_size 16, seq length 571]\tLoss: 0.023211\n","47it [00:01, 29.77it/s]Train epoch: 150 [batch #50, batch_size 16, seq length 709]\tLoss: 0.023310\n","74it [00:02, 28.58it/s]Train epoch: 150 [batch #75, batch_size 16, seq length 806]\tLoss: 0.026110\n","99it [00:03, 27.57it/s]Train epoch: 150 [batch #100, batch_size 16, seq length 892]\tLoss: 0.025257\n","123it [00:04, 27.35it/s]Train epoch: 150 [batch #125, batch_size 16, seq length 978]\tLoss: 0.022579\n","150it [00:05, 26.88it/s]Train epoch: 150 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.024301\n","174it [00:06, 25.77it/s]Train epoch: 150 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.024282\n","198it [00:07, 25.26it/s]Train epoch: 150 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.023979\n","225it [00:08, 23.74it/s]Train epoch: 150 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.022700\n","249it [00:09, 23.25it/s]Train epoch: 150 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.029710\n","273it [00:10, 24.03it/s]Train epoch: 150 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.027491\n","300it [00:11, 22.49it/s]Train epoch: 150 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.026868\n","324it [00:12, 22.39it/s]Train epoch: 150 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.027144\n","348it [00:13, 22.20it/s]Train epoch: 150 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.028836\n","375it [00:15, 20.78it/s]Train epoch: 150 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.029056\n","399it [00:16, 21.11it/s]Train epoch: 150 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.026312\n","425it [00:17, 19.02it/s]Train epoch: 150 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.024941\n","450it [00:18, 18.69it/s]Train epoch: 150 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.024945\n","474it [00:20, 17.49it/s]Train epoch: 150 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.032694\n","500it [00:21, 15.29it/s]Train epoch: 150 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.028004\n","505it [00:22, 22.80it/s]\n","epoch loss: 0.025853418336148456\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 347.61it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3694, 0.5589, 0.4796, 0.5162, 0.8618\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3987, 0.6260, 0.5233, 0.5701, 0.8836\n","rec_at_5: 0.5449\n","prec_at_5: 0.5587\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 151\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 151 [batch #0, batch_size 16, seq length 212]\tLoss: 0.057120\n","23it [00:00, 30.83it/s]Train epoch: 151 [batch #25, batch_size 16, seq length 571]\tLoss: 0.017843\n","50it [00:01, 28.37it/s]Train epoch: 151 [batch #50, batch_size 16, seq length 709]\tLoss: 0.021034\n","74it [00:02, 28.16it/s]Train epoch: 151 [batch #75, batch_size 16, seq length 806]\tLoss: 0.022341\n","98it [00:03, 26.92it/s]Train epoch: 151 [batch #100, batch_size 16, seq length 892]\tLoss: 0.024972\n","125it [00:04, 27.61it/s]Train epoch: 151 [batch #125, batch_size 16, seq length 978]\tLoss: 0.023283\n","149it [00:05, 26.61it/s]Train epoch: 151 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.025075\n","173it [00:06, 25.04it/s]Train epoch: 151 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.021362\n","200it [00:07, 25.82it/s]Train epoch: 151 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.023669\n","224it [00:08, 24.44it/s]Train epoch: 151 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.023595\n","248it [00:09, 23.21it/s]Train epoch: 151 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.026649\n","275it [00:10, 23.26it/s]Train epoch: 151 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.025689\n","299it [00:11, 21.54it/s]Train epoch: 151 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.024724\n","323it [00:12, 22.25it/s]Train epoch: 151 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.025430\n","350it [00:13, 21.01it/s]Train epoch: 151 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.026676\n","374it [00:15, 21.54it/s]Train epoch: 151 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.030679\n","398it [00:16, 20.34it/s]Train epoch: 151 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.025382\n","425it [00:17, 19.23it/s]Train epoch: 151 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.024832\n","450it [00:18, 18.91it/s]Train epoch: 151 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.024643\n","474it [00:20, 17.23it/s]Train epoch: 151 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.030677\n","500it [00:21, 15.26it/s]Train epoch: 151 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.027195\n","505it [00:22, 22.67it/s]\n","epoch loss: 0.02485667012486662\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 347.73it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3707, 0.5599, 0.4805, 0.5172, 0.8617\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3996, 0.6272, 0.5241, 0.5710, 0.8835\n","rec_at_5: 0.5442\n","prec_at_5: 0.5578\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 152\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 152 [batch #0, batch_size 16, seq length 212]\tLoss: 0.040645\n","22it [00:00, 31.19it/s]Train epoch: 152 [batch #25, batch_size 16, seq length 571]\tLoss: 0.020740\n","49it [00:01, 29.26it/s]Train epoch: 152 [batch #50, batch_size 16, seq length 709]\tLoss: 0.020836\n","75it [00:02, 28.04it/s]Train epoch: 152 [batch #75, batch_size 16, seq length 806]\tLoss: 0.027489\n","98it [00:03, 27.94it/s]Train epoch: 152 [batch #100, batch_size 16, seq length 892]\tLoss: 0.024171\n","125it [00:04, 26.56it/s]Train epoch: 152 [batch #125, batch_size 16, seq length 978]\tLoss: 0.022996\n","149it [00:05, 26.51it/s]Train epoch: 152 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.026508\n","173it [00:06, 25.53it/s]Train epoch: 152 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.021911\n","200it [00:07, 24.23it/s]Train epoch: 152 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.022724\n","224it [00:08, 23.58it/s]Train epoch: 152 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.022548\n","248it [00:09, 24.01it/s]Train epoch: 152 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.024817\n","275it [00:10, 23.49it/s]Train epoch: 152 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.024668\n","299it [00:11, 22.45it/s]Train epoch: 152 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.025813\n","323it [00:12, 21.58it/s]Train epoch: 152 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.023633\n","350it [00:13, 21.63it/s]Train epoch: 152 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.026628\n","374it [00:15, 20.74it/s]Train epoch: 152 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.027997\n","398it [00:16, 20.34it/s]Train epoch: 152 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.028432\n","424it [00:17, 19.37it/s]Train epoch: 152 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.023587\n","449it [00:18, 18.17it/s]Train epoch: 152 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.026291\n","475it [00:20, 17.12it/s]Train epoch: 152 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.029758\n","499it [00:22, 15.07it/s]Train epoch: 152 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.026251\n","505it [00:22, 22.53it/s]\n","epoch loss: 0.02466112063139548\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 335.34it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3704, 0.5583, 0.4810, 0.5168, 0.8617\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3997, 0.6257, 0.5253, 0.5711, 0.8836\n","rec_at_5: 0.5427\n","prec_at_5: 0.5569\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 153\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 153 [batch #0, batch_size 16, seq length 212]\tLoss: 0.055157\n","24it [00:00, 29.15it/s]Train epoch: 153 [batch #25, batch_size 16, seq length 571]\tLoss: 0.021315\n","48it [00:01, 29.56it/s]Train epoch: 153 [batch #50, batch_size 16, seq length 709]\tLoss: 0.020080\n","75it [00:02, 27.51it/s]Train epoch: 153 [batch #75, batch_size 16, seq length 806]\tLoss: 0.026826\n","99it [00:03, 27.70it/s]Train epoch: 153 [batch #100, batch_size 16, seq length 892]\tLoss: 0.022646\n","123it [00:04, 26.61it/s]Train epoch: 153 [batch #125, batch_size 16, seq length 978]\tLoss: 0.020597\n","150it [00:05, 24.85it/s]Train epoch: 153 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.022855\n","174it [00:06, 25.65it/s]Train epoch: 153 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.022691\n","198it [00:07, 24.28it/s]Train epoch: 153 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.022834\n","225it [00:08, 22.91it/s]Train epoch: 153 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.022827\n","249it [00:09, 22.49it/s]Train epoch: 153 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.026457\n","273it [00:10, 22.63it/s]Train epoch: 153 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.023207\n","300it [00:11, 22.40it/s]Train epoch: 153 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.026029\n","324it [00:13, 21.64it/s]Train epoch: 153 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.024702\n","348it [00:14, 21.07it/s]Train epoch: 153 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.024644\n","375it [00:15, 20.14it/s]Train epoch: 153 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.027033\n","399it [00:16, 20.37it/s]Train epoch: 153 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.025079\n","423it [00:17, 19.17it/s]Train epoch: 153 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.023996\n","449it [00:19, 17.30it/s]Train epoch: 153 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.025402\n","475it [00:20, 17.25it/s]Train epoch: 153 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.027808\n","499it [00:22, 14.20it/s]Train epoch: 153 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.027072\n","505it [00:22, 22.16it/s]\n","epoch loss: 0.02393415434203009\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 342.20it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3711, 0.5584, 0.4830, 0.5180, 0.8614\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4011, 0.6259, 0.5276, 0.5726, 0.8836\n","rec_at_5: 0.5440\n","prec_at_5: 0.5584\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 154\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 154 [batch #0, batch_size 16, seq length 212]\tLoss: 0.051735\n","23it [00:00, 30.57it/s]Train epoch: 154 [batch #25, batch_size 16, seq length 571]\tLoss: 0.018922\n","50it [00:01, 28.02it/s]Train epoch: 154 [batch #50, batch_size 16, seq length 709]\tLoss: 0.021309\n","75it [00:02, 27.83it/s]Train epoch: 154 [batch #75, batch_size 16, seq length 806]\tLoss: 0.024859\n","99it [00:03, 27.99it/s]Train epoch: 154 [batch #100, batch_size 16, seq length 892]\tLoss: 0.021241\n","123it [00:04, 27.29it/s]Train epoch: 154 [batch #125, batch_size 16, seq length 978]\tLoss: 0.022925\n","150it [00:05, 26.07it/s]Train epoch: 154 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.022745\n","174it [00:06, 25.91it/s]Train epoch: 154 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.020629\n","198it [00:07, 24.41it/s]Train epoch: 154 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.019920\n","225it [00:08, 24.69it/s]Train epoch: 154 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.020785\n","249it [00:09, 22.95it/s]Train epoch: 154 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.025559\n","273it [00:10, 22.31it/s]Train epoch: 154 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.023971\n","300it [00:11, 21.64it/s]Train epoch: 154 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.026526\n","324it [00:12, 21.29it/s]Train epoch: 154 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.021810\n","348it [00:14, 19.94it/s]Train epoch: 154 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.023566\n","375it [00:15, 20.44it/s]Train epoch: 154 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.024326\n","399it [00:16, 19.97it/s]Train epoch: 154 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.022802\n","425it [00:17, 18.95it/s]Train epoch: 154 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.020068\n","450it [00:19, 17.96it/s]Train epoch: 154 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.022517\n","474it [00:20, 17.35it/s]Train epoch: 154 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.028553\n","500it [00:22, 14.60it/s]Train epoch: 154 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.026333\n","505it [00:22, 22.40it/s]\n","epoch loss: 0.02308548519060495\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 344.92it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3697, 0.5560, 0.4808, 0.5157, 0.8612\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3998, 0.6236, 0.5269, 0.5712, 0.8834\n","rec_at_5: 0.5439\n","prec_at_5: 0.5574\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 155\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 155 [batch #0, batch_size 16, seq length 212]\tLoss: 0.040454\n","23it [00:00, 31.42it/s]Train epoch: 155 [batch #25, batch_size 16, seq length 571]\tLoss: 0.018250\n","47it [00:01, 30.13it/s]Train epoch: 155 [batch #50, batch_size 16, seq length 709]\tLoss: 0.022866\n","75it [00:02, 28.61it/s]Train epoch: 155 [batch #75, batch_size 16, seq length 806]\tLoss: 0.024495\n","98it [00:03, 27.75it/s]Train epoch: 155 [batch #100, batch_size 16, seq length 892]\tLoss: 0.021028\n","123it [00:04, 27.10it/s]Train epoch: 155 [batch #125, batch_size 16, seq length 978]\tLoss: 0.019264\n","150it [00:05, 26.05it/s]Train epoch: 155 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.022436\n","174it [00:06, 25.44it/s]Train epoch: 155 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.022589\n","198it [00:07, 24.72it/s]Train epoch: 155 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.019987\n","225it [00:08, 23.99it/s]Train epoch: 155 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.021668\n","249it [00:09, 24.52it/s]Train epoch: 155 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.022056\n","273it [00:10, 23.01it/s]Train epoch: 155 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.023030\n","300it [00:11, 23.28it/s]Train epoch: 155 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.023025\n","324it [00:12, 22.81it/s]Train epoch: 155 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.026539\n","348it [00:13, 21.08it/s]Train epoch: 155 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.022741\n","375it [00:14, 20.62it/s]Train epoch: 155 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.026537\n","399it [00:16, 20.10it/s]Train epoch: 155 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.023561\n","425it [00:17, 19.37it/s]Train epoch: 155 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.021860\n","450it [00:18, 18.87it/s]Train epoch: 155 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.025347\n","474it [00:20, 17.11it/s]Train epoch: 155 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.026916\n","500it [00:21, 15.46it/s]Train epoch: 155 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.026732\n","505it [00:22, 22.86it/s]\n","epoch loss: 0.023016808209894685\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 346.57it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3688, 0.5619, 0.4764, 0.5156, 0.8610\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3994, 0.6285, 0.5229, 0.5709, 0.8830\n","rec_at_5: 0.5421\n","prec_at_5: 0.5574\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 156\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 156 [batch #0, batch_size 16, seq length 212]\tLoss: 0.049073\n","25it [00:00, 29.66it/s]Train epoch: 156 [batch #25, batch_size 16, seq length 571]\tLoss: 0.018689\n","49it [00:01, 28.19it/s]Train epoch: 156 [batch #50, batch_size 16, seq length 709]\tLoss: 0.020615\n","73it [00:02, 28.82it/s]Train epoch: 156 [batch #75, batch_size 16, seq length 806]\tLoss: 0.025907\n","99it [00:03, 27.40it/s]Train epoch: 156 [batch #100, batch_size 16, seq length 892]\tLoss: 0.021455\n","123it [00:04, 27.84it/s]Train epoch: 156 [batch #125, batch_size 16, seq length 978]\tLoss: 0.022634\n","150it [00:05, 26.18it/s]Train epoch: 156 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.023909\n","174it [00:06, 25.76it/s]Train epoch: 156 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.021194\n","198it [00:07, 24.82it/s]Train epoch: 156 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.018518\n","225it [00:08, 24.75it/s]Train epoch: 156 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.019922\n","249it [00:09, 24.48it/s]Train epoch: 156 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.024188\n","273it [00:10, 23.81it/s]Train epoch: 156 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.022966\n","300it [00:11, 23.56it/s]Train epoch: 156 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.022551\n","324it [00:12, 22.17it/s]Train epoch: 156 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.024837\n","348it [00:13, 21.34it/s]Train epoch: 156 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.022941\n","375it [00:15, 21.43it/s]Train epoch: 156 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.025537\n","399it [00:16, 20.39it/s]Train epoch: 156 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.023446\n","423it [00:17, 19.56it/s]Train epoch: 156 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.019864\n","450it [00:18, 18.89it/s]Train epoch: 156 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.022575\n","475it [00:20, 17.77it/s]Train epoch: 156 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.025616\n","499it [00:21, 15.26it/s]Train epoch: 156 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.023702\n","505it [00:22, 22.78it/s]\n","epoch loss: 0.022100728433371734\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 350.58it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3692, 0.5577, 0.4796, 0.5157, 0.8609\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3991, 0.6238, 0.5256, 0.5705, 0.8832\n","rec_at_5: 0.5444\n","prec_at_5: 0.5591\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 157\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 157 [batch #0, batch_size 16, seq length 212]\tLoss: 0.052934\n","22it [00:00, 31.22it/s]Train epoch: 157 [batch #25, batch_size 16, seq length 571]\tLoss: 0.016728\n","50it [00:01, 30.40it/s]Train epoch: 157 [batch #50, batch_size 16, seq length 709]\tLoss: 0.018736\n","73it [00:02, 28.69it/s]Train epoch: 157 [batch #75, batch_size 16, seq length 806]\tLoss: 0.020337\n","99it [00:03, 28.78it/s]Train epoch: 157 [batch #100, batch_size 16, seq length 892]\tLoss: 0.023433\n","123it [00:04, 27.82it/s]Train epoch: 157 [batch #125, batch_size 16, seq length 978]\tLoss: 0.021564\n","150it [00:05, 25.68it/s]Train epoch: 157 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.023671\n","174it [00:06, 25.03it/s]Train epoch: 157 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.019888\n","198it [00:07, 23.89it/s]Train epoch: 157 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.019027\n","225it [00:08, 24.62it/s]Train epoch: 157 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.021157\n","249it [00:09, 23.94it/s]Train epoch: 157 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.021325\n","273it [00:10, 22.98it/s]Train epoch: 157 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.020278\n","300it [00:11, 22.10it/s]Train epoch: 157 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.021334\n","324it [00:12, 22.25it/s]Train epoch: 157 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.022544\n","348it [00:13, 21.74it/s]Train epoch: 157 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.022342\n","375it [00:15, 21.32it/s]Train epoch: 157 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.024429\n","399it [00:16, 20.36it/s]Train epoch: 157 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.023112\n","423it [00:17, 19.60it/s]Train epoch: 157 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.021806\n","450it [00:18, 18.82it/s]Train epoch: 157 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.022712\n","474it [00:20, 16.89it/s]Train epoch: 157 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.028399\n","500it [00:21, 15.91it/s]Train epoch: 157 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.023758\n","505it [00:22, 22.68it/s]\n","epoch loss: 0.02168213206269585\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 346.67it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3668, 0.5608, 0.4734, 0.5134, 0.8609\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3974, 0.6286, 0.5193, 0.5687, 0.8828\n","rec_at_5: 0.5433\n","prec_at_5: 0.5563\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 158\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 158 [batch #0, batch_size 16, seq length 212]\tLoss: 0.053231\n","22it [00:00, 31.43it/s]Train epoch: 158 [batch #25, batch_size 16, seq length 571]\tLoss: 0.016827\n","50it [00:01, 29.72it/s]Train epoch: 158 [batch #50, batch_size 16, seq length 709]\tLoss: 0.016630\n","73it [00:02, 28.71it/s]Train epoch: 158 [batch #75, batch_size 16, seq length 806]\tLoss: 0.022740\n","100it [00:03, 26.75it/s]Train epoch: 158 [batch #100, batch_size 16, seq length 892]\tLoss: 0.020995\n","124it [00:04, 26.21it/s]Train epoch: 158 [batch #125, batch_size 16, seq length 978]\tLoss: 0.018969\n","148it [00:05, 26.12it/s]Train epoch: 158 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.021675\n","175it [00:06, 25.31it/s]Train epoch: 158 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.018805\n","199it [00:07, 25.15it/s]Train epoch: 158 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.021638\n","223it [00:08, 24.91it/s]Train epoch: 158 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.019584\n","250it [00:09, 23.42it/s]Train epoch: 158 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.021426\n","274it [00:10, 22.61it/s]Train epoch: 158 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.021959\n","298it [00:11, 22.85it/s]Train epoch: 158 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.019841\n","325it [00:12, 22.14it/s]Train epoch: 158 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.023863\n","349it [00:13, 22.06it/s]Train epoch: 158 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.022671\n","373it [00:14, 21.76it/s]Train epoch: 158 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.023523\n","400it [00:16, 20.53it/s]Train epoch: 158 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.022064\n","424it [00:17, 19.73it/s]Train epoch: 158 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.018784\n","449it [00:18, 18.54it/s]Train epoch: 158 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.021117\n","475it [00:20, 17.33it/s]Train epoch: 158 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.026373\n","499it [00:21, 15.87it/s]Train epoch: 158 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.021459\n","505it [00:22, 22.78it/s]\n","epoch loss: 0.0207492816827887\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 348.43it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3697, 0.5572, 0.4819, 0.5168, 0.8606\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3993, 0.6243, 0.5257, 0.5708, 0.8824\n","rec_at_5: 0.5422\n","prec_at_5: 0.5566\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 159\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 159 [batch #0, batch_size 16, seq length 212]\tLoss: 0.051004\n","23it [00:00, 29.97it/s]Train epoch: 159 [batch #25, batch_size 16, seq length 571]\tLoss: 0.020269\n","50it [00:01, 29.59it/s]Train epoch: 159 [batch #50, batch_size 16, seq length 709]\tLoss: 0.018770\n","74it [00:02, 28.79it/s]Train epoch: 159 [batch #75, batch_size 16, seq length 806]\tLoss: 0.021967\n","99it [00:03, 28.28it/s]Train epoch: 159 [batch #100, batch_size 16, seq length 892]\tLoss: 0.021472\n","124it [00:04, 27.47it/s]Train epoch: 159 [batch #125, batch_size 16, seq length 978]\tLoss: 0.018927\n","148it [00:05, 26.09it/s]Train epoch: 159 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.020867\n","175it [00:06, 25.89it/s]Train epoch: 159 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.018958\n","199it [00:07, 24.68it/s]Train epoch: 159 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.022042\n","223it [00:08, 23.90it/s]Train epoch: 159 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.019948\n","250it [00:09, 24.14it/s]Train epoch: 159 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.020993\n","274it [00:10, 23.43it/s]Train epoch: 159 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.021531\n","298it [00:11, 23.16it/s]Train epoch: 159 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.020311\n","325it [00:12, 21.87it/s]Train epoch: 159 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.021387\n","349it [00:13, 21.51it/s]Train epoch: 159 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.023493\n","373it [00:14, 21.50it/s]Train epoch: 159 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.022450\n","400it [00:16, 20.03it/s]Train epoch: 159 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.020253\n","425it [00:17, 19.60it/s]Train epoch: 159 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.022015\n","450it [00:18, 18.61it/s]Train epoch: 159 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.020979\n","474it [00:20, 17.24it/s]Train epoch: 159 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.023398\n","500it [00:21, 15.83it/s]Train epoch: 159 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.022607\n","505it [00:22, 22.79it/s]\n","epoch loss: 0.020582902915130305\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 347.29it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3699, 0.5573, 0.4816, 0.5167, 0.8601\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3991, 0.6241, 0.5255, 0.5706, 0.8821\n","rec_at_5: 0.5419\n","prec_at_5: 0.5556\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 160\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 160 [batch #0, batch_size 16, seq length 212]\tLoss: 0.046084\n","23it [00:00, 30.55it/s]Train epoch: 160 [batch #25, batch_size 16, seq length 571]\tLoss: 0.018110\n","47it [00:01, 30.24it/s]Train epoch: 160 [batch #50, batch_size 16, seq length 709]\tLoss: 0.020272\n","75it [00:02, 28.28it/s]Train epoch: 160 [batch #75, batch_size 16, seq length 806]\tLoss: 0.022836\n","98it [00:03, 28.18it/s]Train epoch: 160 [batch #100, batch_size 16, seq length 892]\tLoss: 0.019050\n","125it [00:04, 28.11it/s]Train epoch: 160 [batch #125, batch_size 16, seq length 978]\tLoss: 0.020570\n","149it [00:05, 27.36it/s]Train epoch: 160 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.021022\n","173it [00:06, 25.62it/s]Train epoch: 160 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.018216\n","200it [00:07, 24.57it/s]Train epoch: 160 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.017009\n","224it [00:08, 24.46it/s]Train epoch: 160 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.018590\n","248it [00:09, 24.88it/s]Train epoch: 160 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.021165\n","275it [00:10, 23.97it/s]Train epoch: 160 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.020695\n","299it [00:11, 22.56it/s]Train epoch: 160 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.019903\n","323it [00:12, 22.46it/s]Train epoch: 160 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.020590\n","350it [00:13, 21.42it/s]Train epoch: 160 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.021718\n","374it [00:14, 20.60it/s]Train epoch: 160 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.021550\n","398it [00:15, 20.86it/s]Train epoch: 160 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.019228\n","425it [00:17, 19.97it/s]Train epoch: 160 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.017675\n","450it [00:18, 17.88it/s]Train epoch: 160 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.019911\n","474it [00:20, 17.78it/s]Train epoch: 160 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.022928\n","500it [00:21, 15.86it/s]Train epoch: 160 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.021152\n","505it [00:21, 22.98it/s]\n","epoch loss: 0.02000571896061578\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 350.48it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3692, 0.5579, 0.4809, 0.5166, 0.8600\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3984, 0.6218, 0.5259, 0.5698, 0.8821\n","rec_at_5: 0.5405\n","prec_at_5: 0.5546\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 161\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 161 [batch #0, batch_size 16, seq length 212]\tLoss: 0.039658\n","23it [00:00, 30.38it/s]Train epoch: 161 [batch #25, batch_size 16, seq length 571]\tLoss: 0.017428\n","47it [00:01, 30.98it/s]Train epoch: 161 [batch #50, batch_size 16, seq length 709]\tLoss: 0.017582\n","73it [00:02, 29.26it/s]Train epoch: 161 [batch #75, batch_size 16, seq length 806]\tLoss: 0.023054\n","98it [00:03, 26.56it/s]Train epoch: 161 [batch #100, batch_size 16, seq length 892]\tLoss: 0.020803\n","125it [00:04, 27.44it/s]Train epoch: 161 [batch #125, batch_size 16, seq length 978]\tLoss: 0.016984\n","149it [00:05, 26.00it/s]Train epoch: 161 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.018207\n","173it [00:06, 24.96it/s]Train epoch: 161 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.017818\n","200it [00:07, 24.42it/s]Train epoch: 161 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.017826\n","224it [00:08, 23.57it/s]Train epoch: 161 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.018582\n","248it [00:09, 24.33it/s]Train epoch: 161 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.021814\n","275it [00:10, 23.25it/s]Train epoch: 161 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.019069\n","299it [00:11, 23.40it/s]Train epoch: 161 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.020051\n","323it [00:12, 21.74it/s]Train epoch: 161 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.019231\n","350it [00:13, 21.46it/s]Train epoch: 161 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.020771\n","374it [00:14, 21.06it/s]Train epoch: 161 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.021971\n","398it [00:16, 20.49it/s]Train epoch: 161 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.020546\n","424it [00:17, 18.61it/s]Train epoch: 161 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.019933\n","449it [00:18, 18.40it/s]Train epoch: 161 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.020250\n","475it [00:20, 17.82it/s]Train epoch: 161 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.023106\n","499it [00:21, 15.60it/s]Train epoch: 161 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.020669\n","505it [00:22, 22.73it/s]\n","epoch loss: 0.019508286112790357\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 336.66it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3688, 0.5594, 0.4785, 0.5158, 0.8601\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3988, 0.6236, 0.5253, 0.5702, 0.8825\n","rec_at_5: 0.5414\n","prec_at_5: 0.5559\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 162\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 162 [batch #0, batch_size 16, seq length 212]\tLoss: 0.041197\n","23it [00:00, 30.72it/s]Train epoch: 162 [batch #25, batch_size 16, seq length 571]\tLoss: 0.015139\n","47it [00:01, 29.74it/s]Train epoch: 162 [batch #50, batch_size 16, seq length 709]\tLoss: 0.018123\n","72it [00:02, 26.67it/s]Train epoch: 162 [batch #75, batch_size 16, seq length 806]\tLoss: 0.021582\n","100it [00:03, 27.62it/s]Train epoch: 162 [batch #100, batch_size 16, seq length 892]\tLoss: 0.018976\n","124it [00:04, 27.21it/s]Train epoch: 162 [batch #125, batch_size 16, seq length 978]\tLoss: 0.017178\n","148it [00:05, 26.66it/s]Train epoch: 162 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.020606\n","175it [00:06, 24.53it/s]Train epoch: 162 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.015830\n","199it [00:07, 24.29it/s]Train epoch: 162 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.015816\n","223it [00:08, 23.62it/s]Train epoch: 162 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.018676\n","250it [00:09, 23.61it/s]Train epoch: 162 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.017749\n","274it [00:10, 22.91it/s]Train epoch: 162 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.018588\n","298it [00:11, 21.53it/s]Train epoch: 162 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.020322\n","325it [00:12, 22.13it/s]Train epoch: 162 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.020255\n","349it [00:13, 20.81it/s]Train epoch: 162 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.017469\n","373it [00:15, 21.40it/s]Train epoch: 162 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.021638\n","400it [00:16, 20.33it/s]Train epoch: 162 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.018211\n","424it [00:17, 19.25it/s]Train epoch: 162 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.019030\n","450it [00:19, 18.70it/s]Train epoch: 162 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.019532\n","474it [00:20, 16.70it/s]Train epoch: 162 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.021800\n","500it [00:22, 15.28it/s]Train epoch: 162 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.018534\n","505it [00:22, 22.55it/s]\n","epoch loss: 0.01894492970406087\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 344.37it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3678, 0.5565, 0.4779, 0.5142, 0.8598\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3974, 0.6226, 0.5235, 0.5688, 0.8818\n","rec_at_5: 0.5418\n","prec_at_5: 0.5558\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 163\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 163 [batch #0, batch_size 16, seq length 212]\tLoss: 0.050752\n","23it [00:00, 30.20it/s]Train epoch: 163 [batch #25, batch_size 16, seq length 571]\tLoss: 0.014654\n","50it [00:01, 28.58it/s]Train epoch: 163 [batch #50, batch_size 16, seq length 709]\tLoss: 0.017942\n","74it [00:02, 27.69it/s]Train epoch: 163 [batch #75, batch_size 16, seq length 806]\tLoss: 0.020529\n","98it [00:03, 28.11it/s]Train epoch: 163 [batch #100, batch_size 16, seq length 892]\tLoss: 0.018569\n","125it [00:04, 27.55it/s]Train epoch: 163 [batch #125, batch_size 16, seq length 978]\tLoss: 0.017014\n","149it [00:05, 26.02it/s]Train epoch: 163 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.017986\n","173it [00:06, 26.14it/s]Train epoch: 163 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.017814\n","200it [00:07, 25.10it/s]Train epoch: 163 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.017700\n","224it [00:08, 25.12it/s]Train epoch: 163 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.017579\n","248it [00:09, 23.85it/s]Train epoch: 163 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.020122\n","275it [00:10, 23.27it/s]Train epoch: 163 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.017254\n","299it [00:11, 22.94it/s]Train epoch: 163 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.019688\n","323it [00:12, 22.87it/s]Train epoch: 163 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.019112\n","350it [00:13, 21.40it/s]Train epoch: 163 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.019226\n","374it [00:14, 20.33it/s]Train epoch: 163 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.019972\n","398it [00:16, 20.46it/s]Train epoch: 163 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.019621\n","425it [00:17, 18.53it/s]Train epoch: 163 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.017000\n","449it [00:18, 18.93it/s]Train epoch: 163 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.018242\n","475it [00:20, 16.73it/s]Train epoch: 163 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.021646\n","499it [00:21, 15.06it/s]Train epoch: 163 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.020605\n","505it [00:22, 22.73it/s]\n","epoch loss: 0.018472840778050153\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 342.49it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3704, 0.5555, 0.4835, 0.5170, 0.8597\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3997, 0.6204, 0.5291, 0.5711, 0.8822\n","rec_at_5: 0.5402\n","prec_at_5: 0.5559\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 164\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 164 [batch #0, batch_size 16, seq length 212]\tLoss: 0.045960\n","22it [00:00, 30.78it/s]Train epoch: 164 [batch #25, batch_size 16, seq length 571]\tLoss: 0.015176\n","49it [00:01, 28.96it/s]Train epoch: 164 [batch #50, batch_size 16, seq length 709]\tLoss: 0.016464\n","73it [00:02, 28.18it/s]Train epoch: 164 [batch #75, batch_size 16, seq length 806]\tLoss: 0.020250\n","100it [00:03, 26.88it/s]Train epoch: 164 [batch #100, batch_size 16, seq length 892]\tLoss: 0.018651\n","124it [00:04, 27.09it/s]Train epoch: 164 [batch #125, batch_size 16, seq length 978]\tLoss: 0.016937\n","148it [00:05, 25.96it/s]Train epoch: 164 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.018308\n","175it [00:06, 24.51it/s]Train epoch: 164 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.015442\n","199it [00:07, 23.71it/s]Train epoch: 164 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.017038\n","223it [00:08, 25.27it/s]Train epoch: 164 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.018127\n","250it [00:09, 23.27it/s]Train epoch: 164 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.019091\n","274it [00:10, 22.56it/s]Train epoch: 164 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.021264\n","298it [00:11, 21.86it/s]Train epoch: 164 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.018344\n","325it [00:12, 21.80it/s]Train epoch: 164 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.018270\n","349it [00:13, 20.81it/s]Train epoch: 164 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.018507\n","373it [00:15, 21.53it/s]Train epoch: 164 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.020331\n","400it [00:16, 20.46it/s]Train epoch: 164 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.015997\n","425it [00:17, 19.20it/s]Train epoch: 164 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.015160\n","450it [00:19, 18.47it/s]Train epoch: 164 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.017814\n","474it [00:20, 17.62it/s]Train epoch: 164 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.021392\n","500it [00:22, 14.97it/s]Train epoch: 164 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.019947\n","505it [00:22, 22.52it/s]\n","epoch loss: 0.01793670929959136\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 343.72it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3694, 0.5571, 0.4809, 0.5162, 0.8597\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3989, 0.6220, 0.5266, 0.5703, 0.8820\n","rec_at_5: 0.5413\n","prec_at_5: 0.5551\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 165\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 165 [batch #0, batch_size 16, seq length 212]\tLoss: 0.033473\n","25it [00:00, 29.92it/s]Train epoch: 165 [batch #25, batch_size 16, seq length 571]\tLoss: 0.017632\n","47it [00:01, 28.80it/s]Train epoch: 165 [batch #50, batch_size 16, seq length 709]\tLoss: 0.015404\n","73it [00:02, 28.31it/s]Train epoch: 165 [batch #75, batch_size 16, seq length 806]\tLoss: 0.018668\n","100it [00:03, 27.14it/s]Train epoch: 165 [batch #100, batch_size 16, seq length 892]\tLoss: 0.018365\n","125it [00:04, 26.90it/s]Train epoch: 165 [batch #125, batch_size 16, seq length 978]\tLoss: 0.016222\n","149it [00:05, 24.53it/s]Train epoch: 165 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.015490\n","173it [00:06, 25.07it/s]Train epoch: 165 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.016221\n","200it [00:07, 25.19it/s]Train epoch: 165 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.014027\n","224it [00:08, 23.87it/s]Train epoch: 165 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.018442\n","248it [00:09, 23.84it/s]Train epoch: 165 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.017626\n","275it [00:10, 24.21it/s]Train epoch: 165 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.016442\n","299it [00:11, 22.40it/s]Train epoch: 165 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.019803\n","323it [00:12, 22.30it/s]Train epoch: 165 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.018561\n","350it [00:13, 22.32it/s]Train epoch: 165 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.017340\n","374it [00:15, 21.31it/s]Train epoch: 165 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.020453\n","398it [00:16, 20.37it/s]Train epoch: 165 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.017132\n","424it [00:17, 18.80it/s]Train epoch: 165 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.016171\n","450it [00:18, 19.08it/s]Train epoch: 165 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.017752\n","474it [00:20, 17.27it/s]Train epoch: 165 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.021927\n","500it [00:22, 15.35it/s]Train epoch: 165 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.016917\n","505it [00:22, 22.63it/s]\n","epoch loss: 0.01770836200950077\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 345.79it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3692, 0.5565, 0.4805, 0.5157, 0.8596\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3987, 0.6218, 0.5263, 0.5701, 0.8819\n","rec_at_5: 0.5402\n","prec_at_5: 0.5542\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 166\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 166 [batch #0, batch_size 16, seq length 212]\tLoss: 0.024016\n","23it [00:00, 31.36it/s]Train epoch: 166 [batch #25, batch_size 16, seq length 571]\tLoss: 0.015727\n","50it [00:01, 29.21it/s]Train epoch: 166 [batch #50, batch_size 16, seq length 709]\tLoss: 0.016883\n","73it [00:02, 28.75it/s]Train epoch: 166 [batch #75, batch_size 16, seq length 806]\tLoss: 0.018574\n","100it [00:03, 27.36it/s]Train epoch: 166 [batch #100, batch_size 16, seq length 892]\tLoss: 0.017834\n","124it [00:04, 26.89it/s]Train epoch: 166 [batch #125, batch_size 16, seq length 978]\tLoss: 0.017470\n","148it [00:05, 26.08it/s]Train epoch: 166 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.018423\n","175it [00:06, 25.67it/s]Train epoch: 166 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.014921\n","199it [00:07, 25.00it/s]Train epoch: 166 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.015673\n","223it [00:08, 24.69it/s]Train epoch: 166 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.017188\n","250it [00:09, 23.80it/s]Train epoch: 166 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.019208\n","274it [00:10, 23.26it/s]Train epoch: 166 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.018008\n","298it [00:11, 22.65it/s]Train epoch: 166 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.017863\n","325it [00:12, 22.93it/s]Train epoch: 166 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.016191\n","349it [00:13, 21.90it/s]Train epoch: 166 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.018303\n","373it [00:14, 20.92it/s]Train epoch: 166 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.020130\n","400it [00:16, 20.03it/s]Train epoch: 166 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.017814\n","425it [00:17, 19.17it/s]Train epoch: 166 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.015834\n","448it [00:18, 18.19it/s]Train epoch: 166 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.017141\n","475it [00:20, 16.85it/s]Train epoch: 166 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.021939\n","499it [00:21, 16.15it/s]Train epoch: 166 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.017714\n","505it [00:22, 22.91it/s]\n","epoch loss: 0.01703040738570467\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 345.75it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3680, 0.5567, 0.4788, 0.5148, 0.8594\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3973, 0.6218, 0.5240, 0.5687, 0.8817\n","rec_at_5: 0.5408\n","prec_at_5: 0.5549\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 167\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 167 [batch #0, batch_size 16, seq length 212]\tLoss: 0.047902\n","22it [00:00, 31.39it/s]Train epoch: 167 [batch #25, batch_size 16, seq length 571]\tLoss: 0.015086\n","50it [00:01, 29.37it/s]Train epoch: 167 [batch #50, batch_size 16, seq length 709]\tLoss: 0.014199\n","72it [00:02, 28.67it/s]Train epoch: 167 [batch #75, batch_size 16, seq length 806]\tLoss: 0.018521\n","99it [00:03, 27.86it/s]Train epoch: 167 [batch #100, batch_size 16, seq length 892]\tLoss: 0.017316\n","123it [00:04, 26.12it/s]Train epoch: 167 [batch #125, batch_size 16, seq length 978]\tLoss: 0.014672\n","150it [00:05, 25.55it/s]Train epoch: 167 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.019440\n","174it [00:06, 25.36it/s]Train epoch: 167 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.015295\n","198it [00:07, 24.46it/s]Train epoch: 167 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.015463\n","225it [00:08, 24.21it/s]Train epoch: 167 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.015874\n","249it [00:09, 23.92it/s]Train epoch: 167 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.017588\n","273it [00:10, 22.06it/s]Train epoch: 167 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.018344\n","300it [00:11, 23.30it/s]Train epoch: 167 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.015746\n","324it [00:12, 21.86it/s]Train epoch: 167 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.018806\n","348it [00:13, 21.07it/s]Train epoch: 167 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.015730\n","375it [00:15, 20.95it/s]Train epoch: 167 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.019275\n","399it [00:16, 20.33it/s]Train epoch: 167 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.016808\n","425it [00:17, 19.53it/s]Train epoch: 167 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.015520\n","449it [00:18, 18.77it/s]Train epoch: 167 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.016907\n","475it [00:20, 17.53it/s]Train epoch: 167 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.019940\n","499it [00:21, 15.87it/s]Train epoch: 167 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.017280\n","505it [00:22, 22.77it/s]\n","epoch loss: 0.0166636851445615\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 348.03it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3674, 0.5537, 0.4792, 0.5137, 0.8592\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3972, 0.6191, 0.5257, 0.5686, 0.8817\n","rec_at_5: 0.5409\n","prec_at_5: 0.5550\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 168\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 168 [batch #0, batch_size 16, seq length 212]\tLoss: 0.032375\n","22it [00:00, 30.62it/s]Train epoch: 168 [batch #25, batch_size 16, seq length 571]\tLoss: 0.013705\n","49it [00:01, 29.75it/s]Train epoch: 168 [batch #50, batch_size 16, seq length 709]\tLoss: 0.013079\n","75it [00:02, 28.95it/s]Train epoch: 168 [batch #75, batch_size 16, seq length 806]\tLoss: 0.019851\n","100it [00:03, 27.63it/s]Train epoch: 168 [batch #100, batch_size 16, seq length 892]\tLoss: 0.015468\n","124it [00:04, 27.14it/s]Train epoch: 168 [batch #125, batch_size 16, seq length 978]\tLoss: 0.015678\n","148it [00:05, 26.65it/s]Train epoch: 168 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.015825\n","175it [00:06, 26.15it/s]Train epoch: 168 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.013924\n","199it [00:07, 25.52it/s]Train epoch: 168 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.013941\n","223it [00:08, 23.94it/s]Train epoch: 168 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.017109\n","250it [00:09, 23.65it/s]Train epoch: 168 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.015930\n","274it [00:10, 23.17it/s]Train epoch: 168 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.014195\n","298it [00:11, 22.68it/s]Train epoch: 168 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.015285\n","325it [00:12, 22.70it/s]Train epoch: 168 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.016928\n","349it [00:13, 21.95it/s]Train epoch: 168 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.016692\n","373it [00:14, 20.83it/s]Train epoch: 168 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.017907\n","400it [00:16, 20.43it/s]Train epoch: 168 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.016200\n","425it [00:17, 19.63it/s]Train epoch: 168 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.015200\n","449it [00:18, 18.93it/s]Train epoch: 168 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.015823\n","475it [00:20, 18.10it/s]Train epoch: 168 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.017551\n","499it [00:21, 15.54it/s]Train epoch: 168 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.016806\n","505it [00:22, 22.75it/s]\n","epoch loss: 0.016243239382390248\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 349.83it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3672, 0.5541, 0.4781, 0.5133, 0.8593\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3966, 0.6195, 0.5244, 0.5680, 0.8818\n","rec_at_5: 0.5405\n","prec_at_5: 0.5550\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 169\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 169 [batch #0, batch_size 16, seq length 212]\tLoss: 0.029211\n","24it [00:00, 30.90it/s]Train epoch: 169 [batch #25, batch_size 16, seq length 571]\tLoss: 0.016000\n","48it [00:01, 30.47it/s]Train epoch: 169 [batch #50, batch_size 16, seq length 709]\tLoss: 0.015322\n","73it [00:02, 29.07it/s]Train epoch: 169 [batch #75, batch_size 16, seq length 806]\tLoss: 0.018854\n","98it [00:03, 27.33it/s]Train epoch: 169 [batch #100, batch_size 16, seq length 892]\tLoss: 0.016410\n","123it [00:04, 26.58it/s]Train epoch: 169 [batch #125, batch_size 16, seq length 978]\tLoss: 0.015622\n","150it [00:05, 26.27it/s]Train epoch: 169 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.015004\n","174it [00:06, 25.39it/s]Train epoch: 169 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.015288\n","198it [00:07, 24.33it/s]Train epoch: 169 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.014459\n","225it [00:08, 23.56it/s]Train epoch: 169 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.015892\n","249it [00:09, 23.47it/s]Train epoch: 169 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.017267\n","273it [00:10, 23.68it/s]Train epoch: 169 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.016519\n","300it [00:11, 22.29it/s]Train epoch: 169 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.015120\n","324it [00:12, 21.79it/s]Train epoch: 169 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.017354\n","348it [00:13, 21.63it/s]Train epoch: 169 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.016575\n","375it [00:15, 20.50it/s]Train epoch: 169 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.017192\n","399it [00:16, 20.54it/s]Train epoch: 169 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.017008\n","423it [00:17, 20.04it/s]Train epoch: 169 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.015803\n","450it [00:18, 18.26it/s]Train epoch: 169 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.017626\n","475it [00:20, 17.63it/s]Train epoch: 169 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.018279\n","499it [00:21, 15.63it/s]Train epoch: 169 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.015995\n","505it [00:22, 22.79it/s]\n","epoch loss: 0.016121262919024038\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 349.84it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3662, 0.5545, 0.4756, 0.5120, 0.8590\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3958, 0.6200, 0.5226, 0.5671, 0.8816\n","rec_at_5: 0.5394\n","prec_at_5: 0.5533\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 170\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 170 [batch #0, batch_size 16, seq length 212]\tLoss: 0.035364\n","23it [00:00, 31.06it/s]Train epoch: 170 [batch #25, batch_size 16, seq length 571]\tLoss: 0.014920\n","50it [00:01, 29.47it/s]Train epoch: 170 [batch #50, batch_size 16, seq length 709]\tLoss: 0.014284\n","74it [00:02, 28.00it/s]Train epoch: 170 [batch #75, batch_size 16, seq length 806]\tLoss: 0.015783\n","100it [00:03, 27.19it/s]Train epoch: 170 [batch #100, batch_size 16, seq length 892]\tLoss: 0.016085\n","124it [00:04, 27.37it/s]Train epoch: 170 [batch #125, batch_size 16, seq length 978]\tLoss: 0.015587\n","148it [00:05, 26.36it/s]Train epoch: 170 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.016145\n","175it [00:06, 25.54it/s]Train epoch: 170 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.014466\n","199it [00:07, 24.18it/s]Train epoch: 170 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.015179\n","223it [00:08, 24.70it/s]Train epoch: 170 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.016454\n","250it [00:09, 23.30it/s]Train epoch: 170 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.015536\n","274it [00:10, 23.41it/s]Train epoch: 170 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.016121\n","298it [00:11, 22.70it/s]Train epoch: 170 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.016399\n","325it [00:12, 21.95it/s]Train epoch: 170 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.015674\n","349it [00:13, 21.70it/s]Train epoch: 170 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.015225\n","373it [00:14, 21.06it/s]Train epoch: 170 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.017173\n","398it [00:16, 18.95it/s]Train epoch: 170 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.016594\n","424it [00:17, 19.46it/s]Train epoch: 170 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.013928\n","449it [00:18, 18.29it/s]Train epoch: 170 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.015079\n","475it [00:20, 17.35it/s]Train epoch: 170 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.017201\n","499it [00:21, 15.30it/s]Train epoch: 170 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.014381\n","505it [00:22, 22.66it/s]\n","epoch loss: 0.015665099730065073\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 340.78it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3653, 0.5554, 0.4738, 0.5114, 0.8591\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3954, 0.6222, 0.5203, 0.5667, 0.8812\n","rec_at_5: 0.5397\n","prec_at_5: 0.5535\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 171\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 171 [batch #0, batch_size 16, seq length 212]\tLoss: 0.040771\n","23it [00:00, 30.74it/s]Train epoch: 171 [batch #25, batch_size 16, seq length 571]\tLoss: 0.012896\n","50it [00:01, 29.05it/s]Train epoch: 171 [batch #50, batch_size 16, seq length 709]\tLoss: 0.015871\n","75it [00:02, 28.68it/s]Train epoch: 171 [batch #75, batch_size 16, seq length 806]\tLoss: 0.016506\n","100it [00:03, 27.48it/s]Train epoch: 171 [batch #100, batch_size 16, seq length 892]\tLoss: 0.017112\n","124it [00:04, 26.20it/s]Train epoch: 171 [batch #125, batch_size 16, seq length 978]\tLoss: 0.015599\n","148it [00:05, 26.07it/s]Train epoch: 171 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.013518\n","175it [00:06, 24.48it/s]Train epoch: 171 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.013619\n","199it [00:07, 25.72it/s]Train epoch: 171 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.014702\n","223it [00:08, 25.02it/s]Train epoch: 171 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.015684\n","250it [00:09, 23.73it/s]Train epoch: 171 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.015298\n","274it [00:10, 23.12it/s]Train epoch: 171 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.014377\n","298it [00:11, 23.39it/s]Train epoch: 171 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.015555\n","325it [00:12, 21.89it/s]Train epoch: 171 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.014303\n","349it [00:13, 21.82it/s]Train epoch: 171 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.014872\n","373it [00:14, 21.06it/s]Train epoch: 171 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.017558\n","400it [00:16, 19.75it/s]Train epoch: 171 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.014801\n","424it [00:17, 19.85it/s]Train epoch: 171 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.013600\n","448it [00:18, 18.59it/s]Train epoch: 171 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.014178\n","475it [00:20, 17.93it/s]Train epoch: 171 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.017013\n","499it [00:21, 14.94it/s]Train epoch: 171 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.014757\n","505it [00:22, 22.70it/s]\n","epoch loss: 0.015098896848803183\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 350.61it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3663, 0.5550, 0.4759, 0.5124, 0.8590\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3951, 0.6199, 0.5214, 0.5664, 0.8814\n","rec_at_5: 0.5385\n","prec_at_5: 0.5531\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 172\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 172 [batch #0, batch_size 16, seq length 212]\tLoss: 0.039438\n","25it [00:00, 30.03it/s]Train epoch: 172 [batch #25, batch_size 16, seq length 571]\tLoss: 0.012521\n","50it [00:01, 29.75it/s]Train epoch: 172 [batch #50, batch_size 16, seq length 709]\tLoss: 0.015757\n","73it [00:02, 28.66it/s]Train epoch: 172 [batch #75, batch_size 16, seq length 806]\tLoss: 0.015374\n","100it [00:03, 28.21it/s]Train epoch: 172 [batch #100, batch_size 16, seq length 892]\tLoss: 0.016750\n","125it [00:04, 27.29it/s]Train epoch: 172 [batch #125, batch_size 16, seq length 978]\tLoss: 0.014265\n","149it [00:05, 27.17it/s]Train epoch: 172 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.015645\n","173it [00:06, 25.65it/s]Train epoch: 172 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.013745\n","200it [00:07, 25.47it/s]Train epoch: 172 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.011468\n","224it [00:08, 24.64it/s]Train epoch: 172 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.014399\n","248it [00:09, 23.74it/s]Train epoch: 172 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.015800\n","275it [00:10, 23.39it/s]Train epoch: 172 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.016239\n","299it [00:11, 23.12it/s]Train epoch: 172 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.014287\n","323it [00:12, 22.30it/s]Train epoch: 172 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.016358\n","350it [00:13, 21.29it/s]Train epoch: 172 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.014515\n","374it [00:14, 20.38it/s]Train epoch: 172 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.015830\n","398it [00:16, 20.25it/s]Train epoch: 172 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.015096\n","423it [00:17, 19.34it/s]Train epoch: 172 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.014682\n","449it [00:18, 17.94it/s]Train epoch: 172 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.015490\n","474it [00:20, 17.64it/s]Train epoch: 172 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.017326\n","500it [00:21, 15.69it/s]Train epoch: 172 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.013944\n","505it [00:22, 22.84it/s]\n","epoch loss: 0.014846175123988144\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 353.95it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3654, 0.5540, 0.4749, 0.5114, 0.8588\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3948, 0.6190, 0.5215, 0.5661, 0.8813\n","rec_at_5: 0.5385\n","prec_at_5: 0.5532\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 173\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 173 [batch #0, batch_size 16, seq length 212]\tLoss: 0.044438\n","22it [00:00, 31.34it/s]Train epoch: 173 [batch #25, batch_size 16, seq length 571]\tLoss: 0.013125\n","49it [00:01, 29.92it/s]Train epoch: 173 [batch #50, batch_size 16, seq length 709]\tLoss: 0.013160\n","73it [00:02, 28.61it/s]Train epoch: 173 [batch #75, batch_size 16, seq length 806]\tLoss: 0.017198\n","99it [00:03, 28.17it/s]Train epoch: 173 [batch #100, batch_size 16, seq length 892]\tLoss: 0.014568\n","123it [00:04, 27.35it/s]Train epoch: 173 [batch #125, batch_size 16, seq length 978]\tLoss: 0.015601\n","150it [00:05, 26.03it/s]Train epoch: 173 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.013200\n","174it [00:06, 26.29it/s]Train epoch: 173 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.013171\n","198it [00:07, 25.37it/s]Train epoch: 173 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.014660\n","225it [00:08, 23.68it/s]Train epoch: 173 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.013924\n","249it [00:09, 24.34it/s]Train epoch: 173 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.016816\n","273it [00:10, 23.53it/s]Train epoch: 173 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.014134\n","300it [00:11, 22.34it/s]Train epoch: 173 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.015360\n","324it [00:12, 22.51it/s]Train epoch: 173 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.015788\n","348it [00:13, 21.54it/s]Train epoch: 173 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.013844\n","375it [00:15, 20.61it/s]Train epoch: 173 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.015744\n","399it [00:16, 21.22it/s]Train epoch: 173 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.014455\n","423it [00:17, 20.13it/s]Train epoch: 173 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.013700\n","449it [00:18, 18.35it/s]Train epoch: 173 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.015003\n","475it [00:20, 17.73it/s]Train epoch: 173 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.018183\n","499it [00:21, 15.33it/s]Train epoch: 173 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.015126\n","505it [00:22, 22.86it/s]\n","epoch loss: 0.014623070161992519\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 344.75it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3670, 0.5509, 0.4806, 0.5134, 0.8588\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3970, 0.6155, 0.5278, 0.5683, 0.8819\n","rec_at_5: 0.5384\n","prec_at_5: 0.5522\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 174\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 174 [batch #0, batch_size 16, seq length 212]\tLoss: 0.027835\n","23it [00:00, 29.77it/s]Train epoch: 174 [batch #25, batch_size 16, seq length 571]\tLoss: 0.011954\n","50it [00:01, 30.37it/s]Train epoch: 174 [batch #50, batch_size 16, seq length 709]\tLoss: 0.011806\n","74it [00:02, 28.69it/s]Train epoch: 174 [batch #75, batch_size 16, seq length 806]\tLoss: 0.015532\n","98it [00:03, 27.63it/s]Train epoch: 174 [batch #100, batch_size 16, seq length 892]\tLoss: 0.015829\n","125it [00:04, 28.01it/s]Train epoch: 174 [batch #125, batch_size 16, seq length 978]\tLoss: 0.012001\n","149it [00:05, 26.72it/s]Train epoch: 174 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.015017\n","173it [00:06, 25.09it/s]Train epoch: 174 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.013729\n","200it [00:07, 23.99it/s]Train epoch: 174 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.014123\n","224it [00:08, 23.65it/s]Train epoch: 174 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.016185\n","248it [00:09, 23.46it/s]Train epoch: 174 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.015808\n","275it [00:10, 22.90it/s]Train epoch: 174 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.013577\n","299it [00:11, 23.12it/s]Train epoch: 174 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.013000\n","323it [00:12, 22.81it/s]Train epoch: 174 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.014629\n","350it [00:13, 22.18it/s]Train epoch: 174 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.013492\n","374it [00:14, 21.14it/s]Train epoch: 174 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.015439\n","398it [00:16, 19.60it/s]Train epoch: 174 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.013513\n","424it [00:17, 19.71it/s]Train epoch: 174 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.013311\n","450it [00:18, 18.84it/s]Train epoch: 174 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.015489\n","474it [00:20, 17.17it/s]Train epoch: 174 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.017061\n","500it [00:21, 15.41it/s]Train epoch: 174 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.013349\n","505it [00:22, 22.86it/s]\n","epoch loss: 0.014245925996150582\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 348.73it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3657, 0.5543, 0.4749, 0.5116, 0.8586\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3956, 0.6198, 0.5224, 0.5669, 0.8814\n","rec_at_5: 0.5387\n","prec_at_5: 0.5514\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 175\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 175 [batch #0, batch_size 16, seq length 212]\tLoss: 0.042641\n","23it [00:00, 29.40it/s]Train epoch: 175 [batch #25, batch_size 16, seq length 571]\tLoss: 0.010589\n","49it [00:01, 30.07it/s]Train epoch: 175 [batch #50, batch_size 16, seq length 709]\tLoss: 0.012926\n","74it [00:02, 28.51it/s]Train epoch: 175 [batch #75, batch_size 16, seq length 806]\tLoss: 0.014998\n","98it [00:03, 27.02it/s]Train epoch: 175 [batch #100, batch_size 16, seq length 892]\tLoss: 0.013067\n","125it [00:04, 27.29it/s]Train epoch: 175 [batch #125, batch_size 16, seq length 978]\tLoss: 0.012537\n","149it [00:05, 26.40it/s]Train epoch: 175 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.014081\n","173it [00:06, 25.26it/s]Train epoch: 175 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.012606\n","200it [00:07, 25.71it/s]Train epoch: 175 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.011746\n","224it [00:08, 24.50it/s]Train epoch: 175 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.012520\n","248it [00:09, 24.03it/s]Train epoch: 175 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.014545\n","275it [00:10, 24.08it/s]Train epoch: 175 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.012061\n","299it [00:11, 22.56it/s]Train epoch: 175 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.013009\n","323it [00:12, 22.80it/s]Train epoch: 175 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.014829\n","350it [00:13, 21.23it/s]Train epoch: 175 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.014296\n","374it [00:14, 20.47it/s]Train epoch: 175 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.015257\n","398it [00:16, 20.07it/s]Train epoch: 175 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.012966\n","424it [00:17, 19.60it/s]Train epoch: 175 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.015196\n","449it [00:18, 17.97it/s]Train epoch: 175 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.014871\n","474it [00:20, 17.85it/s]Train epoch: 175 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.016162\n","500it [00:21, 15.19it/s]Train epoch: 175 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.014828\n","505it [00:22, 22.75it/s]\n","epoch loss: 0.013704464034901213\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 350.87it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3670, 0.5537, 0.4779, 0.5130, 0.8587\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3962, 0.6184, 0.5244, 0.5675, 0.8816\n","rec_at_5: 0.5384\n","prec_at_5: 0.5521\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 176\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 176 [batch #0, batch_size 16, seq length 212]\tLoss: 0.035522\n","23it [00:00, 31.55it/s]Train epoch: 176 [batch #25, batch_size 16, seq length 571]\tLoss: 0.010085\n","50it [00:01, 30.17it/s]Train epoch: 176 [batch #50, batch_size 16, seq length 709]\tLoss: 0.012906\n","74it [00:02, 29.04it/s]Train epoch: 176 [batch #75, batch_size 16, seq length 806]\tLoss: 0.015562\n","99it [00:03, 28.52it/s]Train epoch: 176 [batch #100, batch_size 16, seq length 892]\tLoss: 0.016682\n","123it [00:04, 25.97it/s]Train epoch: 176 [batch #125, batch_size 16, seq length 978]\tLoss: 0.013969\n","150it [00:05, 26.46it/s]Train epoch: 176 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.014862\n","174it [00:06, 24.71it/s]Train epoch: 176 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.011478\n","198it [00:07, 24.37it/s]Train epoch: 176 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.012977\n","225it [00:08, 24.85it/s]Train epoch: 176 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.013526\n","249it [00:09, 22.24it/s]Train epoch: 176 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.014950\n","273it [00:10, 23.40it/s]Train epoch: 176 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.012139\n","300it [00:11, 22.46it/s]Train epoch: 176 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.011710\n","324it [00:12, 22.13it/s]Train epoch: 176 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.014431\n","348it [00:13, 21.80it/s]Train epoch: 176 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.012001\n","375it [00:15, 20.40it/s]Train epoch: 176 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.015252\n","399it [00:16, 20.30it/s]Train epoch: 176 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.013098\n","425it [00:17, 19.37it/s]Train epoch: 176 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.011648\n","450it [00:18, 18.43it/s]Train epoch: 176 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.014398\n","474it [00:20, 17.28it/s]Train epoch: 176 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.016482\n","500it [00:21, 15.57it/s]Train epoch: 176 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.012271\n","505it [00:22, 22.74it/s]\n","epoch loss: 0.013295830078515234\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 349.56it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3641, 0.5531, 0.4735, 0.5102, 0.8584\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3935, 0.6175, 0.5203, 0.5647, 0.8812\n","rec_at_5: 0.5377\n","prec_at_5: 0.5518\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 177\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 177 [batch #0, batch_size 16, seq length 212]\tLoss: 0.025987\n","23it [00:00, 30.35it/s]Train epoch: 177 [batch #25, batch_size 16, seq length 571]\tLoss: 0.011904\n","47it [00:01, 30.27it/s]Train epoch: 177 [batch #50, batch_size 16, seq length 709]\tLoss: 0.011914\n","75it [00:02, 28.30it/s]Train epoch: 177 [batch #75, batch_size 16, seq length 806]\tLoss: 0.015390\n","99it [00:03, 28.24it/s]Train epoch: 177 [batch #100, batch_size 16, seq length 892]\tLoss: 0.013355\n","123it [00:04, 27.17it/s]Train epoch: 177 [batch #125, batch_size 16, seq length 978]\tLoss: 0.012298\n","150it [00:05, 26.74it/s]Train epoch: 177 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.013991\n","174it [00:06, 25.52it/s]Train epoch: 177 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.012036\n","198it [00:07, 25.20it/s]Train epoch: 177 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.011470\n","225it [00:08, 23.82it/s]Train epoch: 177 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.013888\n","249it [00:09, 24.51it/s]Train epoch: 177 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.013879\n","273it [00:10, 23.46it/s]Train epoch: 177 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.015186\n","300it [00:11, 23.69it/s]Train epoch: 177 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.013780\n","324it [00:12, 22.50it/s]Train epoch: 177 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.012603\n","348it [00:13, 21.08it/s]Train epoch: 177 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.014280\n","375it [00:14, 21.16it/s]Train epoch: 177 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.015723\n","399it [00:16, 20.46it/s]Train epoch: 177 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.013751\n","424it [00:17, 19.18it/s]Train epoch: 177 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.011762\n","449it [00:18, 18.16it/s]Train epoch: 177 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.014613\n","475it [00:20, 16.98it/s]Train epoch: 177 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.016105\n","499it [00:21, 16.06it/s]Train epoch: 177 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.013003\n","505it [00:22, 22.81it/s]\n","epoch loss: 0.013283687045781568\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 347.98it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3644, 0.5522, 0.4749, 0.5106, 0.8582\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3930, 0.6165, 0.5201, 0.5642, 0.8811\n","rec_at_5: 0.5385\n","prec_at_5: 0.5521\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 178\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 178 [batch #0, batch_size 16, seq length 212]\tLoss: 0.036396\n","23it [00:00, 30.24it/s]Train epoch: 178 [batch #25, batch_size 16, seq length 571]\tLoss: 0.010867\n","47it [00:01, 30.68it/s]Train epoch: 178 [batch #50, batch_size 16, seq length 709]\tLoss: 0.011944\n","74it [00:02, 29.43it/s]Train epoch: 178 [batch #75, batch_size 16, seq length 806]\tLoss: 0.015704\n","100it [00:03, 28.30it/s]Train epoch: 178 [batch #100, batch_size 16, seq length 892]\tLoss: 0.012089\n","125it [00:04, 27.40it/s]Train epoch: 178 [batch #125, batch_size 16, seq length 978]\tLoss: 0.012980\n","149it [00:05, 26.51it/s]Train epoch: 178 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.011448\n","173it [00:06, 25.43it/s]Train epoch: 178 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.012326\n","200it [00:07, 25.05it/s]Train epoch: 178 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.012488\n","224it [00:08, 23.91it/s]Train epoch: 178 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.011304\n","248it [00:09, 23.52it/s]Train epoch: 178 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.014831\n","275it [00:10, 23.21it/s]Train epoch: 178 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.013223\n","299it [00:11, 22.70it/s]Train epoch: 178 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.013010\n","323it [00:12, 22.17it/s]Train epoch: 178 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.012619\n","350it [00:13, 21.99it/s]Train epoch: 178 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.013832\n","374it [00:14, 21.08it/s]Train epoch: 178 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.011291\n","398it [00:16, 20.85it/s]Train epoch: 178 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.011289\n","425it [00:17, 19.80it/s]Train epoch: 178 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.011972\n","449it [00:18, 17.98it/s]Train epoch: 178 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.012688\n","474it [00:20, 17.76it/s]Train epoch: 178 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.013275\n","500it [00:21, 16.01it/s]Train epoch: 178 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.014328\n","505it [00:22, 22.93it/s]\n","epoch loss: 0.012698171881435549\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 348.91it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3662, 0.5546, 0.4758, 0.5122, 0.8581\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3950, 0.6186, 0.5221, 0.5663, 0.8811\n","rec_at_5: 0.5397\n","prec_at_5: 0.5531\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 179\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 179 [batch #0, batch_size 16, seq length 212]\tLoss: 0.028868\n","23it [00:00, 30.08it/s]Train epoch: 179 [batch #25, batch_size 16, seq length 571]\tLoss: 0.013413\n","50it [00:01, 28.56it/s]Train epoch: 179 [batch #50, batch_size 16, seq length 709]\tLoss: 0.012890\n","75it [00:02, 28.79it/s]Train epoch: 179 [batch #75, batch_size 16, seq length 806]\tLoss: 0.013557\n","99it [00:03, 28.09it/s]Train epoch: 179 [batch #100, batch_size 16, seq length 892]\tLoss: 0.014523\n","123it [00:04, 27.80it/s]Train epoch: 179 [batch #125, batch_size 16, seq length 978]\tLoss: 0.013348\n","150it [00:05, 25.12it/s]Train epoch: 179 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.014319\n","174it [00:06, 26.44it/s]Train epoch: 179 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.011080\n","198it [00:07, 25.76it/s]Train epoch: 179 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.011143\n","225it [00:08, 23.89it/s]Train epoch: 179 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.011549\n","249it [00:09, 23.21it/s]Train epoch: 179 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.012924\n","273it [00:10, 23.40it/s]Train epoch: 179 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.010888\n","300it [00:11, 22.75it/s]Train epoch: 179 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.012519\n","324it [00:12, 22.07it/s]Train epoch: 179 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.011962\n","348it [00:13, 21.94it/s]Train epoch: 179 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.011923\n","375it [00:15, 20.31it/s]Train epoch: 179 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.014381\n","399it [00:16, 19.75it/s]Train epoch: 179 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.012795\n","424it [00:17, 19.39it/s]Train epoch: 179 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.010405\n","449it [00:18, 17.81it/s]Train epoch: 179 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.012881\n","475it [00:20, 16.66it/s]Train epoch: 179 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.014402\n","499it [00:21, 15.79it/s]Train epoch: 179 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.012698\n","505it [00:22, 22.73it/s]\n","epoch loss: 0.012430600651267842\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 346.18it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3658, 0.5574, 0.4743, 0.5125, 0.8582\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3964, 0.6225, 0.5219, 0.5678, 0.8809\n","rec_at_5: 0.5416\n","prec_at_5: 0.5549\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 180\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 180 [batch #0, batch_size 16, seq length 212]\tLoss: 0.031079\n","23it [00:00, 29.85it/s]Train epoch: 180 [batch #25, batch_size 16, seq length 571]\tLoss: 0.010096\n","49it [00:01, 30.29it/s]Train epoch: 180 [batch #50, batch_size 16, seq length 709]\tLoss: 0.012602\n","74it [00:02, 28.02it/s]Train epoch: 180 [batch #75, batch_size 16, seq length 806]\tLoss: 0.013974\n","100it [00:03, 27.91it/s]Train epoch: 180 [batch #100, batch_size 16, seq length 892]\tLoss: 0.013814\n","124it [00:04, 27.43it/s]Train epoch: 180 [batch #125, batch_size 16, seq length 978]\tLoss: 0.011725\n","148it [00:05, 25.94it/s]Train epoch: 180 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.010891\n","175it [00:06, 25.16it/s]Train epoch: 180 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.010593\n","199it [00:07, 24.88it/s]Train epoch: 180 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.011052\n","223it [00:08, 24.04it/s]Train epoch: 180 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.013518\n","250it [00:09, 23.41it/s]Train epoch: 180 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.014082\n","274it [00:10, 22.71it/s]Train epoch: 180 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.012630\n","298it [00:11, 23.10it/s]Train epoch: 180 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.013424\n","325it [00:12, 22.01it/s]Train epoch: 180 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.012731\n","349it [00:13, 21.21it/s]Train epoch: 180 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.013963\n","373it [00:14, 21.21it/s]Train epoch: 180 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.012381\n","400it [00:16, 20.16it/s]Train epoch: 180 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.011662\n","423it [00:17, 19.50it/s]Train epoch: 180 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.009318\n","449it [00:18, 19.04it/s]Train epoch: 180 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.011764\n","475it [00:20, 17.50it/s]Train epoch: 180 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.013757\n","499it [00:21, 15.39it/s]Train epoch: 180 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.012851\n","505it [00:22, 22.85it/s]\n","epoch loss: 0.012251712167566403\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 343.49it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3667, 0.5542, 0.4769, 0.5127, 0.8579\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3948, 0.6185, 0.5219, 0.5661, 0.8807\n","rec_at_5: 0.5391\n","prec_at_5: 0.5537\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 181\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 181 [batch #0, batch_size 16, seq length 212]\tLoss: 0.038105\n","23it [00:00, 30.42it/s]Train epoch: 181 [batch #25, batch_size 16, seq length 571]\tLoss: 0.011827\n","47it [00:01, 29.98it/s]Train epoch: 181 [batch #50, batch_size 16, seq length 709]\tLoss: 0.011359\n","73it [00:02, 29.16it/s]Train epoch: 181 [batch #75, batch_size 16, seq length 806]\tLoss: 0.013360\n","99it [00:03, 27.53it/s]Train epoch: 181 [batch #100, batch_size 16, seq length 892]\tLoss: 0.014368\n","123it [00:04, 26.88it/s]Train epoch: 181 [batch #125, batch_size 16, seq length 978]\tLoss: 0.013148\n","150it [00:05, 26.57it/s]Train epoch: 181 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.012543\n","174it [00:06, 25.42it/s]Train epoch: 181 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.010367\n","198it [00:07, 25.06it/s]Train epoch: 181 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.010044\n","225it [00:08, 24.66it/s]Train epoch: 181 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.012035\n","249it [00:09, 23.07it/s]Train epoch: 181 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.011946\n","273it [00:10, 24.16it/s]Train epoch: 181 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.010418\n","300it [00:11, 22.11it/s]Train epoch: 181 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.012073\n","324it [00:12, 21.56it/s]Train epoch: 181 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.010675\n","348it [00:13, 20.95it/s]Train epoch: 181 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.013672\n","375it [00:15, 20.52it/s]Train epoch: 181 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.012213\n","399it [00:16, 20.29it/s]Train epoch: 181 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.013832\n","425it [00:17, 18.90it/s]Train epoch: 181 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.012318\n","450it [00:18, 18.81it/s]Train epoch: 181 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.012908\n","474it [00:20, 17.43it/s]Train epoch: 181 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.013183\n","500it [00:21, 16.47it/s]Train epoch: 181 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.011132\n","505it [00:22, 22.83it/s]\n","epoch loss: 0.012041836724291547\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 348.75it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3651, 0.5569, 0.4725, 0.5113, 0.8580\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3949, 0.6219, 0.5198, 0.5662, 0.8810\n","rec_at_5: 0.5405\n","prec_at_5: 0.5536\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 182\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 182 [batch #0, batch_size 16, seq length 212]\tLoss: 0.029281\n","24it [00:00, 30.02it/s]Train epoch: 182 [batch #25, batch_size 16, seq length 571]\tLoss: 0.009394\n","47it [00:01, 28.56it/s]Train epoch: 182 [batch #50, batch_size 16, seq length 709]\tLoss: 0.010419\n","72it [00:02, 28.05it/s]Train epoch: 182 [batch #75, batch_size 16, seq length 806]\tLoss: 0.013533\n","98it [00:03, 27.64it/s]Train epoch: 182 [batch #100, batch_size 16, seq length 892]\tLoss: 0.014957\n","125it [00:04, 27.37it/s]Train epoch: 182 [batch #125, batch_size 16, seq length 978]\tLoss: 0.009865\n","149it [00:05, 26.34it/s]Train epoch: 182 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.011122\n","173it [00:06, 25.38it/s]Train epoch: 182 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.010055\n","200it [00:07, 24.37it/s]Train epoch: 182 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.009400\n","224it [00:08, 24.83it/s]Train epoch: 182 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.011071\n","248it [00:09, 23.38it/s]Train epoch: 182 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.013490\n","275it [00:10, 23.88it/s]Train epoch: 182 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.010100\n","299it [00:11, 22.61it/s]Train epoch: 182 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.011557\n","323it [00:12, 22.13it/s]Train epoch: 182 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.013271\n","350it [00:13, 21.45it/s]Train epoch: 182 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.011752\n","374it [00:15, 21.45it/s]Train epoch: 182 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.012830\n","398it [00:16, 20.20it/s]Train epoch: 182 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.011338\n","425it [00:17, 19.36it/s]Train epoch: 182 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.010456\n","449it [00:18, 18.68it/s]Train epoch: 182 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.010946\n","475it [00:20, 17.57it/s]Train epoch: 182 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.013180\n","499it [00:21, 15.78it/s]Train epoch: 182 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.012089\n","505it [00:22, 22.69it/s]\n","epoch loss: 0.011637996884620758\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 346.78it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3666, 0.5563, 0.4760, 0.5131, 0.8578\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3968, 0.6204, 0.5240, 0.5681, 0.8809\n","rec_at_5: 0.5417\n","prec_at_5: 0.5538\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 183\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 183 [batch #0, batch_size 16, seq length 212]\tLoss: 0.039479\n","22it [00:00, 30.78it/s]Train epoch: 183 [batch #25, batch_size 16, seq length 571]\tLoss: 0.009955\n","50it [00:01, 30.07it/s]Train epoch: 183 [batch #50, batch_size 16, seq length 709]\tLoss: 0.012644\n","74it [00:02, 27.43it/s]Train epoch: 183 [batch #75, batch_size 16, seq length 806]\tLoss: 0.011497\n","98it [00:03, 27.54it/s]Train epoch: 183 [batch #100, batch_size 16, seq length 892]\tLoss: 0.010988\n","125it [00:04, 26.73it/s]Train epoch: 183 [batch #125, batch_size 16, seq length 978]\tLoss: 0.012539\n","149it [00:05, 26.10it/s]Train epoch: 183 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.011532\n","173it [00:06, 25.33it/s]Train epoch: 183 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.010815\n","200it [00:07, 25.53it/s]Train epoch: 183 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.009708\n","224it [00:08, 23.87it/s]Train epoch: 183 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.010506\n","248it [00:09, 24.01it/s]Train epoch: 183 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.011842\n","275it [00:10, 23.18it/s]Train epoch: 183 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.010547\n","299it [00:11, 22.07it/s]Train epoch: 183 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.012461\n","323it [00:12, 22.51it/s]Train epoch: 183 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.013077\n","350it [00:13, 21.18it/s]Train epoch: 183 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.011821\n","374it [00:15, 20.54it/s]Train epoch: 183 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.012515\n","398it [00:16, 19.64it/s]Train epoch: 183 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.009524\n","425it [00:17, 19.18it/s]Train epoch: 183 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.010644\n","449it [00:18, 18.72it/s]Train epoch: 183 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.012139\n","475it [00:20, 17.01it/s]Train epoch: 183 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.014430\n","499it [00:21, 15.94it/s]Train epoch: 183 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.010554\n","505it [00:22, 22.63it/s]\n","epoch loss: 0.011289646154866637\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 346.32it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3668, 0.5551, 0.4762, 0.5127, 0.8576\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3960, 0.6197, 0.5231, 0.5673, 0.8805\n","rec_at_5: 0.5398\n","prec_at_5: 0.5537\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 184\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 184 [batch #0, batch_size 16, seq length 212]\tLoss: 0.023917\n","23it [00:00, 29.58it/s]Train epoch: 184 [batch #25, batch_size 16, seq length 571]\tLoss: 0.013087\n","47it [00:01, 30.36it/s]Train epoch: 184 [batch #50, batch_size 16, seq length 709]\tLoss: 0.009699\n","73it [00:02, 28.39it/s]Train epoch: 184 [batch #75, batch_size 16, seq length 806]\tLoss: 0.012449\n","98it [00:03, 27.16it/s]Train epoch: 184 [batch #100, batch_size 16, seq length 892]\tLoss: 0.009918\n","125it [00:04, 27.07it/s]Train epoch: 184 [batch #125, batch_size 16, seq length 978]\tLoss: 0.012422\n","149it [00:05, 26.11it/s]Train epoch: 184 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.012370\n","173it [00:06, 24.90it/s]Train epoch: 184 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.010158\n","200it [00:07, 25.17it/s]Train epoch: 184 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.009699\n","224it [00:08, 24.19it/s]Train epoch: 184 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.012510\n","248it [00:09, 24.12it/s]Train epoch: 184 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.011640\n","275it [00:10, 23.49it/s]Train epoch: 184 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.011005\n","299it [00:11, 22.29it/s]Train epoch: 184 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.010053\n","323it [00:12, 22.63it/s]Train epoch: 184 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.012134\n","350it [00:13, 21.49it/s]Train epoch: 184 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.012446\n","374it [00:15, 20.38it/s]Train epoch: 184 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.012340\n","398it [00:16, 19.98it/s]Train epoch: 184 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.009952\n","424it [00:17, 19.45it/s]Train epoch: 184 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.011552\n","449it [00:18, 18.49it/s]Train epoch: 184 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.011675\n","475it [00:20, 17.17it/s]Train epoch: 184 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.013927\n","499it [00:21, 15.64it/s]Train epoch: 184 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.010533\n","505it [00:22, 22.68it/s]\n","epoch loss: 0.010967785614833124\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 344.57it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3674, 0.5559, 0.4770, 0.5134, 0.8576\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3974, 0.6210, 0.5246, 0.5688, 0.8808\n","rec_at_5: 0.5403\n","prec_at_5: 0.5531\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 185\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 185 [batch #0, batch_size 16, seq length 212]\tLoss: 0.023018\n","24it [00:00, 30.91it/s]Train epoch: 185 [batch #25, batch_size 16, seq length 571]\tLoss: 0.010062\n","48it [00:01, 28.66it/s]Train epoch: 185 [batch #50, batch_size 16, seq length 709]\tLoss: 0.009373\n","74it [00:02, 28.90it/s]Train epoch: 185 [batch #75, batch_size 16, seq length 806]\tLoss: 0.010196\n","99it [00:03, 28.48it/s]Train epoch: 185 [batch #100, batch_size 16, seq length 892]\tLoss: 0.011937\n","123it [00:04, 27.42it/s]Train epoch: 185 [batch #125, batch_size 16, seq length 978]\tLoss: 0.010723\n","150it [00:05, 26.19it/s]Train epoch: 185 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.013256\n","174it [00:06, 25.12it/s]Train epoch: 185 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.009940\n","198it [00:07, 23.44it/s]Train epoch: 185 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.010126\n","225it [00:08, 23.60it/s]Train epoch: 185 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.010373\n","249it [00:09, 23.41it/s]Train epoch: 185 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.011402\n","273it [00:10, 22.72it/s]Train epoch: 185 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.011838\n","300it [00:11, 23.19it/s]Train epoch: 185 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.010712\n","324it [00:12, 22.52it/s]Train epoch: 185 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.010339\n","348it [00:13, 21.32it/s]Train epoch: 185 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.011697\n","375it [00:15, 21.31it/s]Train epoch: 185 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.010953\n","399it [00:16, 20.09it/s]Train epoch: 185 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.009593\n","423it [00:17, 19.32it/s]Train epoch: 185 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.009028\n","450it [00:19, 18.17it/s]Train epoch: 185 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.011501\n","474it [00:20, 17.81it/s]Train epoch: 185 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.010779\n","500it [00:21, 15.46it/s]Train epoch: 185 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.009771\n","505it [00:22, 22.67it/s]\n","epoch loss: 0.010763976192660861\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 342.59it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3652, 0.5534, 0.4751, 0.5113, 0.8577\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3955, 0.6200, 0.5221, 0.5669, 0.8808\n","rec_at_5: 0.5410\n","prec_at_5: 0.5545\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 186\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 186 [batch #0, batch_size 16, seq length 212]\tLoss: 0.020056\n","23it [00:00, 29.21it/s]Train epoch: 186 [batch #25, batch_size 16, seq length 571]\tLoss: 0.008921\n","47it [00:01, 29.71it/s]Train epoch: 186 [batch #50, batch_size 16, seq length 709]\tLoss: 0.010763\n","75it [00:02, 27.14it/s]Train epoch: 186 [batch #75, batch_size 16, seq length 806]\tLoss: 0.011962\n","100it [00:03, 27.19it/s]Train epoch: 186 [batch #100, batch_size 16, seq length 892]\tLoss: 0.010820\n","124it [00:04, 27.97it/s]Train epoch: 186 [batch #125, batch_size 16, seq length 978]\tLoss: 0.009728\n","148it [00:05, 25.69it/s]Train epoch: 186 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.010321\n","175it [00:06, 25.36it/s]Train epoch: 186 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.009261\n","199it [00:07, 24.42it/s]Train epoch: 186 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.009071\n","223it [00:08, 24.88it/s]Train epoch: 186 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.010882\n","250it [00:09, 22.99it/s]Train epoch: 186 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.012707\n","274it [00:10, 23.43it/s]Train epoch: 186 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.010289\n","298it [00:11, 23.05it/s]Train epoch: 186 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.010378\n","325it [00:12, 21.43it/s]Train epoch: 186 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.009989\n","349it [00:13, 21.35it/s]Train epoch: 186 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.009724\n","373it [00:15, 20.88it/s]Train epoch: 186 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.010969\n","399it [00:16, 19.60it/s]Train epoch: 186 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.010982\n","425it [00:17, 19.31it/s]Train epoch: 186 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.010442\n","449it [00:18, 18.07it/s]Train epoch: 186 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.009906\n","475it [00:20, 17.45it/s]Train epoch: 186 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.011666\n","499it [00:21, 15.77it/s]Train epoch: 186 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.010581\n","505it [00:22, 22.66it/s]\n","epoch loss: 0.010550883149792726\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 344.68it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3669, 0.5550, 0.4771, 0.5131, 0.8575\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3959, 0.6200, 0.5227, 0.5672, 0.8807\n","rec_at_5: 0.5419\n","prec_at_5: 0.5558\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 187\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 187 [batch #0, batch_size 16, seq length 212]\tLoss: 0.036164\n","22it [00:00, 31.34it/s]Train epoch: 187 [batch #25, batch_size 16, seq length 571]\tLoss: 0.011517\n","50it [00:01, 29.11it/s]Train epoch: 187 [batch #50, batch_size 16, seq length 709]\tLoss: 0.010000\n","73it [00:02, 28.15it/s]Train epoch: 187 [batch #75, batch_size 16, seq length 806]\tLoss: 0.011819\n","98it [00:03, 27.90it/s]Train epoch: 187 [batch #100, batch_size 16, seq length 892]\tLoss: 0.010980\n","125it [00:04, 26.81it/s]Train epoch: 187 [batch #125, batch_size 16, seq length 978]\tLoss: 0.010442\n","149it [00:05, 26.53it/s]Train epoch: 187 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.009911\n","173it [00:06, 25.82it/s]Train epoch: 187 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.010033\n","200it [00:07, 24.40it/s]Train epoch: 187 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.009452\n","224it [00:08, 24.38it/s]Train epoch: 187 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.009689\n","248it [00:09, 23.31it/s]Train epoch: 187 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.009212\n","275it [00:10, 23.12it/s]Train epoch: 187 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.011268\n","299it [00:11, 22.36it/s]Train epoch: 187 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.010783\n","323it [00:12, 22.21it/s]Train epoch: 187 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.009679\n","350it [00:13, 21.23it/s]Train epoch: 187 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.009655\n","374it [00:15, 20.54it/s]Train epoch: 187 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.010954\n","398it [00:16, 19.74it/s]Train epoch: 187 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.008950\n","425it [00:17, 19.35it/s]Train epoch: 187 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.008335\n","450it [00:18, 18.91it/s]Train epoch: 187 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.009812\n","474it [00:20, 17.21it/s]Train epoch: 187 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.009307\n","500it [00:21, 15.52it/s]Train epoch: 187 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.010584\n","505it [00:22, 22.71it/s]\n","epoch loss: 0.01040005412397648\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 348.98it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3655, 0.5551, 0.4745, 0.5116, 0.8573\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3954, 0.6203, 0.5217, 0.5667, 0.8803\n","rec_at_5: 0.5393\n","prec_at_5: 0.5540\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 188\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 188 [batch #0, batch_size 16, seq length 212]\tLoss: 0.030856\n","25it [00:00, 31.35it/s]Train epoch: 188 [batch #25, batch_size 16, seq length 571]\tLoss: 0.009803\n","49it [00:01, 29.22it/s]Train epoch: 188 [batch #50, batch_size 16, seq length 709]\tLoss: 0.008323\n","75it [00:02, 27.98it/s]Train epoch: 188 [batch #75, batch_size 16, seq length 806]\tLoss: 0.012870\n","100it [00:03, 27.37it/s]Train epoch: 188 [batch #100, batch_size 16, seq length 892]\tLoss: 0.009988\n","124it [00:04, 26.22it/s]Train epoch: 188 [batch #125, batch_size 16, seq length 978]\tLoss: 0.009317\n","148it [00:05, 25.41it/s]Train epoch: 188 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.010778\n","175it [00:06, 24.67it/s]Train epoch: 188 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.010398\n","199it [00:07, 24.68it/s]Train epoch: 188 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.009693\n","223it [00:08, 24.23it/s]Train epoch: 188 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.013090\n","250it [00:09, 23.14it/s]Train epoch: 188 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.010868\n","274it [00:10, 22.10it/s]Train epoch: 188 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.011210\n","298it [00:11, 22.54it/s]Train epoch: 188 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.009876\n","325it [00:12, 22.35it/s]Train epoch: 188 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.008991\n","349it [00:13, 22.04it/s]Train epoch: 188 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.012585\n","373it [00:15, 20.67it/s]Train epoch: 188 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.010577\n","400it [00:16, 19.99it/s]Train epoch: 188 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.008449\n","424it [00:17, 18.98it/s]Train epoch: 188 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.009568\n","449it [00:18, 17.97it/s]Train epoch: 188 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.010996\n","475it [00:20, 17.14it/s]Train epoch: 188 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.011430\n","499it [00:22, 15.26it/s]Train epoch: 188 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.009530\n","505it [00:22, 22.53it/s]\n","epoch loss: 0.01025981761256952\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 345.82it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3662, 0.5540, 0.4765, 0.5123, 0.8572\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3955, 0.6187, 0.5229, 0.5668, 0.8801\n","rec_at_5: 0.5394\n","prec_at_5: 0.5542\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 189\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 189 [batch #0, batch_size 16, seq length 212]\tLoss: 0.022048\n","22it [00:00, 30.86it/s]Train epoch: 189 [batch #25, batch_size 16, seq length 571]\tLoss: 0.010322\n","50it [00:01, 29.19it/s]Train epoch: 189 [batch #50, batch_size 16, seq length 709]\tLoss: 0.008412\n","75it [00:02, 28.54it/s]Train epoch: 189 [batch #75, batch_size 16, seq length 806]\tLoss: 0.012309\n","100it [00:03, 28.14it/s]Train epoch: 189 [batch #100, batch_size 16, seq length 892]\tLoss: 0.012124\n","124it [00:04, 27.40it/s]Train epoch: 189 [batch #125, batch_size 16, seq length 978]\tLoss: 0.009766\n","148it [00:05, 25.97it/s]Train epoch: 189 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.010576\n","175it [00:06, 25.91it/s]Train epoch: 189 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.008467\n","199it [00:07, 24.95it/s]Train epoch: 189 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.007907\n","223it [00:08, 23.91it/s]Train epoch: 189 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.008888\n","250it [00:09, 23.79it/s]Train epoch: 189 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.012644\n","274it [00:10, 23.38it/s]Train epoch: 189 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.009400\n","298it [00:11, 23.47it/s]Train epoch: 189 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.010026\n","325it [00:12, 21.92it/s]Train epoch: 189 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.010487\n","349it [00:13, 21.33it/s]Train epoch: 189 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.010455\n","373it [00:14, 20.59it/s]Train epoch: 189 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.009082\n","400it [00:16, 20.22it/s]Train epoch: 189 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.010194\n","423it [00:17, 19.03it/s]Train epoch: 189 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.008270\n","450it [00:18, 18.70it/s]Train epoch: 189 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.008969\n","474it [00:20, 16.89it/s]Train epoch: 189 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.010536\n","500it [00:21, 15.64it/s]Train epoch: 189 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.009679\n","505it [00:22, 22.85it/s]\n","epoch loss: 0.00991085387482815\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 345.18it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3640, 0.5526, 0.4724, 0.5093, 0.8570\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3937, 0.6195, 0.5192, 0.5650, 0.8799\n","rec_at_5: 0.5398\n","prec_at_5: 0.5550\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 190\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 190 [batch #0, batch_size 16, seq length 212]\tLoss: 0.022040\n","23it [00:00, 31.01it/s]Train epoch: 190 [batch #25, batch_size 16, seq length 571]\tLoss: 0.007193\n","47it [00:01, 30.08it/s]Train epoch: 190 [batch #50, batch_size 16, seq length 709]\tLoss: 0.011530\n","73it [00:02, 28.80it/s]Train epoch: 190 [batch #75, batch_size 16, seq length 806]\tLoss: 0.009335\n","98it [00:03, 27.51it/s]Train epoch: 190 [batch #100, batch_size 16, seq length 892]\tLoss: 0.010303\n","125it [00:04, 26.94it/s]Train epoch: 190 [batch #125, batch_size 16, seq length 978]\tLoss: 0.011017\n","149it [00:05, 26.15it/s]Train epoch: 190 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.010277\n","173it [00:06, 25.50it/s]Train epoch: 190 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.009624\n","200it [00:07, 26.01it/s]Train epoch: 190 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.007790\n","224it [00:08, 24.39it/s]Train epoch: 190 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.010049\n","248it [00:09, 23.71it/s]Train epoch: 190 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.011173\n","275it [00:10, 23.28it/s]Train epoch: 190 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.008348\n","299it [00:11, 23.01it/s]Train epoch: 190 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.008413\n","323it [00:12, 21.91it/s]Train epoch: 190 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.009672\n","350it [00:13, 21.62it/s]Train epoch: 190 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.010180\n","374it [00:15, 20.63it/s]Train epoch: 190 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.011404\n","400it [00:16, 19.62it/s]Train epoch: 190 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.009803\n","425it [00:17, 19.14it/s]Train epoch: 190 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.009746\n","449it [00:18, 18.56it/s]Train epoch: 190 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.010136\n","474it [00:20, 17.51it/s]Train epoch: 190 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.009826\n","500it [00:21, 15.61it/s]Train epoch: 190 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.009664\n","505it [00:22, 22.69it/s]\n","epoch loss: 0.009671546731116526\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 349.41it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3652, 0.5556, 0.4740, 0.5116, 0.8570\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3952, 0.6200, 0.5215, 0.5665, 0.8799\n","rec_at_5: 0.5382\n","prec_at_5: 0.5524\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 191\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 191 [batch #0, batch_size 16, seq length 212]\tLoss: 0.020088\n","23it [00:00, 30.10it/s]Train epoch: 191 [batch #25, batch_size 16, seq length 571]\tLoss: 0.010369\n","47it [00:01, 30.10it/s]Train epoch: 191 [batch #50, batch_size 16, seq length 709]\tLoss: 0.009494\n","75it [00:02, 27.73it/s]Train epoch: 191 [batch #75, batch_size 16, seq length 806]\tLoss: 0.012822\n","100it [00:03, 27.11it/s]Train epoch: 191 [batch #100, batch_size 16, seq length 892]\tLoss: 0.009011\n","125it [00:04, 27.05it/s]Train epoch: 191 [batch #125, batch_size 16, seq length 978]\tLoss: 0.009149\n","149it [00:05, 25.99it/s]Train epoch: 191 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.009582\n","173it [00:06, 25.84it/s]Train epoch: 191 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.008003\n","200it [00:07, 24.97it/s]Train epoch: 191 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.010441\n","224it [00:08, 24.75it/s]Train epoch: 191 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.010192\n","248it [00:09, 23.81it/s]Train epoch: 191 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.010555\n","275it [00:10, 23.44it/s]Train epoch: 191 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.007406\n","299it [00:11, 22.16it/s]Train epoch: 191 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.010476\n","323it [00:12, 21.96it/s]Train epoch: 191 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.011268\n","350it [00:13, 21.04it/s]Train epoch: 191 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.010045\n","374it [00:14, 21.17it/s]Train epoch: 191 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.009012\n","398it [00:16, 20.10it/s]Train epoch: 191 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.011273\n","424it [00:17, 19.52it/s]Train epoch: 191 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.008663\n","450it [00:18, 18.12it/s]Train epoch: 191 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.009076\n","474it [00:20, 16.44it/s]Train epoch: 191 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.012337\n","500it [00:22, 14.92it/s]Train epoch: 191 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.008766\n","505it [00:22, 22.63it/s]\n","epoch loss: 0.00948582099506281\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 348.86it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3656, 0.5554, 0.4759, 0.5126, 0.8569\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3961, 0.6211, 0.5222, 0.5674, 0.8800\n","rec_at_5: 0.5387\n","prec_at_5: 0.5518\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 192\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 192 [batch #0, batch_size 16, seq length 212]\tLoss: 0.022663\n","23it [00:00, 30.81it/s]Train epoch: 192 [batch #25, batch_size 16, seq length 571]\tLoss: 0.009811\n","47it [00:01, 30.83it/s]Train epoch: 192 [batch #50, batch_size 16, seq length 709]\tLoss: 0.008409\n","75it [00:02, 28.84it/s]Train epoch: 192 [batch #75, batch_size 16, seq length 806]\tLoss: 0.011639\n","100it [00:03, 27.25it/s]Train epoch: 192 [batch #100, batch_size 16, seq length 892]\tLoss: 0.010860\n","124it [00:04, 26.47it/s]Train epoch: 192 [batch #125, batch_size 16, seq length 978]\tLoss: 0.007888\n","148it [00:05, 26.69it/s]Train epoch: 192 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.008879\n","175it [00:06, 25.33it/s]Train epoch: 192 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.009362\n","199it [00:07, 25.26it/s]Train epoch: 192 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.008591\n","223it [00:08, 23.89it/s]Train epoch: 192 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.008663\n","250it [00:09, 23.95it/s]Train epoch: 192 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.010090\n","274it [00:10, 23.41it/s]Train epoch: 192 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.009529\n","298it [00:11, 22.48it/s]Train epoch: 192 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.009184\n","325it [00:12, 22.13it/s]Train epoch: 192 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.008028\n","349it [00:13, 21.90it/s]Train epoch: 192 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.009734\n","373it [00:14, 20.96it/s]Train epoch: 192 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.008598\n","400it [00:16, 19.15it/s]Train epoch: 192 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.008085\n","425it [00:17, 19.03it/s]Train epoch: 192 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.009090\n","449it [00:18, 18.22it/s]Train epoch: 192 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.008378\n","475it [00:20, 16.89it/s]Train epoch: 192 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.009700\n","499it [00:21, 15.06it/s]Train epoch: 192 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.008273\n","505it [00:22, 22.73it/s]\n","epoch loss: 0.009147450056746557\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 347.65it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3665, 0.5524, 0.4780, 0.5125, 0.8566\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3965, 0.6180, 0.5252, 0.5678, 0.8797\n","rec_at_5: 0.5385\n","prec_at_5: 0.5530\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 193\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 193 [batch #0, batch_size 16, seq length 212]\tLoss: 0.023767\n","23it [00:00, 30.57it/s]Train epoch: 193 [batch #25, batch_size 16, seq length 571]\tLoss: 0.007815\n","47it [00:01, 30.12it/s]Train epoch: 193 [batch #50, batch_size 16, seq length 709]\tLoss: 0.007404\n","74it [00:02, 27.61it/s]Train epoch: 193 [batch #75, batch_size 16, seq length 806]\tLoss: 0.010751\n","100it [00:03, 27.10it/s]Train epoch: 193 [batch #100, batch_size 16, seq length 892]\tLoss: 0.010467\n","124it [00:04, 26.98it/s]Train epoch: 193 [batch #125, batch_size 16, seq length 978]\tLoss: 0.010281\n","148it [00:05, 27.01it/s]Train epoch: 193 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.010350\n","175it [00:06, 25.67it/s]Train epoch: 193 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.008491\n","199it [00:07, 24.58it/s]Train epoch: 193 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.007700\n","223it [00:08, 23.57it/s]Train epoch: 193 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.009100\n","250it [00:09, 22.62it/s]Train epoch: 193 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.010880\n","274it [00:10, 22.78it/s]Train epoch: 193 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.009664\n","298it [00:11, 22.48it/s]Train epoch: 193 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.010531\n","325it [00:12, 22.53it/s]Train epoch: 193 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.008220\n","349it [00:13, 21.63it/s]Train epoch: 193 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.008445\n","373it [00:15, 21.03it/s]Train epoch: 193 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.009014\n","400it [00:16, 20.30it/s]Train epoch: 193 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.008469\n","425it [00:17, 19.23it/s]Train epoch: 193 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.010587\n","449it [00:18, 18.65it/s]Train epoch: 193 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.008975\n","475it [00:20, 17.61it/s]Train epoch: 193 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.009147\n","499it [00:21, 15.69it/s]Train epoch: 193 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.007962\n","505it [00:22, 22.72it/s]\n","epoch loss: 0.009062585390246666\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 347.65it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3651, 0.5533, 0.4744, 0.5109, 0.8568\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3943, 0.6188, 0.5208, 0.5656, 0.8802\n","rec_at_5: 0.5398\n","prec_at_5: 0.5540\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 194\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 194 [batch #0, batch_size 16, seq length 212]\tLoss: 0.023403\n","23it [00:00, 30.74it/s]Train epoch: 194 [batch #25, batch_size 16, seq length 571]\tLoss: 0.009017\n","47it [00:01, 29.98it/s]Train epoch: 194 [batch #50, batch_size 16, seq length 709]\tLoss: 0.008818\n","73it [00:02, 28.71it/s]Train epoch: 194 [batch #75, batch_size 16, seq length 806]\tLoss: 0.009005\n","99it [00:03, 28.27it/s]Train epoch: 194 [batch #100, batch_size 16, seq length 892]\tLoss: 0.009739\n","123it [00:04, 26.93it/s]Train epoch: 194 [batch #125, batch_size 16, seq length 978]\tLoss: 0.009045\n","150it [00:05, 26.19it/s]Train epoch: 194 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.008364\n","174it [00:06, 25.12it/s]Train epoch: 194 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.008729\n","198it [00:07, 25.47it/s]Train epoch: 194 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.007643\n","225it [00:08, 24.89it/s]Train epoch: 194 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.007754\n","249it [00:09, 23.80it/s]Train epoch: 194 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.010733\n","273it [00:10, 23.06it/s]Train epoch: 194 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.007962\n","300it [00:11, 22.68it/s]Train epoch: 194 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.008862\n","324it [00:12, 21.86it/s]Train epoch: 194 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.007794\n","348it [00:13, 21.59it/s]Train epoch: 194 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.009101\n","375it [00:15, 20.58it/s]Train epoch: 194 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.008816\n","399it [00:16, 20.47it/s]Train epoch: 194 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.008145\n","424it [00:17, 19.14it/s]Train epoch: 194 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.009204\n","449it [00:18, 18.97it/s]Train epoch: 194 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.007074\n","475it [00:20, 17.42it/s]Train epoch: 194 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.010329\n","499it [00:21, 15.27it/s]Train epoch: 194 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.009822\n","505it [00:22, 22.76it/s]\n","epoch loss: 0.008835135474899038\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 347.37it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3641, 0.5544, 0.4723, 0.5100, 0.8568\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3927, 0.6207, 0.5166, 0.5639, 0.8797\n","rec_at_5: 0.5357\n","prec_at_5: 0.5513\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 195\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 195 [batch #0, batch_size 16, seq length 212]\tLoss: 0.018841\n","22it [00:00, 30.59it/s]Train epoch: 195 [batch #25, batch_size 16, seq length 571]\tLoss: 0.006893\n","48it [00:01, 28.68it/s]Train epoch: 195 [batch #50, batch_size 16, seq length 709]\tLoss: 0.007859\n","74it [00:02, 28.56it/s]Train epoch: 195 [batch #75, batch_size 16, seq length 806]\tLoss: 0.009823\n","99it [00:03, 27.50it/s]Train epoch: 195 [batch #100, batch_size 16, seq length 892]\tLoss: 0.009821\n","123it [00:04, 27.01it/s]Train epoch: 195 [batch #125, batch_size 16, seq length 978]\tLoss: 0.007881\n","150it [00:05, 24.74it/s]Train epoch: 195 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.009831\n","174it [00:06, 24.71it/s]Train epoch: 195 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.007153\n","198it [00:07, 25.52it/s]Train epoch: 195 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.007642\n","225it [00:08, 23.62it/s]Train epoch: 195 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.008859\n","249it [00:09, 23.23it/s]Train epoch: 195 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.012881\n","273it [00:10, 23.53it/s]Train epoch: 195 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.008579\n","300it [00:11, 22.18it/s]Train epoch: 195 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.007636\n","324it [00:12, 22.44it/s]Train epoch: 195 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.008416\n","348it [00:13, 21.79it/s]Train epoch: 195 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.007492\n","375it [00:15, 20.94it/s]Train epoch: 195 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.008706\n","399it [00:16, 20.62it/s]Train epoch: 195 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.006797\n","423it [00:17, 20.02it/s]Train epoch: 195 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.008531\n","449it [00:18, 18.43it/s]Train epoch: 195 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.010152\n","475it [00:20, 17.60it/s]Train epoch: 195 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.009252\n","499it [00:21, 15.26it/s]Train epoch: 195 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.008254\n","505it [00:22, 22.78it/s]\n","epoch loss: 0.008405664747080646\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 351.48it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3664, 0.5532, 0.4763, 0.5119, 0.8568\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3955, 0.6188, 0.5228, 0.5668, 0.8801\n","rec_at_5: 0.5379\n","prec_at_5: 0.5527\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 196\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 196 [batch #0, batch_size 16, seq length 212]\tLoss: 0.015958\n","24it [00:00, 29.92it/s]Train epoch: 196 [batch #25, batch_size 16, seq length 571]\tLoss: 0.007946\n","48it [00:01, 29.73it/s]Train epoch: 196 [batch #50, batch_size 16, seq length 709]\tLoss: 0.008691\n","74it [00:02, 27.47it/s]Train epoch: 196 [batch #75, batch_size 16, seq length 806]\tLoss: 0.009521\n","98it [00:03, 27.48it/s]Train epoch: 196 [batch #100, batch_size 16, seq length 892]\tLoss: 0.009430\n","125it [00:04, 27.70it/s]Train epoch: 196 [batch #125, batch_size 16, seq length 978]\tLoss: 0.008292\n","149it [00:05, 25.67it/s]Train epoch: 196 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.010136\n","173it [00:06, 25.97it/s]Train epoch: 196 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.007230\n","200it [00:07, 25.37it/s]Train epoch: 196 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.006781\n","224it [00:08, 25.06it/s]Train epoch: 196 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.009577\n","248it [00:09, 23.74it/s]Train epoch: 196 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.009078\n","275it [00:10, 23.79it/s]Train epoch: 196 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.008677\n","299it [00:11, 22.35it/s]Train epoch: 196 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.007813\n","323it [00:12, 21.66it/s]Train epoch: 196 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.007257\n","350it [00:13, 21.23it/s]Train epoch: 196 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.010448\n","374it [00:15, 20.31it/s]Train epoch: 196 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.009151\n","398it [00:16, 20.48it/s]Train epoch: 196 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.006911\n","423it [00:17, 19.00it/s]Train epoch: 196 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.008035\n","450it [00:19, 18.36it/s]Train epoch: 196 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.008341\n","474it [00:20, 17.32it/s]Train epoch: 196 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.009198\n","500it [00:21, 15.44it/s]Train epoch: 196 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.007132\n","505it [00:22, 22.65it/s]\n","epoch loss: 0.008342372798729725\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 351.77it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3659, 0.5514, 0.4769, 0.5115, 0.8566\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3954, 0.6173, 0.5238, 0.5667, 0.8802\n","rec_at_5: 0.5394\n","prec_at_5: 0.5528\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 197\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 197 [batch #0, batch_size 16, seq length 212]\tLoss: 0.023007\n","22it [00:00, 30.57it/s]Train epoch: 197 [batch #25, batch_size 16, seq length 571]\tLoss: 0.009142\n","49it [00:01, 30.40it/s]Train epoch: 197 [batch #50, batch_size 16, seq length 709]\tLoss: 0.008213\n","74it [00:02, 28.77it/s]Train epoch: 197 [batch #75, batch_size 16, seq length 806]\tLoss: 0.008908\n","99it [00:03, 27.72it/s]Train epoch: 197 [batch #100, batch_size 16, seq length 892]\tLoss: 0.008874\n","123it [00:04, 27.43it/s]Train epoch: 197 [batch #125, batch_size 16, seq length 978]\tLoss: 0.007826\n","150it [00:05, 25.92it/s]Train epoch: 197 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.009182\n","174it [00:06, 25.68it/s]Train epoch: 197 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.007468\n","198it [00:07, 24.21it/s]Train epoch: 197 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.006196\n","225it [00:08, 24.41it/s]Train epoch: 197 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.008407\n","249it [00:09, 23.43it/s]Train epoch: 197 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.007793\n","273it [00:10, 23.50it/s]Train epoch: 197 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.007481\n","300it [00:11, 22.21it/s]Train epoch: 197 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.008542\n","324it [00:12, 21.70it/s]Train epoch: 197 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.007842\n","348it [00:13, 21.13it/s]Train epoch: 197 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.007233\n","375it [00:15, 20.92it/s]Train epoch: 197 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.009562\n","399it [00:16, 19.76it/s]Train epoch: 197 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.008025\n","423it [00:17, 19.30it/s]Train epoch: 197 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.008724\n","449it [00:18, 18.63it/s]Train epoch: 197 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.008764\n","474it [00:20, 17.29it/s]Train epoch: 197 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.008402\n","500it [00:21, 15.44it/s]Train epoch: 197 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.006958\n","505it [00:22, 22.70it/s]\n","epoch loss: 0.00831867587005738\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 343.56it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3619, 0.5523, 0.4692, 0.5073, 0.8566\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3932, 0.6192, 0.5186, 0.5645, 0.8798\n","rec_at_5: 0.5352\n","prec_at_5: 0.5507\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 198\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 198 [batch #0, batch_size 16, seq length 212]\tLoss: 0.025650\n","23it [00:00, 30.65it/s]Train epoch: 198 [batch #25, batch_size 16, seq length 571]\tLoss: 0.006659\n","47it [00:01, 30.25it/s]Train epoch: 198 [batch #50, batch_size 16, seq length 709]\tLoss: 0.005922\n","74it [00:02, 28.35it/s]Train epoch: 198 [batch #75, batch_size 16, seq length 806]\tLoss: 0.009884\n","98it [00:03, 28.41it/s]Train epoch: 198 [batch #100, batch_size 16, seq length 892]\tLoss: 0.008876\n","125it [00:04, 27.65it/s]Train epoch: 198 [batch #125, batch_size 16, seq length 978]\tLoss: 0.009184\n","149it [00:05, 26.73it/s]Train epoch: 198 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.009193\n","173it [00:06, 25.13it/s]Train epoch: 198 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.008994\n","200it [00:07, 24.28it/s]Train epoch: 198 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.007373\n","224it [00:08, 24.10it/s]Train epoch: 198 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.010572\n","248it [00:09, 23.16it/s]Train epoch: 198 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.009432\n","275it [00:10, 22.83it/s]Train epoch: 198 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.006372\n","299it [00:11, 22.45it/s]Train epoch: 198 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.006594\n","323it [00:12, 22.52it/s]Train epoch: 198 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.006704\n","350it [00:13, 22.07it/s]Train epoch: 198 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.008387\n","374it [00:14, 21.28it/s]Train epoch: 198 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.010695\n","399it [00:16, 19.45it/s]Train epoch: 198 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.007222\n","424it [00:17, 19.73it/s]Train epoch: 198 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.007713\n","450it [00:18, 18.71it/s]Train epoch: 198 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.007270\n","474it [00:20, 17.59it/s]Train epoch: 198 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.008209\n","500it [00:21, 14.89it/s]Train epoch: 198 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.007167\n","505it [00:22, 22.75it/s]\n","epoch loss: 0.008092251867701364\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 343.71it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3619, 0.5511, 0.4708, 0.5078, 0.8567\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3933, 0.6178, 0.5199, 0.5646, 0.8799\n","rec_at_5: 0.5357\n","prec_at_5: 0.5507\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","EPOCH 199\n","0it [00:00, ?it/s]/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","Train epoch: 199 [batch #0, batch_size 16, seq length 212]\tLoss: 0.025552\n","23it [00:00, 30.44it/s]Train epoch: 199 [batch #25, batch_size 16, seq length 571]\tLoss: 0.005583\n","50it [00:01, 29.12it/s]Train epoch: 199 [batch #50, batch_size 16, seq length 709]\tLoss: 0.007534\n","74it [00:02, 28.61it/s]Train epoch: 199 [batch #75, batch_size 16, seq length 806]\tLoss: 0.010387\n","100it [00:03, 27.84it/s]Train epoch: 199 [batch #100, batch_size 16, seq length 892]\tLoss: 0.009893\n","124it [00:04, 26.57it/s]Train epoch: 199 [batch #125, batch_size 16, seq length 978]\tLoss: 0.007961\n","148it [00:05, 25.96it/s]Train epoch: 199 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.009730\n","175it [00:06, 24.53it/s]Train epoch: 199 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.006646\n","199it [00:07, 23.89it/s]Train epoch: 199 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.005418\n","223it [00:08, 24.01it/s]Train epoch: 199 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.008171\n","250it [00:09, 23.99it/s]Train epoch: 199 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.008323\n","274it [00:10, 23.47it/s]Train epoch: 199 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.007039\n","298it [00:11, 23.02it/s]Train epoch: 199 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.006919\n","325it [00:12, 21.92it/s]Train epoch: 199 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.006695\n","349it [00:13, 22.35it/s]Train epoch: 199 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.008224\n","373it [00:15, 21.29it/s]Train epoch: 199 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.006664\n","400it [00:16, 20.27it/s]Train epoch: 199 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.007543\n","424it [00:17, 19.41it/s]Train epoch: 199 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.006973\n","449it [00:18, 18.21it/s]Train epoch: 199 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.006749\n","475it [00:20, 17.10it/s]Train epoch: 199 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.009353\n","499it [00:21, 15.40it/s]Train epoch: 199 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.007354\n","505it [00:22, 22.70it/s]\n","epoch loss: 0.007814491953929002\n","last epoch: testing on test and train sets\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:04, 345.99it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3638, 0.5493, 0.4748, 0.5093, 0.8565\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3940, 0.6163, 0.5221, 0.5653, 0.8797\n","rec_at_5: 0.5366\n","prec_at_5: 0.5519\n","\n","\n","evaluating on test\n","file for evaluation: ../../mimicdata/mimic3/test_50.csv\n","1729it [00:05, 314.26it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3711, 0.5595, 0.4850, 0.5196, 0.8530\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3958, 0.6169, 0.5247, 0.5671, 0.8788\n","rec_at_5: 0.5302\n","prec_at_5: 0.5603\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/conv_attn_May_03_021908\n","\n","TOTAL ELAPSED TIME FOR conv_attn MODEL AND 200 EPOCHS: 5369.860424\n"]}]},{"cell_type":"markdown","source":["### CNN_mimic3_full\n","No starting model"],"metadata":{"id":"gzBtqfRbsPZZ"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"YwCFXMYIx-82","outputId":"69b5d0be-3bdd-486e-fb70-7c2d0e6d0515"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/predictions/CNN_mimic3_full\n","ARGS: Namespace(Y='full', batch_size=16, bidirectional=None, cell_type='gru', code_emb=None, command='python ../../learn/training.py ../../mimicdata/mimic3/train_full.csv ../../mimicdata/mimic3/vocab.csv full cnn_vanilla 100 --filter-size 4 --num-filter-maps 500 --dropout 0.2 --lr 0.003 --embed-file ../../mimicdata/mimic3/processed_full.embed --patience 10 --criterion prec_at_8 --gpu', criterion='prec_at_8', data_path='../../mimicdata/mimic3/train_full.csv', dropout=0.2, embed_file='../../mimicdata/mimic3/processed_full.embed', embed_size=100, filter_size='4', gpu=True, lmbda=0, lr=0.003, model='cnn_vanilla', n_epochs=100, num_filter_maps=500, patience=10, pool=None, public_model=None, quiet=None, rnn_dim=128, rnn_layers=1, samples=None, stack_filters=None, test_model=None, version='mimic3', vocab='../../mimicdata/mimic3/vocab.csv', weight_decay=0)\n","loading lookups...\n","loading pretrained embeddings...\n","adding unk embedding\n","VanillaConv(\n","  (embed_drop): Dropout(p=0.2, inplace=False)\n","  (embed): Embedding(51919, 100, padding_idx=0)\n","  (conv): Conv1d(100, 500, kernel_size=(4,), stride=(1,))\n","  (fc): Linear(in_features=500, out_features=8921, bias=True)\n",")\n","EPOCH 0\n","0it [00:00, ?it/s]Train epoch: 0 [batch #0, batch_size 16, seq length 117]\tLoss: 0.694051\n","25it [00:00, 67.45it/s]Train epoch: 0 [batch #25, batch_size 16, seq length 337]\tLoss: 0.009741\n","43it [00:00, 75.40it/s]Train epoch: 0 [batch #50, batch_size 16, seq length 402]\tLoss: 0.008226\n","69it [00:00, 79.68it/s]Train epoch: 0 [batch #75, batch_size 16, seq length 452]\tLoss: 0.006811\n","93it [00:01, 77.99it/s]Train epoch: 0 [batch #100, batch_size 16, seq length 490]\tLoss: 0.006265\n","125it [00:01, 74.82it/s]Train epoch: 0 [batch #125, batch_size 16, seq length 520]\tLoss: 0.006689\n","149it [00:02, 71.34it/s]Train epoch: 0 [batch #150, batch_size 16, seq length 548]\tLoss: 0.006689\n","173it [00:02, 69.43it/s]Train epoch: 0 [batch #175, batch_size 16, seq length 574]\tLoss: 0.006185\n","194it [00:02, 67.89it/s]Train epoch: 0 [batch #200, batch_size 16, seq length 596]\tLoss: 0.006387\n","222it [00:03, 66.30it/s]Train epoch: 0 [batch #225, batch_size 16, seq length 618]\tLoss: 0.006517\n","250it [00:03, 64.52it/s]Train epoch: 0 [batch #250, batch_size 16, seq length 638]\tLoss: 0.005973\n","271it [00:03, 63.80it/s]Train epoch: 0 [batch #275, batch_size 16, seq length 656]\tLoss: 0.006517\n","299it [00:04, 63.79it/s]Train epoch: 0 [batch #300, batch_size 16, seq length 674]\tLoss: 0.006552\n","320it [00:04, 63.11it/s]Train epoch: 0 [batch #325, batch_size 16, seq length 693]\tLoss: 0.006004\n","348it [00:05, 60.44it/s]Train epoch: 0 [batch #350, batch_size 16, seq length 710]\tLoss: 0.006189\n","369it [00:05, 60.28it/s]Train epoch: 0 [batch #375, batch_size 16, seq length 726]\tLoss: 0.006970\n","400it [00:06, 59.17it/s]Train epoch: 0 [batch #400, batch_size 16, seq length 743]\tLoss: 0.006135\n","424it [00:06, 58.51it/s]Train epoch: 0 [batch #425, batch_size 16, seq length 758]\tLoss: 0.006106\n","448it [00:06, 57.41it/s]Train epoch: 0 [batch #450, batch_size 16, seq length 773]\tLoss: 0.006405\n","472it [00:07, 56.37it/s]Train epoch: 0 [batch #475, batch_size 16, seq length 789]\tLoss: 0.005747\n","496it [00:07, 56.24it/s]Train epoch: 0 [batch #500, batch_size 16, seq length 803]\tLoss: 0.006226\n","520it [00:08, 55.50it/s]Train epoch: 0 [batch #525, batch_size 16, seq length 818]\tLoss: 0.006232\n","550it [00:08, 55.32it/s]Train epoch: 0 [batch #550, batch_size 16, seq length 833]\tLoss: 0.006062\n","574it [00:09, 55.10it/s]Train epoch: 0 [batch #575, batch_size 16, seq length 846]\tLoss: 0.005936\n","598it [00:09, 54.61it/s]Train epoch: 0 [batch #600, batch_size 16, seq length 860]\tLoss: 0.006894\n","622it [00:09, 54.03it/s]Train epoch: 0 [batch #625, batch_size 16, seq length 874]\tLoss: 0.006517\n","646it [00:10, 53.07it/s]Train epoch: 0 [batch #650, batch_size 16, seq length 887]\tLoss: 0.005946\n","670it [00:10, 52.64it/s]Train epoch: 0 [batch #675, batch_size 16, seq length 901]\tLoss: 0.006238\n","700it [00:11, 52.53it/s]Train epoch: 0 [batch #700, batch_size 16, seq length 915]\tLoss: 0.006042\n","724it [00:11, 52.48it/s]Train epoch: 0 [batch #725, batch_size 16, seq length 929]\tLoss: 0.006508\n","748it [00:12, 51.30it/s]Train epoch: 0 [batch #750, batch_size 16, seq length 942]\tLoss: 0.006663\n","772it [00:12, 49.40it/s]Train epoch: 0 [batch #775, batch_size 16, seq length 954]\tLoss: 0.006774\n","797it [00:13, 48.27it/s]Train epoch: 0 [batch #800, batch_size 16, seq length 968]\tLoss: 0.006531\n","822it [00:13, 48.47it/s]Train epoch: 0 [batch #825, batch_size 16, seq length 981]\tLoss: 0.006728\n","847it [00:14, 48.20it/s]Train epoch: 0 [batch #850, batch_size 16, seq length 994]\tLoss: 0.006683\n","873it [00:14, 48.74it/s]Train epoch: 0 [batch #875, batch_size 16, seq length 1006]\tLoss: 0.006289\n","898it [00:15, 48.07it/s]Train epoch: 0 [batch #900, batch_size 16, seq length 1020]\tLoss: 0.006840\n","923it [00:16, 46.32it/s]Train epoch: 0 [batch #925, batch_size 16, seq length 1033]\tLoss: 0.006510\n","948it [00:16, 46.58it/s]Train epoch: 0 [batch #950, batch_size 16, seq length 1046]\tLoss: 0.006944\n","973it [00:17, 46.64it/s]Train epoch: 0 [batch #975, batch_size 16, seq length 1059]\tLoss: 0.006554\n","998it [00:17, 45.62it/s]Train epoch: 0 [batch #1000, batch_size 16, seq length 1073]\tLoss: 0.007024\n","1023it [00:18, 46.11it/s]Train epoch: 0 [batch #1025, batch_size 16, seq length 1085]\tLoss: 0.006811\n","1048it [00:18, 44.88it/s]Train epoch: 0 [batch #1050, batch_size 16, seq length 1099]\tLoss: 0.006886\n","1073it [00:19, 45.00it/s]Train epoch: 0 [batch #1075, batch_size 16, seq length 1113]\tLoss: 0.006913\n","1098it [00:19, 44.21it/s]Train epoch: 0 [batch #1100, batch_size 16, seq length 1126]\tLoss: 0.006787\n","1123it [00:20, 44.07it/s]Train epoch: 0 [batch #1125, batch_size 16, seq length 1140]\tLoss: 0.006981\n","1148it [00:21, 42.37it/s]Train epoch: 0 [batch #1150, batch_size 16, seq length 1154]\tLoss: 0.006988\n","1173it [00:21, 43.03it/s]Train epoch: 0 [batch #1175, batch_size 16, seq length 1167]\tLoss: 0.006904\n","1198it [00:22, 42.54it/s]Train epoch: 0 [batch #1200, batch_size 16, seq length 1180]\tLoss: 0.006959\n","1223it [00:22, 40.45it/s]Train epoch: 0 [batch #1225, batch_size 16, seq length 1195]\tLoss: 0.006736\n","1248it [00:23, 41.77it/s]Train epoch: 0 [batch #1250, batch_size 16, seq length 1208]\tLoss: 0.006654\n","1273it [00:24, 40.98it/s]Train epoch: 0 [batch #1275, batch_size 16, seq length 1220]\tLoss: 0.006579\n","1298it [00:24, 41.60it/s]Train epoch: 0 [batch #1300, batch_size 16, seq length 1234]\tLoss: 0.006906\n","1323it [00:25, 41.15it/s]Train epoch: 0 [batch #1325, batch_size 16, seq length 1248]\tLoss: 0.006473\n","1348it [00:25, 40.45it/s]Train epoch: 0 [batch #1350, batch_size 16, seq length 1262]\tLoss: 0.007011\n","1375it [00:26, 39.85it/s]Train epoch: 0 [batch #1375, batch_size 16, seq length 1276]\tLoss: 0.007724\n","1399it [00:27, 38.86it/s]Train epoch: 0 [batch #1400, batch_size 16, seq length 1290]\tLoss: 0.007167\n","1423it [00:27, 38.78it/s]Train epoch: 0 [batch #1425, batch_size 16, seq length 1304]\tLoss: 0.007150\n","1447it [00:28, 38.71it/s]Train epoch: 0 [batch #1450, batch_size 16, seq length 1319]\tLoss: 0.007065\n","1475it [00:29, 37.92it/s]Train epoch: 0 [batch #1475, batch_size 16, seq length 1332]\tLoss: 0.007539\n","1499it [00:29, 38.02it/s]Train epoch: 0 [batch #1500, batch_size 16, seq length 1346]\tLoss: 0.007189\n","1523it [00:30, 37.39it/s]Train epoch: 0 [batch #1525, batch_size 16, seq length 1362]\tLoss: 0.006531\n","1547it [00:31, 36.67it/s]Train epoch: 0 [batch #1550, batch_size 16, seq length 1377]\tLoss: 0.007422\n","1575it [00:31, 37.54it/s]Train epoch: 0 [batch #1575, batch_size 16, seq length 1392]\tLoss: 0.006728\n","1599it [00:32, 37.15it/s]Train epoch: 0 [batch #1600, batch_size 16, seq length 1406]\tLoss: 0.006857\n","1623it [00:33, 37.05it/s]Train epoch: 0 [batch #1625, batch_size 16, seq length 1422]\tLoss: 0.007295\n","1647it [00:33, 36.34it/s]Train epoch: 0 [batch #1650, batch_size 16, seq length 1437]\tLoss: 0.007037\n","1675it [00:34, 36.08it/s]Train epoch: 0 [batch #1675, batch_size 16, seq length 1454]\tLoss: 0.007359\n","1699it [00:35, 35.24it/s]Train epoch: 0 [batch #1700, batch_size 16, seq length 1469]\tLoss: 0.007463\n","1723it [00:35, 35.48it/s]Train epoch: 0 [batch #1725, batch_size 16, seq length 1484]\tLoss: 0.006666\n","1747it [00:36, 35.50it/s]Train epoch: 0 [batch #1750, batch_size 16, seq length 1501]\tLoss: 0.006818\n","1775it [00:37, 34.57it/s]Train epoch: 0 [batch #1775, batch_size 16, seq length 1518]\tLoss: 0.007307\n","1799it [00:38, 34.14it/s]Train epoch: 0 [batch #1800, batch_size 16, seq length 1533]\tLoss: 0.007450\n","1823it [00:38, 34.26it/s]Train epoch: 0 [batch #1825, batch_size 16, seq length 1548]\tLoss: 0.007570\n","1847it [00:39, 33.96it/s]Train epoch: 0 [batch #1850, batch_size 16, seq length 1566]\tLoss: 0.007043\n","1875it [00:40, 32.92it/s]Train epoch: 0 [batch #1875, batch_size 16, seq length 1582]\tLoss: 0.007263\n","1899it [00:41, 33.31it/s]Train epoch: 0 [batch #1900, batch_size 16, seq length 1599]\tLoss: 0.007935\n","1923it [00:41, 32.75it/s]Train epoch: 0 [batch #1925, batch_size 16, seq length 1616]\tLoss: 0.007401\n","1947it [00:42, 32.62it/s]Train epoch: 0 [batch #1950, batch_size 16, seq length 1633]\tLoss: 0.007352\n","1975it [00:43, 32.07it/s]Train epoch: 0 [batch #1975, batch_size 16, seq length 1651]\tLoss: 0.007286\n","1999it [00:44, 32.37it/s]Train epoch: 0 [batch #2000, batch_size 16, seq length 1670]\tLoss: 0.007057\n","2023it [00:44, 32.03it/s]Train epoch: 0 [batch #2025, batch_size 16, seq length 1688]\tLoss: 0.007120\n","2047it [00:45, 31.61it/s]Train epoch: 0 [batch #2050, batch_size 16, seq length 1707]\tLoss: 0.007412\n","2075it [00:46, 31.51it/s]Train epoch: 0 [batch #2075, batch_size 16, seq length 1728]\tLoss: 0.007540\n","2099it [00:47, 31.47it/s]Train epoch: 0 [batch #2100, batch_size 16, seq length 1748]\tLoss: 0.006937\n","2123it [00:48, 31.03it/s]Train epoch: 0 [batch #2125, batch_size 16, seq length 1768]\tLoss: 0.007890\n","2147it [00:48, 30.23it/s]Train epoch: 0 [batch #2150, batch_size 16, seq length 1787]\tLoss: 0.007208\n","2174it [00:49, 29.92it/s]Train epoch: 0 [batch #2175, batch_size 16, seq length 1809]\tLoss: 0.008065\n","2199it [00:50, 29.73it/s]Train epoch: 0 [batch #2200, batch_size 16, seq length 1833]\tLoss: 0.007431\n","2223it [00:51, 29.70it/s]Train epoch: 0 [batch #2225, batch_size 16, seq length 1855]\tLoss: 0.007501\n","2250it [00:52, 29.15it/s]Train epoch: 0 [batch #2250, batch_size 16, seq length 1878]\tLoss: 0.008232\n","2274it [00:53, 29.37it/s]Train epoch: 0 [batch #2275, batch_size 16, seq length 1901]\tLoss: 0.007593\n","2298it [00:54, 28.31it/s]Train epoch: 0 [batch #2300, batch_size 16, seq length 1927]\tLoss: 0.007684\n","2325it [00:54, 28.59it/s]Train epoch: 0 [batch #2325, batch_size 16, seq length 1953]\tLoss: 0.008418\n","2349it [00:55, 27.70it/s]Train epoch: 0 [batch #2350, batch_size 16, seq length 1980]\tLoss: 0.007825\n","2373it [00:56, 27.01it/s]Train epoch: 0 [batch #2375, batch_size 16, seq length 2006]\tLoss: 0.008234\n","2400it [00:57, 26.87it/s]Train epoch: 0 [batch #2400, batch_size 16, seq length 2034]\tLoss: 0.008130\n","2424it [00:58, 27.34it/s]Train epoch: 0 [batch #2425, batch_size 16, seq length 2064]\tLoss: 0.007861\n","2448it [00:59, 26.99it/s]Train epoch: 0 [batch #2450, batch_size 16, seq length 2094]\tLoss: 0.008111\n","2475it [01:00, 26.26it/s]Train epoch: 0 [batch #2475, batch_size 16, seq length 2126]\tLoss: 0.008536\n","2499it [01:01, 26.15it/s]Train epoch: 0 [batch #2500, batch_size 16, seq length 2160]\tLoss: 0.008430\n","2523it [01:02, 25.18it/s]Train epoch: 0 [batch #2525, batch_size 16, seq length 2192]\tLoss: 0.008460\n","2550it [01:03, 25.73it/s]Train epoch: 0 [batch #2550, batch_size 16, seq length 2228]\tLoss: 0.008278\n","2574it [01:04, 25.53it/s]Train epoch: 0 [batch #2575, batch_size 16, seq length 2266]\tLoss: 0.008677\n","2598it [01:05, 24.93it/s]Train epoch: 0 [batch #2600, batch_size 16, seq length 2307]\tLoss: 0.009227\n","2625it [01:06, 24.32it/s]Train epoch: 0 [batch #2625, batch_size 16, seq length 2350]\tLoss: 0.008437\n","2649it [01:07, 23.77it/s]Train epoch: 0 [batch #2650, batch_size 16, seq length 2392]\tLoss: 0.008297\n","2673it [01:08, 23.72it/s]Train epoch: 0 [batch #2675, batch_size 16, seq length 2439]\tLoss: 0.008656\n","2700it [01:09, 23.74it/s]Train epoch: 0 [batch #2700, batch_size 16, seq length 2491]\tLoss: 0.008809\n","2724it [01:10, 23.34it/s]Train epoch: 0 [batch #2725, batch_size 16, seq length 2500]\tLoss: 0.009050\n","2748it [01:11, 23.24it/s]Train epoch: 0 [batch #2750, batch_size 16, seq length 2500]\tLoss: 0.009000\n","2775it [01:12, 23.01it/s]Train epoch: 0 [batch #2775, batch_size 16, seq length 2500]\tLoss: 0.009586\n","2799it [01:13, 22.63it/s]Train epoch: 0 [batch #2800, batch_size 16, seq length 2500]\tLoss: 0.009777\n","2823it [01:14, 22.73it/s]Train epoch: 0 [batch #2825, batch_size 16, seq length 2500]\tLoss: 0.009099\n","2850it [01:16, 22.18it/s]Train epoch: 0 [batch #2850, batch_size 16, seq length 2500]\tLoss: 0.009640\n","2874it [01:17, 21.56it/s]Train epoch: 0 [batch #2875, batch_size 16, seq length 2500]\tLoss: 0.010489\n","2898it [01:18, 21.50it/s]Train epoch: 0 [batch #2900, batch_size 16, seq length 2500]\tLoss: 0.010251\n","2925it [01:19, 20.87it/s]Train epoch: 0 [batch #2925, batch_size 16, seq length 2500]\tLoss: 0.010616\n","2949it [01:20, 20.37it/s]Train epoch: 0 [batch #2950, batch_size 16, seq length 2500]\tLoss: 0.010253\n","2975it [01:22, 18.07it/s]Train epoch: 0 [batch #2975, batch_size 16, seq length 2500]\tLoss: 0.012460\n","2983it [01:22, 36.08it/s]\n","epoch loss: 0.008188956072931767\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:03, 415.69it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0072, 0.0170, 0.0099, 0.0125, 0.7753\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2057, 0.5871, 0.2405, 0.3412, 0.9621\n","rec_at_8: 0.2750\n","prec_at_8: 0.5331\n","rec_at_15: 0.3671\n","prec_at_15: 0.3934\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/cnn_vanilla_May_02_220247\n","\n","EPOCH 1\n","0it [00:00, ?it/s]Train epoch: 1 [batch #0, batch_size 16, seq length 117]\tLoss: 0.019154\n","22it [00:00, 68.79it/s]Train epoch: 1 [batch #25, batch_size 16, seq length 337]\tLoss: 0.009281\n","48it [00:00, 78.09it/s]Train epoch: 1 [batch #50, batch_size 16, seq length 402]\tLoss: 0.005057\n","74it [00:00, 79.30it/s]Train epoch: 1 [batch #75, batch_size 16, seq length 452]\tLoss: 0.004197\n","98it [00:01, 77.35it/s]Train epoch: 1 [batch #100, batch_size 16, seq length 490]\tLoss: 0.003962\n","122it [00:01, 75.73it/s]Train epoch: 1 [batch #125, batch_size 16, seq length 520]\tLoss: 0.004321\n","146it [00:01, 71.76it/s]Train epoch: 1 [batch #150, batch_size 16, seq length 548]\tLoss: 0.004496\n","170it [00:02, 68.24it/s]Train epoch: 1 [batch #175, batch_size 16, seq length 574]\tLoss: 0.004020\n","198it [00:02, 66.55it/s]Train epoch: 1 [batch #200, batch_size 16, seq length 596]\tLoss: 0.004320\n","219it [00:03, 66.41it/s]Train epoch: 1 [batch #225, batch_size 16, seq length 618]\tLoss: 0.004471\n","247it [00:03, 65.38it/s]Train epoch: 1 [batch #250, batch_size 16, seq length 638]\tLoss: 0.004149\n","275it [00:03, 63.37it/s]Train epoch: 1 [batch #275, batch_size 16, seq length 656]\tLoss: 0.004535\n","296it [00:04, 62.64it/s]Train epoch: 1 [batch #300, batch_size 16, seq length 674]\tLoss: 0.004684\n","324it [00:04, 61.22it/s]Train epoch: 1 [batch #325, batch_size 16, seq length 693]\tLoss: 0.004266\n","345it [00:05, 60.96it/s]Train epoch: 1 [batch #350, batch_size 16, seq length 710]\tLoss: 0.004388\n","373it [00:05, 60.10it/s]Train epoch: 1 [batch #375, batch_size 16, seq length 726]\tLoss: 0.005122\n","399it [00:05, 57.60it/s]Train epoch: 1 [batch #400, batch_size 16, seq length 743]\tLoss: 0.004374\n","423it [00:06, 54.98it/s]Train epoch: 1 [batch #425, batch_size 16, seq length 758]\tLoss: 0.004456\n","447it [00:06, 57.68it/s]Train epoch: 1 [batch #450, batch_size 16, seq length 773]\tLoss: 0.004724\n","471it [00:07, 56.86it/s]Train epoch: 1 [batch #475, batch_size 16, seq length 789]\tLoss: 0.004164\n","495it [00:07, 55.92it/s]Train epoch: 1 [batch #500, batch_size 16, seq length 803]\tLoss: 0.004592\n","525it [00:08, 55.81it/s]Train epoch: 1 [batch #525, batch_size 16, seq length 818]\tLoss: 0.004574\n","549it [00:08, 55.43it/s]Train epoch: 1 [batch #550, batch_size 16, seq length 833]\tLoss: 0.004552\n","573it [00:09, 55.37it/s]Train epoch: 1 [batch #575, batch_size 16, seq length 846]\tLoss: 0.004423\n","597it [00:09, 53.69it/s]Train epoch: 1 [batch #600, batch_size 16, seq length 860]\tLoss: 0.005152\n","621it [00:10, 52.99it/s]Train epoch: 1 [batch #625, batch_size 16, seq length 874]\tLoss: 0.004896\n","645it [00:10, 53.76it/s]Train epoch: 1 [batch #650, batch_size 16, seq length 887]\tLoss: 0.004514\n","675it [00:11, 53.49it/s]Train epoch: 1 [batch #675, batch_size 16, seq length 901]\tLoss: 0.004615\n","699it [00:11, 53.07it/s]Train epoch: 1 [batch #700, batch_size 16, seq length 915]\tLoss: 0.004500\n","723it [00:11, 52.36it/s]Train epoch: 1 [batch #725, batch_size 16, seq length 929]\tLoss: 0.004838\n","747it [00:12, 52.10it/s]Train epoch: 1 [batch #750, batch_size 16, seq length 942]\tLoss: 0.005111\n","771it [00:12, 50.50it/s]Train epoch: 1 [batch #775, batch_size 16, seq length 954]\tLoss: 0.005112\n","797it [00:13, 49.18it/s]Train epoch: 1 [batch #800, batch_size 16, seq length 968]\tLoss: 0.004997\n","823it [00:13, 49.06it/s]Train epoch: 1 [batch #825, batch_size 16, seq length 981]\tLoss: 0.005120\n","849it [00:14, 48.43it/s]Train epoch: 1 [batch #850, batch_size 16, seq length 994]\tLoss: 0.004985\n","874it [00:15, 46.92it/s]Train epoch: 1 [batch #875, batch_size 16, seq length 1006]\tLoss: 0.004743\n","899it [00:15, 46.65it/s]Train epoch: 1 [batch #900, batch_size 16, seq length 1020]\tLoss: 0.005191\n","924it [00:16, 46.15it/s]Train epoch: 1 [batch #925, batch_size 16, seq length 1033]\tLoss: 0.004880\n","949it [00:16, 45.56it/s]Train epoch: 1 [batch #950, batch_size 16, seq length 1046]\tLoss: 0.005430\n","974it [00:17, 45.01it/s]Train epoch: 1 [batch #975, batch_size 16, seq length 1059]\tLoss: 0.005147\n","999it [00:17, 43.90it/s]Train epoch: 1 [batch #1000, batch_size 16, seq length 1073]\tLoss: 0.005435\n","1024it [00:18, 44.69it/s]Train epoch: 1 [batch #1025, batch_size 16, seq length 1085]\tLoss: 0.005342\n","1049it [00:18, 44.34it/s]Train epoch: 1 [batch #1050, batch_size 16, seq length 1099]\tLoss: 0.005440\n","1074it [00:19, 44.26it/s]Train epoch: 1 [batch #1075, batch_size 16, seq length 1113]\tLoss: 0.005434\n","1099it [00:20, 43.30it/s]Train epoch: 1 [batch #1100, batch_size 16, seq length 1126]\tLoss: 0.005218\n","1124it [00:20, 43.10it/s]Train epoch: 1 [batch #1125, batch_size 16, seq length 1140]\tLoss: 0.005546\n","1149it [00:21, 42.92it/s]Train epoch: 1 [batch #1150, batch_size 16, seq length 1154]\tLoss: 0.005524\n","1174it [00:21, 42.10it/s]Train epoch: 1 [batch #1175, batch_size 16, seq length 1167]\tLoss: 0.005534\n","1199it [00:22, 42.25it/s]Train epoch: 1 [batch #1200, batch_size 16, seq length 1180]\tLoss: 0.005529\n","1224it [00:22, 41.54it/s]Train epoch: 1 [batch #1225, batch_size 16, seq length 1195]\tLoss: 0.005504\n","1249it [00:23, 39.20it/s]Train epoch: 1 [batch #1250, batch_size 16, seq length 1208]\tLoss: 0.005355\n","1273it [00:24, 40.86it/s]Train epoch: 1 [batch #1275, batch_size 16, seq length 1220]\tLoss: 0.005269\n","1298it [00:24, 40.17it/s]Train epoch: 1 [batch #1300, batch_size 16, seq length 1234]\tLoss: 0.005589\n","1323it [00:25, 40.04it/s]Train epoch: 1 [batch #1325, batch_size 16, seq length 1248]\tLoss: 0.005129\n","1346it [00:26, 40.23it/s]Train epoch: 1 [batch #1350, batch_size 16, seq length 1262]\tLoss: 0.005592\n","1372it [00:26, 38.89it/s]Train epoch: 1 [batch #1375, batch_size 16, seq length 1276]\tLoss: 0.006346\n","1400it [00:27, 38.87it/s]Train epoch: 1 [batch #1400, batch_size 16, seq length 1290]\tLoss: 0.005914\n","1424it [00:28, 37.99it/s]Train epoch: 1 [batch #1425, batch_size 16, seq length 1304]\tLoss: 0.005910\n","1448it [00:28, 38.05it/s]Train epoch: 1 [batch #1450, batch_size 16, seq length 1319]\tLoss: 0.005736\n","1472it [00:29, 37.89it/s]Train epoch: 1 [batch #1475, batch_size 16, seq length 1332]\tLoss: 0.006130\n","1500it [00:30, 37.67it/s]Train epoch: 1 [batch #1500, batch_size 16, seq length 1346]\tLoss: 0.005893\n","1524it [00:30, 37.21it/s]Train epoch: 1 [batch #1525, batch_size 16, seq length 1362]\tLoss: 0.005252\n","1548it [00:31, 36.23it/s]Train epoch: 1 [batch #1550, batch_size 16, seq length 1377]\tLoss: 0.006245\n","1572it [00:32, 35.46it/s]Train epoch: 1 [batch #1575, batch_size 16, seq length 1392]\tLoss: 0.005620\n","1600it [00:32, 35.70it/s]Train epoch: 1 [batch #1600, batch_size 16, seq length 1406]\tLoss: 0.005567\n","1624it [00:33, 35.86it/s]Train epoch: 1 [batch #1625, batch_size 16, seq length 1422]\tLoss: 0.006378\n","1648it [00:34, 35.29it/s]Train epoch: 1 [batch #1650, batch_size 16, seq length 1437]\tLoss: 0.006089\n","1672it [00:34, 34.81it/s]Train epoch: 1 [batch #1675, batch_size 16, seq length 1454]\tLoss: 0.006187\n","1700it [00:35, 34.67it/s]Train epoch: 1 [batch #1700, batch_size 16, seq length 1469]\tLoss: 0.006359\n","1724it [00:36, 35.32it/s]Train epoch: 1 [batch #1725, batch_size 16, seq length 1484]\tLoss: 0.005609\n","1748it [00:37, 35.27it/s]Train epoch: 1 [batch #1750, batch_size 16, seq length 1501]\tLoss: 0.005713\n","1772it [00:37, 34.71it/s]Train epoch: 1 [batch #1775, batch_size 16, seq length 1518]\tLoss: 0.006183\n","1800it [00:38, 33.83it/s]Train epoch: 1 [batch #1800, batch_size 16, seq length 1533]\tLoss: 0.006224\n","1824it [00:39, 33.37it/s]Train epoch: 1 [batch #1825, batch_size 16, seq length 1548]\tLoss: 0.006308\n","1848it [00:39, 32.59it/s]Train epoch: 1 [batch #1850, batch_size 16, seq length 1566]\tLoss: 0.005940\n","1872it [00:40, 32.75it/s]Train epoch: 1 [batch #1875, batch_size 16, seq length 1582]\tLoss: 0.006120\n","1900it [00:41, 33.27it/s]Train epoch: 1 [batch #1900, batch_size 16, seq length 1599]\tLoss: 0.007401\n","1924it [00:42, 32.44it/s]Train epoch: 1 [batch #1925, batch_size 16, seq length 1616]\tLoss: 0.006461\n","1948it [00:43, 32.09it/s]Train epoch: 1 [batch #1950, batch_size 16, seq length 1633]\tLoss: 0.006164\n","1972it [00:43, 31.83it/s]Train epoch: 1 [batch #1975, batch_size 16, seq length 1651]\tLoss: 0.006211\n","2000it [00:44, 32.14it/s]Train epoch: 1 [batch #2000, batch_size 16, seq length 1670]\tLoss: 0.005933\n","2024it [00:45, 32.28it/s]Train epoch: 1 [batch #2025, batch_size 16, seq length 1688]\tLoss: 0.005965\n","2048it [00:46, 32.02it/s]Train epoch: 1 [batch #2050, batch_size 16, seq length 1707]\tLoss: 0.006305\n","2072it [00:46, 31.81it/s]Train epoch: 1 [batch #2075, batch_size 16, seq length 1728]\tLoss: 0.006425\n","2100it [00:47, 30.83it/s]Train epoch: 1 [batch #2100, batch_size 16, seq length 1748]\tLoss: 0.005865\n","2124it [00:48, 31.21it/s]Train epoch: 1 [batch #2125, batch_size 16, seq length 1768]\tLoss: 0.006906\n","2148it [00:49, 30.41it/s]Train epoch: 1 [batch #2150, batch_size 16, seq length 1787]\tLoss: 0.006217\n","2173it [00:50, 29.49it/s]Train epoch: 1 [batch #2175, batch_size 16, seq length 1809]\tLoss: 0.006892\n","2200it [00:51, 29.61it/s]Train epoch: 1 [batch #2200, batch_size 16, seq length 1833]\tLoss: 0.006352\n","2223it [00:51, 30.02it/s]Train epoch: 1 [batch #2225, batch_size 16, seq length 1855]\tLoss: 0.006354\n","2250it [00:52, 29.69it/s]Train epoch: 1 [batch #2250, batch_size 16, seq length 1878]\tLoss: 0.006936\n","2274it [00:53, 29.19it/s]Train epoch: 1 [batch #2275, batch_size 16, seq length 1901]\tLoss: 0.006485\n","2298it [00:54, 28.20it/s]Train epoch: 1 [batch #2300, batch_size 16, seq length 1927]\tLoss: 0.006521\n","2325it [00:55, 28.49it/s]Train epoch: 1 [batch #2325, batch_size 16, seq length 1953]\tLoss: 0.007228\n","2349it [00:56, 27.76it/s]Train epoch: 1 [batch #2350, batch_size 16, seq length 1980]\tLoss: 0.006661\n","2373it [00:57, 28.49it/s]Train epoch: 1 [batch #2375, batch_size 16, seq length 2006]\tLoss: 0.007031\n","2400it [00:58, 27.55it/s]Train epoch: 1 [batch #2400, batch_size 16, seq length 2034]\tLoss: 0.007014\n","2424it [00:59, 27.28it/s]Train epoch: 1 [batch #2425, batch_size 16, seq length 2064]\tLoss: 0.006702\n","2448it [00:59, 27.23it/s]Train epoch: 1 [batch #2450, batch_size 16, seq length 2094]\tLoss: 0.006971\n","2475it [01:00, 27.12it/s]Train epoch: 1 [batch #2475, batch_size 16, seq length 2126]\tLoss: 0.007401\n","2499it [01:01, 26.30it/s]Train epoch: 1 [batch #2500, batch_size 16, seq length 2160]\tLoss: 0.007160\n","2523it [01:02, 26.04it/s]Train epoch: 1 [batch #2525, batch_size 16, seq length 2192]\tLoss: 0.007329\n","2550it [01:03, 26.35it/s]Train epoch: 1 [batch #2550, batch_size 16, seq length 2228]\tLoss: 0.007109\n","2574it [01:04, 25.33it/s]Train epoch: 1 [batch #2575, batch_size 16, seq length 2266]\tLoss: 0.007674\n","2598it [01:05, 24.52it/s]Train epoch: 1 [batch #2600, batch_size 16, seq length 2307]\tLoss: 0.008014\n","2625it [01:06, 24.15it/s]Train epoch: 1 [batch #2625, batch_size 16, seq length 2350]\tLoss: 0.007432\n","2649it [01:07, 24.51it/s]Train epoch: 1 [batch #2650, batch_size 16, seq length 2392]\tLoss: 0.007222\n","2673it [01:08, 24.27it/s]Train epoch: 1 [batch #2675, batch_size 16, seq length 2439]\tLoss: 0.007555\n","2700it [01:09, 23.88it/s]Train epoch: 1 [batch #2700, batch_size 16, seq length 2491]\tLoss: 0.007709\n","2724it [01:10, 23.34it/s]Train epoch: 1 [batch #2725, batch_size 16, seq length 2500]\tLoss: 0.007902\n","2748it [01:11, 23.30it/s]Train epoch: 1 [batch #2750, batch_size 16, seq length 2500]\tLoss: 0.007849\n","2775it [01:13, 22.85it/s]Train epoch: 1 [batch #2775, batch_size 16, seq length 2500]\tLoss: 0.008310\n","2799it [01:14, 22.71it/s]Train epoch: 1 [batch #2800, batch_size 16, seq length 2500]\tLoss: 0.008529\n","2823it [01:15, 22.20it/s]Train epoch: 1 [batch #2825, batch_size 16, seq length 2500]\tLoss: 0.007998\n","2850it [01:16, 22.12it/s]Train epoch: 1 [batch #2850, batch_size 16, seq length 2500]\tLoss: 0.008467\n","2874it [01:17, 21.69it/s]Train epoch: 1 [batch #2875, batch_size 16, seq length 2500]\tLoss: 0.009257\n","2898it [01:18, 21.37it/s]Train epoch: 1 [batch #2900, batch_size 16, seq length 2500]\tLoss: 0.009110\n","2925it [01:19, 20.98it/s]Train epoch: 1 [batch #2925, batch_size 16, seq length 2500]\tLoss: 0.009553\n","2949it [01:21, 20.31it/s]Train epoch: 1 [batch #2950, batch_size 16, seq length 2500]\tLoss: 0.009482\n","2974it [01:22, 18.43it/s]Train epoch: 1 [batch #2975, batch_size 16, seq length 2500]\tLoss: 0.011496\n","2983it [01:22, 35.95it/s]\n","epoch loss: 0.006056077159070755\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:03, 436.68it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0196, 0.0330, 0.0339, 0.0334, 0.8106\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2453, 0.4333, 0.3612, 0.3940, 0.9657\n","rec_at_8: 0.3143\n","prec_at_8: 0.5924\n","rec_at_15: 0.4250\n","prec_at_15: 0.4454\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/cnn_vanilla_May_02_220247\n","\n","EPOCH 2\n","0it [00:00, ?it/s]Train epoch: 2 [batch #0, batch_size 16, seq length 117]\tLoss: 0.085211\n","21it [00:00, 65.87it/s]Train epoch: 2 [batch #25, batch_size 16, seq length 337]\tLoss: 0.012081\n","46it [00:00, 75.70it/s]Train epoch: 2 [batch #50, batch_size 16, seq length 402]\tLoss: 0.010892\n","72it [00:00, 79.41it/s]Train epoch: 2 [batch #75, batch_size 16, seq length 452]\tLoss: 0.009606\n","96it [00:01, 78.79it/s]Train epoch: 2 [batch #100, batch_size 16, seq length 490]\tLoss: 0.008659\n","120it [00:01, 76.96it/s]Train epoch: 2 [batch #125, batch_size 16, seq length 520]\tLoss: 0.009517\n","144it [00:01, 71.25it/s]Train epoch: 2 [batch #150, batch_size 16, seq length 548]\tLoss: 0.009782\n","175it [00:02, 68.96it/s]Train epoch: 2 [batch #175, batch_size 16, seq length 574]\tLoss: 0.008398\n","196it [00:02, 66.37it/s]Train epoch: 2 [batch #200, batch_size 16, seq length 596]\tLoss: 0.008778\n","224it [00:03, 65.55it/s]Train epoch: 2 [batch #225, batch_size 16, seq length 618]\tLoss: 0.009025\n","245it [00:03, 65.02it/s]Train epoch: 2 [batch #250, batch_size 16, seq length 638]\tLoss: 0.008066\n","273it [00:03, 62.86it/s]Train epoch: 2 [batch #275, batch_size 16, seq length 656]\tLoss: 0.008521\n","294it [00:04, 59.85it/s]Train epoch: 2 [batch #300, batch_size 16, seq length 674]\tLoss: 0.008692\n","322it [00:04, 60.77it/s]Train epoch: 2 [batch #325, batch_size 16, seq length 693]\tLoss: 0.007792\n","350it [00:05, 60.62it/s]Train epoch: 2 [batch #350, batch_size 16, seq length 710]\tLoss: 0.007634\n","371it [00:05, 61.19it/s]Train epoch: 2 [batch #375, batch_size 16, seq length 726]\tLoss: 0.009146\n","397it [00:06, 54.70it/s]Train epoch: 2 [batch #400, batch_size 16, seq length 743]\tLoss: 0.007455\n","421it [00:06, 57.31it/s]Train epoch: 2 [batch #425, batch_size 16, seq length 758]\tLoss: 0.007682\n","446it [00:06, 58.00it/s]Train epoch: 2 [batch #450, batch_size 16, seq length 773]\tLoss: 0.007724\n","470it [00:07, 56.82it/s]Train epoch: 2 [batch #475, batch_size 16, seq length 789]\tLoss: 0.006565\n","500it [00:07, 55.75it/s]Train epoch: 2 [batch #500, batch_size 16, seq length 803]\tLoss: 0.007362\n","524it [00:08, 55.02it/s]Train epoch: 2 [batch #525, batch_size 16, seq length 818]\tLoss: 0.007307\n","548it [00:08, 55.17it/s]Train epoch: 2 [batch #550, batch_size 16, seq length 833]\tLoss: 0.007059\n","572it [00:09, 54.67it/s]Train epoch: 2 [batch #575, batch_size 16, seq length 846]\tLoss: 0.006981\n","596it [00:09, 54.50it/s]Train epoch: 2 [batch #600, batch_size 16, seq length 860]\tLoss: 0.007916\n","620it [00:10, 53.82it/s]Train epoch: 2 [batch #625, batch_size 16, seq length 874]\tLoss: 0.007525\n","650it [00:10, 54.57it/s]Train epoch: 2 [batch #650, batch_size 16, seq length 887]\tLoss: 0.006470\n","674it [00:11, 53.82it/s]Train epoch: 2 [batch #675, batch_size 16, seq length 901]\tLoss: 0.006700\n","698it [00:11, 52.90it/s]Train epoch: 2 [batch #700, batch_size 16, seq length 915]\tLoss: 0.006552\n","722it [00:11, 52.59it/s]Train epoch: 2 [batch #725, batch_size 16, seq length 929]\tLoss: 0.007716\n","746it [00:12, 52.30it/s]Train epoch: 2 [batch #750, batch_size 16, seq length 942]\tLoss: 0.007508\n","770it [00:12, 50.03it/s]Train epoch: 2 [batch #775, batch_size 16, seq length 954]\tLoss: 0.007533\n","799it [00:13, 49.61it/s]Train epoch: 2 [batch #800, batch_size 16, seq length 968]\tLoss: 0.007135\n","820it [00:13, 48.92it/s]Train epoch: 2 [batch #825, batch_size 16, seq length 981]\tLoss: 0.007218\n","848it [00:14, 49.30it/s]Train epoch: 2 [batch #850, batch_size 16, seq length 994]\tLoss: 0.007052\n","873it [00:14, 48.46it/s]Train epoch: 2 [batch #875, batch_size 16, seq length 1006]\tLoss: 0.006439\n","898it [00:15, 47.26it/s]Train epoch: 2 [batch #900, batch_size 16, seq length 1020]\tLoss: 0.007126\n","923it [00:16, 45.27it/s]Train epoch: 2 [batch #925, batch_size 16, seq length 1033]\tLoss: 0.006741\n","948it [00:16, 44.96it/s]Train epoch: 2 [batch #950, batch_size 16, seq length 1046]\tLoss: 0.007382\n","973it [00:17, 44.20it/s]Train epoch: 2 [batch #975, batch_size 16, seq length 1059]\tLoss: 0.006896\n","998it [00:17, 44.95it/s]Train epoch: 2 [batch #1000, batch_size 16, seq length 1073]\tLoss: 0.007463\n","1023it [00:18, 44.16it/s]Train epoch: 2 [batch #1025, batch_size 16, seq length 1085]\tLoss: 0.007002\n","1048it [00:18, 43.50it/s]Train epoch: 2 [batch #1050, batch_size 16, seq length 1099]\tLoss: 0.007120\n","1073it [00:19, 43.28it/s]Train epoch: 2 [batch #1075, batch_size 16, seq length 1113]\tLoss: 0.006972\n","1098it [00:20, 43.04it/s]Train epoch: 2 [batch #1100, batch_size 16, seq length 1126]\tLoss: 0.006720\n","1123it [00:20, 42.80it/s]Train epoch: 2 [batch #1125, batch_size 16, seq length 1140]\tLoss: 0.007191\n","1148it [00:21, 41.47it/s]Train epoch: 2 [batch #1150, batch_size 16, seq length 1154]\tLoss: 0.007033\n","1173it [00:21, 41.84it/s]Train epoch: 2 [batch #1175, batch_size 16, seq length 1167]\tLoss: 0.006997\n","1198it [00:22, 41.12it/s]Train epoch: 2 [batch #1200, batch_size 16, seq length 1180]\tLoss: 0.006835\n","1223it [00:23, 40.54it/s]Train epoch: 2 [batch #1225, batch_size 16, seq length 1195]\tLoss: 0.006657\n","1248it [00:23, 41.29it/s]Train epoch: 2 [batch #1250, batch_size 16, seq length 1208]\tLoss: 0.006598\n","1273it [00:24, 40.43it/s]Train epoch: 2 [batch #1275, batch_size 16, seq length 1220]\tLoss: 0.006582\n","1298it [00:24, 40.74it/s]Train epoch: 2 [batch #1300, batch_size 16, seq length 1234]\tLoss: 0.007035\n","1322it [00:25, 40.35it/s]Train epoch: 2 [batch #1325, batch_size 16, seq length 1248]\tLoss: 0.006105\n","1347it [00:26, 39.68it/s]Train epoch: 2 [batch #1350, batch_size 16, seq length 1262]\tLoss: 0.006685\n","1373it [00:26, 39.32it/s]Train epoch: 2 [batch #1375, batch_size 16, seq length 1276]\tLoss: 0.007565\n","1397it [00:27, 39.04it/s]Train epoch: 2 [batch #1400, batch_size 16, seq length 1290]\tLoss: 0.006860\n","1425it [00:28, 36.18it/s]Train epoch: 2 [batch #1425, batch_size 16, seq length 1304]\tLoss: 0.006953\n","1449it [00:28, 37.64it/s]Train epoch: 2 [batch #1450, batch_size 16, seq length 1319]\tLoss: 0.006753\n","1473it [00:29, 37.59it/s]Train epoch: 2 [batch #1475, batch_size 16, seq length 1332]\tLoss: 0.007243\n","1497it [00:30, 37.46it/s]Train epoch: 2 [batch #1500, batch_size 16, seq length 1346]\tLoss: 0.006810\n","1525it [00:30, 36.92it/s]Train epoch: 2 [batch #1525, batch_size 16, seq length 1362]\tLoss: 0.006039\n","1549it [00:31, 36.56it/s]Train epoch: 2 [batch #1550, batch_size 16, seq length 1377]\tLoss: 0.007041\n","1573it [00:32, 36.25it/s]Train epoch: 2 [batch #1575, batch_size 16, seq length 1392]\tLoss: 0.006204\n","1597it [00:32, 36.83it/s]Train epoch: 2 [batch #1600, batch_size 16, seq length 1406]\tLoss: 0.006267\n","1625it [00:33, 35.83it/s]Train epoch: 2 [batch #1625, batch_size 16, seq length 1422]\tLoss: 0.006692\n","1649it [00:34, 35.64it/s]Train epoch: 2 [batch #1650, batch_size 16, seq length 1437]\tLoss: 0.006455\n","1673it [00:34, 35.76it/s]Train epoch: 2 [batch #1675, batch_size 16, seq length 1454]\tLoss: 0.006819\n","1697it [00:35, 35.20it/s]Train epoch: 2 [batch #1700, batch_size 16, seq length 1469]\tLoss: 0.006919\n","1725it [00:36, 34.93it/s]Train epoch: 2 [batch #1725, batch_size 16, seq length 1484]\tLoss: 0.006073\n","1749it [00:37, 34.62it/s]Train epoch: 2 [batch #1750, batch_size 16, seq length 1501]\tLoss: 0.006252\n","1773it [00:37, 34.21it/s]Train epoch: 2 [batch #1775, batch_size 16, seq length 1518]\tLoss: 0.006791\n","1797it [00:38, 33.08it/s]Train epoch: 2 [batch #1800, batch_size 16, seq length 1533]\tLoss: 0.006815\n","1825it [00:39, 32.18it/s]Train epoch: 2 [batch #1825, batch_size 16, seq length 1548]\tLoss: 0.006856\n","1849it [00:40, 32.70it/s]Train epoch: 2 [batch #1850, batch_size 16, seq length 1566]\tLoss: 0.006402\n","1873it [00:40, 33.21it/s]Train epoch: 2 [batch #1875, batch_size 16, seq length 1582]\tLoss: 0.006605\n","1897it [00:41, 33.02it/s]Train epoch: 2 [batch #1900, batch_size 16, seq length 1599]\tLoss: 0.007195\n","1925it [00:42, 32.66it/s]Train epoch: 2 [batch #1925, batch_size 16, seq length 1616]\tLoss: 0.006767\n","1949it [00:43, 33.07it/s]Train epoch: 2 [batch #1950, batch_size 16, seq length 1633]\tLoss: 0.006565\n","1973it [00:43, 32.92it/s]Train epoch: 2 [batch #1975, batch_size 16, seq length 1651]\tLoss: 0.006492\n","1997it [00:44, 32.55it/s]Train epoch: 2 [batch #2000, batch_size 16, seq length 1670]\tLoss: 0.006223\n","2025it [00:45, 32.17it/s]Train epoch: 2 [batch #2025, batch_size 16, seq length 1688]\tLoss: 0.006281\n","2049it [00:46, 31.79it/s]Train epoch: 2 [batch #2050, batch_size 16, seq length 1707]\tLoss: 0.006572\n","2073it [00:47, 31.66it/s]Train epoch: 2 [batch #2075, batch_size 16, seq length 1728]\tLoss: 0.006738\n","2097it [00:47, 31.00it/s]Train epoch: 2 [batch #2100, batch_size 16, seq length 1748]\tLoss: 0.006156\n","2125it [00:48, 30.55it/s]Train epoch: 2 [batch #2125, batch_size 16, seq length 1768]\tLoss: 0.006971\n","2149it [00:49, 30.20it/s]Train epoch: 2 [batch #2150, batch_size 16, seq length 1787]\tLoss: 0.006281\n","2173it [00:50, 30.11it/s]Train epoch: 2 [batch #2175, batch_size 16, seq length 1809]\tLoss: 0.007032\n","2197it [00:51, 30.08it/s]Train epoch: 2 [batch #2200, batch_size 16, seq length 1833]\tLoss: 0.006498\n","2223it [00:52, 29.73it/s]Train epoch: 2 [batch #2225, batch_size 16, seq length 1855]\tLoss: 0.006657\n","2250it [00:52, 29.70it/s]Train epoch: 2 [batch #2250, batch_size 16, seq length 1878]\tLoss: 0.007145\n","2275it [00:53, 29.51it/s]Train epoch: 2 [batch #2275, batch_size 16, seq length 1901]\tLoss: 0.006642\n","2299it [00:54, 28.51it/s]Train epoch: 2 [batch #2300, batch_size 16, seq length 1927]\tLoss: 0.006548\n","2323it [00:55, 28.46it/s]Train epoch: 2 [batch #2325, batch_size 16, seq length 1953]\tLoss: 0.007134\n","2350it [00:56, 28.39it/s]Train epoch: 2 [batch #2350, batch_size 16, seq length 1980]\tLoss: 0.006661\n","2374it [00:57, 28.62it/s]Train epoch: 2 [batch #2375, batch_size 16, seq length 2006]\tLoss: 0.006959\n","2398it [00:58, 28.05it/s]Train epoch: 2 [batch #2400, batch_size 16, seq length 2034]\tLoss: 0.006962\n","2425it [00:59, 27.35it/s]Train epoch: 2 [batch #2425, batch_size 16, seq length 2064]\tLoss: 0.006539\n","2449it [00:59, 26.69it/s]Train epoch: 2 [batch #2450, batch_size 16, seq length 2094]\tLoss: 0.006921\n","2473it [01:00, 26.79it/s]Train epoch: 2 [batch #2475, batch_size 16, seq length 2126]\tLoss: 0.007235\n","2500it [01:01, 26.53it/s]Train epoch: 2 [batch #2500, batch_size 16, seq length 2160]\tLoss: 0.006954\n","2524it [01:02, 26.15it/s]Train epoch: 2 [batch #2525, batch_size 16, seq length 2192]\tLoss: 0.007092\n","2548it [01:03, 26.18it/s]Train epoch: 2 [batch #2550, batch_size 16, seq length 2228]\tLoss: 0.006972\n","2575it [01:04, 24.48it/s]Train epoch: 2 [batch #2575, batch_size 16, seq length 2266]\tLoss: 0.007438\n","2599it [01:05, 25.01it/s]Train epoch: 2 [batch #2600, batch_size 16, seq length 2307]\tLoss: 0.007702\n","2623it [01:06, 24.91it/s]Train epoch: 2 [batch #2625, batch_size 16, seq length 2350]\tLoss: 0.007058\n","2650it [01:07, 24.27it/s]Train epoch: 2 [batch #2650, batch_size 16, seq length 2392]\tLoss: 0.007022\n","2674it [01:08, 24.42it/s]Train epoch: 2 [batch #2675, batch_size 16, seq length 2439]\tLoss: 0.007387\n","2698it [01:09, 23.68it/s]Train epoch: 2 [batch #2700, batch_size 16, seq length 2491]\tLoss: 0.007499\n","2725it [01:10, 23.36it/s]Train epoch: 2 [batch #2725, batch_size 16, seq length 2500]\tLoss: 0.007688\n","2749it [01:12, 23.05it/s]Train epoch: 2 [batch #2750, batch_size 16, seq length 2500]\tLoss: 0.007537\n","2773it [01:13, 22.76it/s]Train epoch: 2 [batch #2775, batch_size 16, seq length 2500]\tLoss: 0.008017\n","2800it [01:14, 22.81it/s]Train epoch: 2 [batch #2800, batch_size 16, seq length 2500]\tLoss: 0.007974\n","2824it [01:15, 22.41it/s]Train epoch: 2 [batch #2825, batch_size 16, seq length 2500]\tLoss: 0.007641\n","2848it [01:16, 22.27it/s]Train epoch: 2 [batch #2850, batch_size 16, seq length 2500]\tLoss: 0.008001\n","2875it [01:17, 21.95it/s]Train epoch: 2 [batch #2875, batch_size 16, seq length 2500]\tLoss: 0.008876\n","2899it [01:18, 21.12it/s]Train epoch: 2 [batch #2900, batch_size 16, seq length 2500]\tLoss: 0.008735\n","2923it [01:19, 21.03it/s]Train epoch: 2 [batch #2925, batch_size 16, seq length 2500]\tLoss: 0.009156\n","2950it [01:21, 19.93it/s]Train epoch: 2 [batch #2950, batch_size 16, seq length 2500]\tLoss: 0.008912\n","2975it [01:22, 18.17it/s]Train epoch: 2 [batch #2975, batch_size 16, seq length 2500]\tLoss: 0.011028\n","2983it [01:23, 35.92it/s]\n","epoch loss: 0.00740006437869833\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:03, 437.77it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0177, 0.0335, 0.0257, 0.0291, 0.8414\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2679, 0.5422, 0.3462, 0.4226, 0.9715\n","rec_at_8: 0.3111\n","prec_at_8: 0.5883\n","rec_at_15: 0.4180\n","prec_at_15: 0.4378\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/cnn_vanilla_May_02_220247\n","\n","EPOCH 3\n","0it [00:00, ?it/s]Train epoch: 3 [batch #0, batch_size 16, seq length 117]\tLoss: 0.006781\n","22it [00:00, 70.99it/s]Train epoch: 3 [batch #25, batch_size 16, seq length 337]\tLoss: 0.004627\n","48it [00:00, 79.83it/s]Train epoch: 3 [batch #50, batch_size 16, seq length 402]\tLoss: 0.003959\n","75it [00:00, 80.57it/s]Train epoch: 3 [batch #75, batch_size 16, seq length 452]\tLoss: 0.003652\n","100it [00:01, 77.89it/s]Train epoch: 3 [batch #100, batch_size 16, seq length 490]\tLoss: 0.003419\n","124it [00:01, 75.22it/s]Train epoch: 3 [batch #125, batch_size 16, seq length 520]\tLoss: 0.003821\n","148it [00:01, 70.62it/s]Train epoch: 3 [batch #150, batch_size 16, seq length 548]\tLoss: 0.004011\n","170it [00:02, 68.59it/s]Train epoch: 3 [batch #175, batch_size 16, seq length 574]\tLoss: 0.003638\n","198it [00:02, 68.07it/s]Train epoch: 3 [batch #200, batch_size 16, seq length 596]\tLoss: 0.003899\n","219it [00:03, 66.84it/s]Train epoch: 3 [batch #225, batch_size 16, seq length 618]\tLoss: 0.003953\n","247it [00:03, 65.79it/s]Train epoch: 3 [batch #250, batch_size 16, seq length 638]\tLoss: 0.003715\n","275it [00:03, 63.24it/s]Train epoch: 3 [batch #275, batch_size 16, seq length 656]\tLoss: 0.004092\n","296it [00:04, 61.83it/s]Train epoch: 3 [batch #300, batch_size 16, seq length 674]\tLoss: 0.004143\n","324it [00:04, 61.97it/s]Train epoch: 3 [batch #325, batch_size 16, seq length 693]\tLoss: 0.003791\n","345it [00:05, 61.08it/s]Train epoch: 3 [batch #350, batch_size 16, seq length 710]\tLoss: 0.003974\n","373it [00:05, 59.90it/s]Train epoch: 3 [batch #375, batch_size 16, seq length 726]\tLoss: 0.004610\n","400it [00:05, 59.26it/s]Train epoch: 3 [batch #400, batch_size 16, seq length 743]\tLoss: 0.003915\n","425it [00:06, 59.18it/s]Train epoch: 3 [batch #425, batch_size 16, seq length 758]\tLoss: 0.003906\n","449it [00:06, 58.25it/s]Train epoch: 3 [batch #450, batch_size 16, seq length 773]\tLoss: 0.004265\n","473it [00:07, 56.34it/s]Train epoch: 3 [batch #475, batch_size 16, seq length 789]\tLoss: 0.003691\n","497it [00:07, 56.08it/s]Train epoch: 3 [batch #500, batch_size 16, seq length 803]\tLoss: 0.004137\n","521it [00:08, 55.68it/s]Train epoch: 3 [batch #525, batch_size 16, seq length 818]\tLoss: 0.004086\n","545it [00:08, 54.92it/s]Train epoch: 3 [batch #550, batch_size 16, seq length 833]\tLoss: 0.003993\n","575it [00:09, 54.83it/s]Train epoch: 3 [batch #575, batch_size 16, seq length 846]\tLoss: 0.003855\n","599it [00:09, 53.72it/s]Train epoch: 3 [batch #600, batch_size 16, seq length 860]\tLoss: 0.004576\n","623it [00:10, 52.07it/s]Train epoch: 3 [batch #625, batch_size 16, seq length 874]\tLoss: 0.004311\n","647it [00:10, 52.93it/s]Train epoch: 3 [batch #650, batch_size 16, seq length 887]\tLoss: 0.003999\n","671it [00:10, 53.67it/s]Train epoch: 3 [batch #675, batch_size 16, seq length 901]\tLoss: 0.004131\n","695it [00:11, 52.74it/s]Train epoch: 3 [batch #700, batch_size 16, seq length 915]\tLoss: 0.003942\n","725it [00:11, 52.42it/s]Train epoch: 3 [batch #725, batch_size 16, seq length 929]\tLoss: 0.004307\n","749it [00:12, 50.29it/s]Train epoch: 3 [batch #750, batch_size 16, seq length 942]\tLoss: 0.004491\n","773it [00:12, 50.91it/s]Train epoch: 3 [batch #775, batch_size 16, seq length 954]\tLoss: 0.004521\n","797it [00:13, 49.31it/s]Train epoch: 3 [batch #800, batch_size 16, seq length 968]\tLoss: 0.004476\n","824it [00:13, 49.69it/s]Train epoch: 3 [batch #825, batch_size 16, seq length 981]\tLoss: 0.004460\n","850it [00:14, 47.70it/s]Train epoch: 3 [batch #850, batch_size 16, seq length 994]\tLoss: 0.004403\n","872it [00:14, 48.79it/s]Train epoch: 3 [batch #875, batch_size 16, seq length 1006]\tLoss: 0.004206\n","897it [00:15, 47.04it/s]Train epoch: 3 [batch #900, batch_size 16, seq length 1020]\tLoss: 0.004662\n","922it [00:15, 47.04it/s]Train epoch: 3 [batch #925, batch_size 16, seq length 1033]\tLoss: 0.004296\n","947it [00:16, 45.93it/s]Train epoch: 3 [batch #950, batch_size 16, seq length 1046]\tLoss: 0.004728\n","972it [00:17, 45.60it/s]Train epoch: 3 [batch #975, batch_size 16, seq length 1059]\tLoss: 0.004510\n","997it [00:17, 44.45it/s]Train epoch: 3 [batch #1000, batch_size 16, seq length 1073]\tLoss: 0.004681\n","1022it [00:18, 45.10it/s]Train epoch: 3 [batch #1025, batch_size 16, seq length 1085]\tLoss: 0.004678\n","1047it [00:18, 43.77it/s]Train epoch: 3 [batch #1050, batch_size 16, seq length 1099]\tLoss: 0.004766\n","1072it [00:19, 44.16it/s]Train epoch: 3 [batch #1075, batch_size 16, seq length 1113]\tLoss: 0.004818\n","1097it [00:19, 42.63it/s]Train epoch: 3 [batch #1100, batch_size 16, seq length 1126]\tLoss: 0.004550\n","1122it [00:20, 42.63it/s]Train epoch: 3 [batch #1125, batch_size 16, seq length 1140]\tLoss: 0.004864\n","1147it [00:21, 42.87it/s]Train epoch: 3 [batch #1150, batch_size 16, seq length 1154]\tLoss: 0.004848\n","1172it [00:21, 42.10it/s]Train epoch: 3 [batch #1175, batch_size 16, seq length 1167]\tLoss: 0.004917\n","1197it [00:22, 42.48it/s]Train epoch: 3 [batch #1200, batch_size 16, seq length 1180]\tLoss: 0.004738\n","1222it [00:22, 41.45it/s]Train epoch: 3 [batch #1225, batch_size 16, seq length 1195]\tLoss: 0.004735\n","1247it [00:23, 40.98it/s]Train epoch: 3 [batch #1250, batch_size 16, seq length 1208]\tLoss: 0.004663\n","1272it [00:24, 41.10it/s]Train epoch: 3 [batch #1275, batch_size 16, seq length 1220]\tLoss: 0.004611\n","1300it [00:24, 39.05it/s]Train epoch: 3 [batch #1300, batch_size 16, seq length 1234]\tLoss: 0.004884\n","1323it [00:25, 40.31it/s]Train epoch: 3 [batch #1325, batch_size 16, seq length 1248]\tLoss: 0.004453\n","1348it [00:26, 40.19it/s]Train epoch: 3 [batch #1350, batch_size 16, seq length 1262]\tLoss: 0.004979\n","1372it [00:26, 39.39it/s]Train epoch: 3 [batch #1375, batch_size 16, seq length 1276]\tLoss: 0.005453\n","1400it [00:27, 38.78it/s]Train epoch: 3 [batch #1400, batch_size 16, seq length 1290]\tLoss: 0.005126\n","1424it [00:27, 38.43it/s]Train epoch: 3 [batch #1425, batch_size 16, seq length 1304]\tLoss: 0.005216\n","1448it [00:28, 37.81it/s]Train epoch: 3 [batch #1450, batch_size 16, seq length 1319]\tLoss: 0.004992\n","1472it [00:29, 38.44it/s]Train epoch: 3 [batch #1475, batch_size 16, seq length 1332]\tLoss: 0.005380\n","1500it [00:29, 37.60it/s]Train epoch: 3 [batch #1500, batch_size 16, seq length 1346]\tLoss: 0.005173\n","1524it [00:30, 37.40it/s]Train epoch: 3 [batch #1525, batch_size 16, seq length 1362]\tLoss: 0.004545\n","1548it [00:31, 37.27it/s]Train epoch: 3 [batch #1550, batch_size 16, seq length 1377]\tLoss: 0.005308\n","1572it [00:31, 37.03it/s]Train epoch: 3 [batch #1575, batch_size 16, seq length 1392]\tLoss: 0.004731\n","1600it [00:32, 35.85it/s]Train epoch: 3 [batch #1600, batch_size 16, seq length 1406]\tLoss: 0.004805\n","1624it [00:33, 36.26it/s]Train epoch: 3 [batch #1625, batch_size 16, seq length 1422]\tLoss: 0.005257\n","1648it [00:34, 36.11it/s]Train epoch: 3 [batch #1650, batch_size 16, seq length 1437]\tLoss: 0.005165\n","1672it [00:34, 36.00it/s]Train epoch: 3 [batch #1675, batch_size 16, seq length 1454]\tLoss: 0.005336\n","1700it [00:35, 36.13it/s]Train epoch: 3 [batch #1700, batch_size 16, seq length 1469]\tLoss: 0.005542\n","1724it [00:36, 34.97it/s]Train epoch: 3 [batch #1725, batch_size 16, seq length 1484]\tLoss: 0.004947\n","1748it [00:36, 34.79it/s]Train epoch: 3 [batch #1750, batch_size 16, seq length 1501]\tLoss: 0.004959\n","1772it [00:37, 34.94it/s]Train epoch: 3 [batch #1775, batch_size 16, seq length 1518]\tLoss: 0.005451\n","1800it [00:38, 34.43it/s]Train epoch: 3 [batch #1800, batch_size 16, seq length 1533]\tLoss: 0.005505\n","1824it [00:39, 33.65it/s]Train epoch: 3 [batch #1825, batch_size 16, seq length 1548]\tLoss: 0.005501\n","1848it [00:39, 33.49it/s]Train epoch: 3 [batch #1850, batch_size 16, seq length 1566]\tLoss: 0.005121\n","1872it [00:40, 33.23it/s]Train epoch: 3 [batch #1875, batch_size 16, seq length 1582]\tLoss: 0.005362\n","1900it [00:41, 32.68it/s]Train epoch: 3 [batch #1900, batch_size 16, seq length 1599]\tLoss: 0.005847\n","1924it [00:42, 32.76it/s]Train epoch: 3 [batch #1925, batch_size 16, seq length 1616]\tLoss: 0.005454\n","1948it [00:42, 32.84it/s]Train epoch: 3 [batch #1950, batch_size 16, seq length 1633]\tLoss: 0.005334\n","1972it [00:43, 32.11it/s]Train epoch: 3 [batch #1975, batch_size 16, seq length 1651]\tLoss: 0.005416\n","2000it [00:44, 32.37it/s]Train epoch: 3 [batch #2000, batch_size 16, seq length 1670]\tLoss: 0.005196\n","2024it [00:45, 31.74it/s]Train epoch: 3 [batch #2025, batch_size 16, seq length 1688]\tLoss: 0.005235\n","2048it [00:45, 31.38it/s]Train epoch: 3 [batch #2050, batch_size 16, seq length 1707]\tLoss: 0.005494\n","2072it [00:46, 31.55it/s]Train epoch: 3 [batch #2075, batch_size 16, seq length 1728]\tLoss: 0.005617\n","2100it [00:47, 31.41it/s]Train epoch: 3 [batch #2100, batch_size 16, seq length 1748]\tLoss: 0.005224\n","2124it [00:48, 31.27it/s]Train epoch: 3 [batch #2125, batch_size 16, seq length 1768]\tLoss: 0.005898\n","2147it [00:49, 30.14it/s]Train epoch: 3 [batch #2150, batch_size 16, seq length 1787]\tLoss: 0.005213\n","2173it [00:50, 29.24it/s]Train epoch: 3 [batch #2175, batch_size 16, seq length 1809]\tLoss: 0.006065\n","2198it [00:50, 29.51it/s]Train epoch: 3 [batch #2200, batch_size 16, seq length 1833]\tLoss: 0.005523\n","2223it [00:51, 29.90it/s]Train epoch: 3 [batch #2225, batch_size 16, seq length 1855]\tLoss: 0.005564\n","2248it [00:52, 29.29it/s]Train epoch: 3 [batch #2250, batch_size 16, seq length 1878]\tLoss: 0.006065\n","2275it [00:53, 29.30it/s]Train epoch: 3 [batch #2275, batch_size 16, seq length 1901]\tLoss: 0.005760\n","2299it [00:54, 28.76it/s]Train epoch: 3 [batch #2300, batch_size 16, seq length 1927]\tLoss: 0.005692\n","2323it [00:55, 28.11it/s]Train epoch: 3 [batch #2325, batch_size 16, seq length 1953]\tLoss: 0.006116\n","2350it [00:56, 28.41it/s]Train epoch: 3 [batch #2350, batch_size 16, seq length 1980]\tLoss: 0.005736\n","2374it [00:56, 28.15it/s]Train epoch: 3 [batch #2375, batch_size 16, seq length 2006]\tLoss: 0.006117\n","2398it [00:57, 27.55it/s]Train epoch: 3 [batch #2400, batch_size 16, seq length 2034]\tLoss: 0.006034\n","2425it [00:58, 27.19it/s]Train epoch: 3 [batch #2425, batch_size 16, seq length 2064]\tLoss: 0.005852\n","2449it [00:59, 26.75it/s]Train epoch: 3 [batch #2450, batch_size 16, seq length 2094]\tLoss: 0.006184\n","2473it [01:00, 26.87it/s]Train epoch: 3 [batch #2475, batch_size 16, seq length 2126]\tLoss: 0.006451\n","2500it [01:01, 26.34it/s]Train epoch: 3 [batch #2500, batch_size 16, seq length 2160]\tLoss: 0.006131\n","2524it [01:02, 26.45it/s]Train epoch: 3 [batch #2525, batch_size 16, seq length 2192]\tLoss: 0.006417\n","2548it [01:03, 26.15it/s]Train epoch: 3 [batch #2550, batch_size 16, seq length 2228]\tLoss: 0.006180\n","2575it [01:04, 26.19it/s]Train epoch: 3 [batch #2575, batch_size 16, seq length 2266]\tLoss: 0.006632\n","2599it [01:05, 24.79it/s]Train epoch: 3 [batch #2600, batch_size 16, seq length 2307]\tLoss: 0.006830\n","2623it [01:06, 24.83it/s]Train epoch: 3 [batch #2625, batch_size 16, seq length 2350]\tLoss: 0.006445\n","2650it [01:07, 24.53it/s]Train epoch: 3 [batch #2650, batch_size 16, seq length 2392]\tLoss: 0.006283\n","2674it [01:08, 24.16it/s]Train epoch: 3 [batch #2675, batch_size 16, seq length 2439]\tLoss: 0.006738\n","2698it [01:09, 23.50it/s]Train epoch: 3 [batch #2700, batch_size 16, seq length 2491]\tLoss: 0.006812\n","2725it [01:10, 23.39it/s]Train epoch: 3 [batch #2725, batch_size 16, seq length 2500]\tLoss: 0.006959\n","2749it [01:11, 23.28it/s]Train epoch: 3 [batch #2750, batch_size 16, seq length 2500]\tLoss: 0.006936\n","2773it [01:12, 23.08it/s]Train epoch: 3 [batch #2775, batch_size 16, seq length 2500]\tLoss: 0.007160\n","2800it [01:13, 22.63it/s]Train epoch: 3 [batch #2800, batch_size 16, seq length 2500]\tLoss: 0.007270\n","2824it [01:14, 22.60it/s]Train epoch: 3 [batch #2825, batch_size 16, seq length 2500]\tLoss: 0.007057\n","2848it [01:16, 21.83it/s]Train epoch: 3 [batch #2850, batch_size 16, seq length 2500]\tLoss: 0.007365\n","2875it [01:17, 22.09it/s]Train epoch: 3 [batch #2875, batch_size 16, seq length 2500]\tLoss: 0.008118\n","2899it [01:18, 21.61it/s]Train epoch: 3 [batch #2900, batch_size 16, seq length 2500]\tLoss: 0.008103\n","2923it [01:19, 21.28it/s]Train epoch: 3 [batch #2925, batch_size 16, seq length 2500]\tLoss: 0.008492\n","2949it [01:20, 20.06it/s]Train epoch: 3 [batch #2950, batch_size 16, seq length 2500]\tLoss: 0.008613\n","2975it [01:22, 18.09it/s]Train epoch: 3 [batch #2975, batch_size 16, seq length 2500]\tLoss: 0.010329\n","2983it [01:22, 36.10it/s]\n","epoch loss: 0.005258497720233974\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:03, 434.61it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0240, 0.0412, 0.0379, 0.0395, 0.8373\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2745, 0.5047, 0.3757, 0.4308, 0.9712\n","rec_at_8: 0.3156\n","prec_at_8: 0.5897\n","rec_at_15: 0.4291\n","prec_at_15: 0.4472\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/cnn_vanilla_May_02_220247\n","\n","EPOCH 4\n","0it [00:00, ?it/s]Train epoch: 4 [batch #0, batch_size 16, seq length 117]\tLoss: 0.009670\n","23it [00:00, 73.08it/s]Train epoch: 4 [batch #25, batch_size 16, seq length 337]\tLoss: 0.004722\n","49it [00:00, 80.19it/s]Train epoch: 4 [batch #50, batch_size 16, seq length 402]\tLoss: 0.003751\n","75it [00:00, 79.27it/s]Train epoch: 4 [batch #75, batch_size 16, seq length 452]\tLoss: 0.003402\n","99it [00:01, 77.53it/s]Train epoch: 4 [batch #100, batch_size 16, seq length 490]\tLoss: 0.003210\n","123it [00:01, 74.07it/s]Train epoch: 4 [batch #125, batch_size 16, seq length 520]\tLoss: 0.003587\n","147it [00:01, 70.19it/s]Train epoch: 4 [batch #150, batch_size 16, seq length 548]\tLoss: 0.003663\n","169it [00:02, 68.51it/s]Train epoch: 4 [batch #175, batch_size 16, seq length 574]\tLoss: 0.003344\n","197it [00:02, 68.10it/s]Train epoch: 4 [batch #200, batch_size 16, seq length 596]\tLoss: 0.003593\n","225it [00:03, 65.58it/s]Train epoch: 4 [batch #225, batch_size 16, seq length 618]\tLoss: 0.003649\n","246it [00:03, 65.22it/s]Train epoch: 4 [batch #250, batch_size 16, seq length 638]\tLoss: 0.003382\n","274it [00:03, 63.82it/s]Train epoch: 4 [batch #275, batch_size 16, seq length 656]\tLoss: 0.003716\n","295it [00:04, 63.06it/s]Train epoch: 4 [batch #300, batch_size 16, seq length 674]\tLoss: 0.003765\n","323it [00:04, 61.44it/s]Train epoch: 4 [batch #325, batch_size 16, seq length 693]\tLoss: 0.003520\n","344it [00:05, 60.40it/s]Train epoch: 4 [batch #350, batch_size 16, seq length 710]\tLoss: 0.003619\n","372it [00:05, 59.68it/s]Train epoch: 4 [batch #375, batch_size 16, seq length 726]\tLoss: 0.004202\n","396it [00:05, 57.35it/s]Train epoch: 4 [batch #400, batch_size 16, seq length 743]\tLoss: 0.003589\n","420it [00:06, 57.38it/s]Train epoch: 4 [batch #425, batch_size 16, seq length 758]\tLoss: 0.003668\n","450it [00:06, 57.46it/s]Train epoch: 4 [batch #450, batch_size 16, seq length 773]\tLoss: 0.003968\n","474it [00:07, 55.75it/s]Train epoch: 4 [batch #475, batch_size 16, seq length 789]\tLoss: 0.003409\n","498it [00:07, 55.92it/s]Train epoch: 4 [batch #500, batch_size 16, seq length 803]\tLoss: 0.003787\n","522it [00:08, 55.19it/s]Train epoch: 4 [batch #525, batch_size 16, seq length 818]\tLoss: 0.003772\n","546it [00:08, 55.41it/s]Train epoch: 4 [batch #550, batch_size 16, seq length 833]\tLoss: 0.003719\n","570it [00:09, 54.30it/s]Train epoch: 4 [batch #575, batch_size 16, seq length 846]\tLoss: 0.003548\n","600it [00:09, 54.66it/s]Train epoch: 4 [batch #600, batch_size 16, seq length 860]\tLoss: 0.004142\n","624it [00:10, 54.28it/s]Train epoch: 4 [batch #625, batch_size 16, seq length 874]\tLoss: 0.003995\n","648it [00:10, 53.38it/s]Train epoch: 4 [batch #650, batch_size 16, seq length 887]\tLoss: 0.003732\n","672it [00:10, 54.06it/s]Train epoch: 4 [batch #675, batch_size 16, seq length 901]\tLoss: 0.003759\n","696it [00:11, 52.59it/s]Train epoch: 4 [batch #700, batch_size 16, seq length 915]\tLoss: 0.003622\n","720it [00:11, 52.42it/s]Train epoch: 4 [batch #725, batch_size 16, seq length 929]\tLoss: 0.003921\n","750it [00:12, 51.87it/s]Train epoch: 4 [batch #750, batch_size 16, seq length 942]\tLoss: 0.004196\n","773it [00:12, 49.55it/s]Train epoch: 4 [batch #775, batch_size 16, seq length 954]\tLoss: 0.004094\n","796it [00:13, 49.80it/s]Train epoch: 4 [batch #800, batch_size 16, seq length 968]\tLoss: 0.004123\n","823it [00:13, 48.72it/s]Train epoch: 4 [batch #825, batch_size 16, seq length 981]\tLoss: 0.004134\n","850it [00:14, 47.78it/s]Train epoch: 4 [batch #850, batch_size 16, seq length 994]\tLoss: 0.004083\n","875it [00:15, 47.12it/s]Train epoch: 4 [batch #875, batch_size 16, seq length 1006]\tLoss: 0.003902\n","900it [00:15, 46.60it/s]Train epoch: 4 [batch #900, batch_size 16, seq length 1020]\tLoss: 0.004294\n","925it [00:16, 46.57it/s]Train epoch: 4 [batch #925, batch_size 16, seq length 1033]\tLoss: 0.003962\n","950it [00:16, 45.51it/s]Train epoch: 4 [batch #950, batch_size 16, seq length 1046]\tLoss: 0.004347\n","975it [00:17, 44.41it/s]Train epoch: 4 [batch #975, batch_size 16, seq length 1059]\tLoss: 0.004160\n","1000it [00:17, 45.04it/s]Train epoch: 4 [batch #1000, batch_size 16, seq length 1073]\tLoss: 0.004279\n","1025it [00:18, 44.00it/s]Train epoch: 4 [batch #1025, batch_size 16, seq length 1085]\tLoss: 0.004366\n","1050it [00:18, 44.15it/s]Train epoch: 4 [batch #1050, batch_size 16, seq length 1099]\tLoss: 0.004367\n","1075it [00:19, 43.98it/s]Train epoch: 4 [batch #1075, batch_size 16, seq length 1113]\tLoss: 0.004466\n","1100it [00:20, 43.28it/s]Train epoch: 4 [batch #1100, batch_size 16, seq length 1126]\tLoss: 0.004270\n","1125it [00:20, 42.72it/s]Train epoch: 4 [batch #1125, batch_size 16, seq length 1140]\tLoss: 0.004519\n","1150it [00:21, 42.04it/s]Train epoch: 4 [batch #1150, batch_size 16, seq length 1154]\tLoss: 0.004467\n","1175it [00:21, 41.20it/s]Train epoch: 4 [batch #1175, batch_size 16, seq length 1167]\tLoss: 0.004549\n","1200it [00:22, 41.86it/s]Train epoch: 4 [batch #1200, batch_size 16, seq length 1180]\tLoss: 0.004426\n","1225it [00:23, 41.55it/s]Train epoch: 4 [batch #1225, batch_size 16, seq length 1195]\tLoss: 0.004387\n","1250it [00:23, 41.14it/s]Train epoch: 4 [batch #1250, batch_size 16, seq length 1208]\tLoss: 0.004305\n","1275it [00:24, 40.39it/s]Train epoch: 4 [batch #1275, batch_size 16, seq length 1220]\tLoss: 0.004354\n","1300it [00:24, 40.75it/s]Train epoch: 4 [batch #1300, batch_size 16, seq length 1234]\tLoss: 0.004528\n","1324it [00:25, 39.53it/s]Train epoch: 4 [batch #1325, batch_size 16, seq length 1248]\tLoss: 0.004233\n","1346it [00:26, 39.75it/s]Train epoch: 4 [batch #1350, batch_size 16, seq length 1262]\tLoss: 0.004598\n","1372it [00:26, 39.39it/s]Train epoch: 4 [batch #1375, batch_size 16, seq length 1276]\tLoss: 0.005081\n","1400it [00:27, 38.22it/s]Train epoch: 4 [batch #1400, batch_size 16, seq length 1290]\tLoss: 0.004832\n","1424it [00:28, 38.03it/s]Train epoch: 4 [batch #1425, batch_size 16, seq length 1304]\tLoss: 0.004912\n","1448it [00:28, 38.10it/s]Train epoch: 4 [batch #1450, batch_size 16, seq length 1319]\tLoss: 0.004660\n","1472it [00:29, 38.22it/s]Train epoch: 4 [batch #1475, batch_size 16, seq length 1332]\tLoss: 0.004961\n","1500it [00:30, 37.01it/s]Train epoch: 4 [batch #1500, batch_size 16, seq length 1346]\tLoss: 0.004769\n","1524it [00:30, 37.03it/s]Train epoch: 4 [batch #1525, batch_size 16, seq length 1362]\tLoss: 0.004260\n","1548it [00:31, 37.00it/s]Train epoch: 4 [batch #1550, batch_size 16, seq length 1377]\tLoss: 0.005017\n","1572it [00:32, 36.61it/s]Train epoch: 4 [batch #1575, batch_size 16, seq length 1392]\tLoss: 0.004396\n","1600it [00:32, 36.25it/s]Train epoch: 4 [batch #1600, batch_size 16, seq length 1406]\tLoss: 0.004545\n","1624it [00:33, 36.44it/s]Train epoch: 4 [batch #1625, batch_size 16, seq length 1422]\tLoss: 0.004933\n","1648it [00:34, 36.01it/s]Train epoch: 4 [batch #1650, batch_size 16, seq length 1437]\tLoss: 0.004801\n","1672it [00:34, 35.64it/s]Train epoch: 4 [batch #1675, batch_size 16, seq length 1454]\tLoss: 0.004957\n","1700it [00:35, 35.16it/s]Train epoch: 4 [batch #1700, batch_size 16, seq length 1469]\tLoss: 0.005197\n","1724it [00:36, 34.24it/s]Train epoch: 4 [batch #1725, batch_size 16, seq length 1484]\tLoss: 0.004691\n","1748it [00:36, 34.05it/s]Train epoch: 4 [batch #1750, batch_size 16, seq length 1501]\tLoss: 0.004661\n","1772it [00:37, 34.63it/s]Train epoch: 4 [batch #1775, batch_size 16, seq length 1518]\tLoss: 0.005074\n","1800it [00:38, 33.97it/s]Train epoch: 4 [batch #1800, batch_size 16, seq length 1533]\tLoss: 0.005100\n","1824it [00:39, 33.20it/s]Train epoch: 4 [batch #1825, batch_size 16, seq length 1548]\tLoss: 0.005145\n","1848it [00:39, 33.29it/s]Train epoch: 4 [batch #1850, batch_size 16, seq length 1566]\tLoss: 0.004823\n","1872it [00:40, 33.35it/s]Train epoch: 4 [batch #1875, batch_size 16, seq length 1582]\tLoss: 0.005071\n","1900it [00:41, 32.94it/s]Train epoch: 4 [batch #1900, batch_size 16, seq length 1599]\tLoss: 0.005507\n","1924it [00:42, 32.71it/s]Train epoch: 4 [batch #1925, batch_size 16, seq length 1616]\tLoss: 0.005095\n","1948it [00:43, 32.54it/s]Train epoch: 4 [batch #1950, batch_size 16, seq length 1633]\tLoss: 0.004991\n","1972it [00:43, 31.67it/s]Train epoch: 4 [batch #1975, batch_size 16, seq length 1651]\tLoss: 0.005009\n","2000it [00:44, 32.19it/s]Train epoch: 4 [batch #2000, batch_size 16, seq length 1670]\tLoss: 0.004893\n","2024it [00:45, 32.16it/s]Train epoch: 4 [batch #2025, batch_size 16, seq length 1688]\tLoss: 0.004979\n","2048it [00:46, 31.31it/s]Train epoch: 4 [batch #2050, batch_size 16, seq length 1707]\tLoss: 0.005157\n","2072it [00:46, 31.30it/s]Train epoch: 4 [batch #2075, batch_size 16, seq length 1728]\tLoss: 0.005278\n","2100it [00:47, 31.14it/s]Train epoch: 4 [batch #2100, batch_size 16, seq length 1748]\tLoss: 0.004892\n","2124it [00:48, 31.42it/s]Train epoch: 4 [batch #2125, batch_size 16, seq length 1768]\tLoss: 0.005543\n","2148it [00:49, 30.33it/s]Train epoch: 4 [batch #2150, batch_size 16, seq length 1787]\tLoss: 0.004917\n","2172it [00:50, 30.04it/s]Train epoch: 4 [batch #2175, batch_size 16, seq length 1809]\tLoss: 0.005660\n","2197it [00:51, 29.35it/s]Train epoch: 4 [batch #2200, batch_size 16, seq length 1833]\tLoss: 0.005168\n","2225it [00:51, 29.21it/s]Train epoch: 4 [batch #2225, batch_size 16, seq length 1855]\tLoss: 0.005298\n","2250it [00:52, 29.51it/s]Train epoch: 4 [batch #2250, batch_size 16, seq length 1878]\tLoss: 0.005684\n","2274it [00:53, 29.06it/s]Train epoch: 4 [batch #2275, batch_size 16, seq length 1901]\tLoss: 0.005426\n","2298it [00:54, 28.81it/s]Train epoch: 4 [batch #2300, batch_size 16, seq length 1927]\tLoss: 0.005375\n","2325it [00:55, 29.00it/s]Train epoch: 4 [batch #2325, batch_size 16, seq length 1953]\tLoss: 0.005751\n","2349it [00:56, 28.18it/s]Train epoch: 4 [batch #2350, batch_size 16, seq length 1980]\tLoss: 0.005460\n","2373it [00:57, 27.77it/s]Train epoch: 4 [batch #2375, batch_size 16, seq length 2006]\tLoss: 0.005750\n","2400it [00:58, 26.94it/s]Train epoch: 4 [batch #2400, batch_size 16, seq length 2034]\tLoss: 0.005668\n","2424it [00:59, 26.80it/s]Train epoch: 4 [batch #2425, batch_size 16, seq length 2064]\tLoss: 0.005557\n","2448it [00:59, 27.09it/s]Train epoch: 4 [batch #2450, batch_size 16, seq length 2094]\tLoss: 0.005866\n","2475it [01:00, 26.83it/s]Train epoch: 4 [batch #2475, batch_size 16, seq length 2126]\tLoss: 0.006055\n","2499it [01:01, 26.83it/s]Train epoch: 4 [batch #2500, batch_size 16, seq length 2160]\tLoss: 0.005901\n","2523it [01:02, 26.33it/s]Train epoch: 4 [batch #2525, batch_size 16, seq length 2192]\tLoss: 0.006048\n","2550it [01:03, 25.89it/s]Train epoch: 4 [batch #2550, batch_size 16, seq length 2228]\tLoss: 0.005914\n","2574it [01:04, 25.90it/s]Train epoch: 4 [batch #2575, batch_size 16, seq length 2266]\tLoss: 0.006285\n","2598it [01:05, 25.48it/s]Train epoch: 4 [batch #2600, batch_size 16, seq length 2307]\tLoss: 0.006479\n","2625it [01:06, 24.72it/s]Train epoch: 4 [batch #2625, batch_size 16, seq length 2350]\tLoss: 0.006107\n","2649it [01:07, 23.62it/s]Train epoch: 4 [batch #2650, batch_size 16, seq length 2392]\tLoss: 0.006058\n","2673it [01:08, 24.12it/s]Train epoch: 4 [batch #2675, batch_size 16, seq length 2439]\tLoss: 0.006380\n","2700it [01:09, 23.91it/s]Train epoch: 4 [batch #2700, batch_size 16, seq length 2491]\tLoss: 0.006538\n","2724it [01:10, 23.22it/s]Train epoch: 4 [batch #2725, batch_size 16, seq length 2500]\tLoss: 0.006644\n","2748it [01:11, 23.06it/s]Train epoch: 4 [batch #2750, batch_size 16, seq length 2500]\tLoss: 0.006605\n","2775it [01:13, 22.90it/s]Train epoch: 4 [batch #2775, batch_size 16, seq length 2500]\tLoss: 0.006857\n","2799it [01:14, 22.94it/s]Train epoch: 4 [batch #2800, batch_size 16, seq length 2500]\tLoss: 0.006934\n","2823it [01:15, 22.11it/s]Train epoch: 4 [batch #2825, batch_size 16, seq length 2500]\tLoss: 0.006736\n","2850it [01:16, 22.46it/s]Train epoch: 4 [batch #2850, batch_size 16, seq length 2500]\tLoss: 0.007073\n","2874it [01:17, 21.55it/s]Train epoch: 4 [batch #2875, batch_size 16, seq length 2500]\tLoss: 0.007764\n","2898it [01:18, 21.30it/s]Train epoch: 4 [batch #2900, batch_size 16, seq length 2500]\tLoss: 0.007730\n","2925it [01:19, 20.57it/s]Train epoch: 4 [batch #2925, batch_size 16, seq length 2500]\tLoss: 0.008239\n","2950it [01:21, 19.80it/s]Train epoch: 4 [batch #2950, batch_size 16, seq length 2500]\tLoss: 0.008059\n","2974it [01:22, 17.88it/s]Train epoch: 4 [batch #2975, batch_size 16, seq length 2500]\tLoss: 0.009677\n","2983it [01:23, 35.92it/s]\n","epoch loss: 0.004933715258852057\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:03, 441.94it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0243, 0.0425, 0.0357, 0.0388, 0.8376\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2809, 0.5512, 0.3642, 0.4386, 0.9724\n","rec_at_8: 0.3279\n","prec_at_8: 0.6087\n","rec_at_15: 0.4448\n","prec_at_15: 0.4612\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/cnn_vanilla_May_02_220247\n","\n","EPOCH 5\n","0it [00:00, ?it/s]Train epoch: 5 [batch #0, batch_size 16, seq length 117]\tLoss: 0.007237\n","25it [00:00, 67.03it/s]Train epoch: 5 [batch #25, batch_size 16, seq length 337]\tLoss: 0.004644\n","43it [00:00, 75.91it/s]Train epoch: 5 [batch #50, batch_size 16, seq length 402]\tLoss: 0.003479\n","68it [00:00, 78.90it/s]Train epoch: 5 [batch #75, batch_size 16, seq length 452]\tLoss: 0.003159\n","93it [00:01, 77.15it/s]Train epoch: 5 [batch #100, batch_size 16, seq length 490]\tLoss: 0.002989\n","125it [00:01, 75.60it/s]Train epoch: 5 [batch #125, batch_size 16, seq length 520]\tLoss: 0.003560\n","149it [00:02, 70.82it/s]Train epoch: 5 [batch #150, batch_size 16, seq length 548]\tLoss: 0.003582\n","171it [00:02, 67.85it/s]Train epoch: 5 [batch #175, batch_size 16, seq length 574]\tLoss: 0.003201\n","199it [00:02, 66.94it/s]Train epoch: 5 [batch #200, batch_size 16, seq length 596]\tLoss: 0.003394\n","220it [00:03, 65.88it/s]Train epoch: 5 [batch #225, batch_size 16, seq length 618]\tLoss: 0.003500\n","248it [00:03, 64.95it/s]Train epoch: 5 [batch #250, batch_size 16, seq length 638]\tLoss: 0.003193\n","269it [00:03, 62.68it/s]Train epoch: 5 [batch #275, batch_size 16, seq length 656]\tLoss: 0.003543\n","297it [00:04, 62.09it/s]Train epoch: 5 [batch #300, batch_size 16, seq length 674]\tLoss: 0.003615\n","325it [00:04, 61.76it/s]Train epoch: 5 [batch #325, batch_size 16, seq length 693]\tLoss: 0.003360\n","346it [00:05, 60.76it/s]Train epoch: 5 [batch #350, batch_size 16, seq length 710]\tLoss: 0.003494\n","372it [00:05, 57.51it/s]Train epoch: 5 [batch #375, batch_size 16, seq length 726]\tLoss: 0.004076\n","396it [00:06, 53.38it/s]Train epoch: 5 [batch #400, batch_size 16, seq length 743]\tLoss: 0.003405\n","420it [00:06, 54.68it/s]Train epoch: 5 [batch #425, batch_size 16, seq length 758]\tLoss: 0.003430\n","450it [00:07, 56.74it/s]Train epoch: 5 [batch #450, batch_size 16, seq length 773]\tLoss: 0.003716\n","474it [00:07, 55.52it/s]Train epoch: 5 [batch #475, batch_size 16, seq length 789]\tLoss: 0.003254\n","498it [00:07, 55.29it/s]Train epoch: 5 [batch #500, batch_size 16, seq length 803]\tLoss: 0.003639\n","522it [00:08, 55.11it/s]Train epoch: 5 [batch #525, batch_size 16, seq length 818]\tLoss: 0.003544\n","546it [00:08, 55.26it/s]Train epoch: 5 [batch #550, batch_size 16, seq length 833]\tLoss: 0.003533\n","570it [00:09, 54.80it/s]Train epoch: 5 [batch #575, batch_size 16, seq length 846]\tLoss: 0.003387\n","600it [00:09, 52.86it/s]Train epoch: 5 [batch #600, batch_size 16, seq length 860]\tLoss: 0.003929\n","624it [00:10, 53.82it/s]Train epoch: 5 [batch #625, batch_size 16, seq length 874]\tLoss: 0.003806\n","648it [00:10, 53.73it/s]Train epoch: 5 [batch #650, batch_size 16, seq length 887]\tLoss: 0.003562\n","672it [00:11, 53.64it/s]Train epoch: 5 [batch #675, batch_size 16, seq length 901]\tLoss: 0.003605\n","696it [00:11, 52.80it/s]Train epoch: 5 [batch #700, batch_size 16, seq length 915]\tLoss: 0.003436\n","720it [00:12, 52.21it/s]Train epoch: 5 [batch #725, batch_size 16, seq length 929]\tLoss: 0.003662\n","750it [00:12, 51.51it/s]Train epoch: 5 [batch #750, batch_size 16, seq length 942]\tLoss: 0.003925\n","774it [00:13, 50.20it/s]Train epoch: 5 [batch #775, batch_size 16, seq length 954]\tLoss: 0.003831\n","796it [00:13, 49.36it/s]Train epoch: 5 [batch #800, batch_size 16, seq length 968]\tLoss: 0.003889\n","822it [00:14, 48.81it/s]Train epoch: 5 [batch #825, batch_size 16, seq length 981]\tLoss: 0.003864\n","849it [00:14, 47.58it/s]Train epoch: 5 [batch #850, batch_size 16, seq length 994]\tLoss: 0.003932\n","875it [00:15, 48.76it/s]Train epoch: 5 [batch #875, batch_size 16, seq length 1006]\tLoss: 0.003713\n","896it [00:15, 47.30it/s]Train epoch: 5 [batch #900, batch_size 16, seq length 1020]\tLoss: 0.004066\n","921it [00:16, 46.50it/s]Train epoch: 5 [batch #925, batch_size 16, seq length 1033]\tLoss: 0.003734\n","946it [00:16, 45.58it/s]Train epoch: 5 [batch #950, batch_size 16, seq length 1046]\tLoss: 0.004166\n","971it [00:17, 45.33it/s]Train epoch: 5 [batch #975, batch_size 16, seq length 1059]\tLoss: 0.003917\n","996it [00:17, 44.83it/s]Train epoch: 5 [batch #1000, batch_size 16, seq length 1073]\tLoss: 0.004057\n","1021it [00:18, 44.64it/s]Train epoch: 5 [batch #1025, batch_size 16, seq length 1085]\tLoss: 0.004168\n","1046it [00:18, 43.72it/s]Train epoch: 5 [batch #1050, batch_size 16, seq length 1099]\tLoss: 0.004194\n","1071it [00:19, 43.15it/s]Train epoch: 5 [batch #1075, batch_size 16, seq length 1113]\tLoss: 0.004201\n","1096it [00:20, 43.15it/s]Train epoch: 5 [batch #1100, batch_size 16, seq length 1126]\tLoss: 0.004015\n","1121it [00:20, 43.05it/s]Train epoch: 5 [batch #1125, batch_size 16, seq length 1140]\tLoss: 0.004262\n","1146it [00:21, 42.52it/s]Train epoch: 5 [batch #1150, batch_size 16, seq length 1154]\tLoss: 0.004175\n","1171it [00:21, 41.88it/s]Train epoch: 5 [batch #1175, batch_size 16, seq length 1167]\tLoss: 0.004326\n","1196it [00:22, 42.06it/s]Train epoch: 5 [batch #1200, batch_size 16, seq length 1180]\tLoss: 0.004062\n","1221it [00:23, 41.20it/s]Train epoch: 5 [batch #1225, batch_size 16, seq length 1195]\tLoss: 0.004108\n","1246it [00:23, 40.68it/s]Train epoch: 5 [batch #1250, batch_size 16, seq length 1208]\tLoss: 0.004081\n","1271it [00:24, 40.90it/s]Train epoch: 5 [batch #1275, batch_size 16, seq length 1220]\tLoss: 0.004094\n","1296it [00:24, 40.04it/s]Train epoch: 5 [batch #1300, batch_size 16, seq length 1234]\tLoss: 0.004266\n","1321it [00:25, 40.46it/s]Train epoch: 5 [batch #1325, batch_size 16, seq length 1248]\tLoss: 0.004052\n","1346it [00:26, 40.49it/s]Train epoch: 5 [batch #1350, batch_size 16, seq length 1262]\tLoss: 0.004340\n","1372it [00:26, 38.86it/s]Train epoch: 5 [batch #1375, batch_size 16, seq length 1276]\tLoss: 0.004778\n","1400it [00:27, 38.51it/s]Train epoch: 5 [batch #1400, batch_size 16, seq length 1290]\tLoss: 0.004591\n","1424it [00:28, 38.77it/s]Train epoch: 5 [batch #1425, batch_size 16, seq length 1304]\tLoss: 0.004683\n","1448it [00:28, 37.57it/s]Train epoch: 5 [batch #1450, batch_size 16, seq length 1319]\tLoss: 0.004400\n","1472it [00:29, 36.39it/s]Train epoch: 5 [batch #1475, batch_size 16, seq length 1332]\tLoss: 0.004620\n","1500it [00:30, 37.03it/s]Train epoch: 5 [batch #1500, batch_size 16, seq length 1346]\tLoss: 0.004561\n","1524it [00:30, 36.60it/s]Train epoch: 5 [batch #1525, batch_size 16, seq length 1362]\tLoss: 0.004006\n","1548it [00:31, 36.98it/s]Train epoch: 5 [batch #1550, batch_size 16, seq length 1377]\tLoss: 0.004778\n","1572it [00:32, 37.24it/s]Train epoch: 5 [batch #1575, batch_size 16, seq length 1392]\tLoss: 0.004129\n","1600it [00:32, 36.33it/s]Train epoch: 5 [batch #1600, batch_size 16, seq length 1406]\tLoss: 0.004294\n","1624it [00:33, 36.06it/s]Train epoch: 5 [batch #1625, batch_size 16, seq length 1422]\tLoss: 0.004795\n","1648it [00:34, 35.55it/s]Train epoch: 5 [batch #1650, batch_size 16, seq length 1437]\tLoss: 0.004580\n","1672it [00:34, 35.77it/s]Train epoch: 5 [batch #1675, batch_size 16, seq length 1454]\tLoss: 0.004681\n","1700it [00:35, 35.47it/s]Train epoch: 5 [batch #1700, batch_size 16, seq length 1469]\tLoss: 0.004933\n","1724it [00:36, 34.94it/s]Train epoch: 5 [batch #1725, batch_size 16, seq length 1484]\tLoss: 0.004436\n","1748it [00:37, 35.01it/s]Train epoch: 5 [batch #1750, batch_size 16, seq length 1501]\tLoss: 0.004366\n","1772it [00:37, 35.03it/s]Train epoch: 5 [batch #1775, batch_size 16, seq length 1518]\tLoss: 0.004842\n","1800it [00:38, 34.36it/s]Train epoch: 5 [batch #1800, batch_size 16, seq length 1533]\tLoss: 0.004877\n","1824it [00:39, 34.00it/s]Train epoch: 5 [batch #1825, batch_size 16, seq length 1548]\tLoss: 0.004957\n","1848it [00:40, 33.40it/s]Train epoch: 5 [batch #1850, batch_size 16, seq length 1566]\tLoss: 0.004643\n","1872it [00:40, 32.34it/s]Train epoch: 5 [batch #1875, batch_size 16, seq length 1582]\tLoss: 0.004794\n","1900it [00:41, 33.01it/s]Train epoch: 5 [batch #1900, batch_size 16, seq length 1599]\tLoss: 0.005157\n","1924it [00:42, 32.56it/s]Train epoch: 5 [batch #1925, batch_size 16, seq length 1616]\tLoss: 0.004825\n","1948it [00:43, 32.34it/s]Train epoch: 5 [batch #1950, batch_size 16, seq length 1633]\tLoss: 0.004810\n","1972it [00:43, 32.18it/s]Train epoch: 5 [batch #1975, batch_size 16, seq length 1651]\tLoss: 0.004771\n","2000it [00:44, 31.93it/s]Train epoch: 5 [batch #2000, batch_size 16, seq length 1670]\tLoss: 0.004656\n","2024it [00:45, 32.18it/s]Train epoch: 5 [batch #2025, batch_size 16, seq length 1688]\tLoss: 0.004726\n","2048it [00:46, 32.03it/s]Train epoch: 5 [batch #2050, batch_size 16, seq length 1707]\tLoss: 0.004882\n","2072it [00:47, 31.91it/s]Train epoch: 5 [batch #2075, batch_size 16, seq length 1728]\tLoss: 0.004998\n","2100it [00:47, 31.50it/s]Train epoch: 5 [batch #2100, batch_size 16, seq length 1748]\tLoss: 0.004669\n","2124it [00:48, 31.01it/s]Train epoch: 5 [batch #2125, batch_size 16, seq length 1768]\tLoss: 0.005290\n","2148it [00:49, 30.35it/s]Train epoch: 5 [batch #2150, batch_size 16, seq length 1787]\tLoss: 0.004677\n","2175it [00:50, 29.98it/s]Train epoch: 5 [batch #2175, batch_size 16, seq length 1809]\tLoss: 0.005453\n","2198it [00:51, 29.28it/s]Train epoch: 5 [batch #2200, batch_size 16, seq length 1833]\tLoss: 0.005038\n","2225it [00:52, 30.02it/s]Train epoch: 5 [batch #2225, batch_size 16, seq length 1855]\tLoss: 0.005018\n","2250it [00:52, 29.80it/s]Train epoch: 5 [batch #2250, batch_size 16, seq length 1878]\tLoss: 0.005378\n","2274it [00:53, 29.25it/s]Train epoch: 5 [batch #2275, batch_size 16, seq length 1901]\tLoss: 0.005260\n","2298it [00:54, 28.99it/s]Train epoch: 5 [batch #2300, batch_size 16, seq length 1927]\tLoss: 0.005157\n","2325it [00:55, 28.54it/s]Train epoch: 5 [batch #2325, batch_size 16, seq length 1953]\tLoss: 0.005572\n","2349it [00:56, 28.05it/s]Train epoch: 5 [batch #2350, batch_size 16, seq length 1980]\tLoss: 0.005211\n","2373it [00:57, 28.29it/s]Train epoch: 5 [batch #2375, batch_size 16, seq length 2006]\tLoss: 0.005508\n","2400it [00:58, 27.88it/s]Train epoch: 5 [batch #2400, batch_size 16, seq length 2034]\tLoss: 0.005419\n","2424it [00:59, 27.60it/s]Train epoch: 5 [batch #2425, batch_size 16, seq length 2064]\tLoss: 0.005366\n","2448it [00:59, 27.16it/s]Train epoch: 5 [batch #2450, batch_size 16, seq length 2094]\tLoss: 0.005671\n","2475it [01:01, 26.42it/s]Train epoch: 5 [batch #2475, batch_size 16, seq length 2126]\tLoss: 0.005764\n","2499it [01:01, 26.72it/s]Train epoch: 5 [batch #2500, batch_size 16, seq length 2160]\tLoss: 0.005602\n","2523it [01:02, 26.60it/s]Train epoch: 5 [batch #2525, batch_size 16, seq length 2192]\tLoss: 0.005838\n","2550it [01:03, 25.88it/s]Train epoch: 5 [batch #2550, batch_size 16, seq length 2228]\tLoss: 0.005672\n","2574it [01:04, 25.55it/s]Train epoch: 5 [batch #2575, batch_size 16, seq length 2266]\tLoss: 0.006070\n","2598it [01:05, 24.63it/s]Train epoch: 5 [batch #2600, batch_size 16, seq length 2307]\tLoss: 0.006095\n","2625it [01:06, 23.83it/s]Train epoch: 5 [batch #2625, batch_size 16, seq length 2350]\tLoss: 0.005840\n","2649it [01:07, 24.54it/s]Train epoch: 5 [batch #2650, batch_size 16, seq length 2392]\tLoss: 0.005847\n","2673it [01:08, 24.66it/s]Train epoch: 5 [batch #2675, batch_size 16, seq length 2439]\tLoss: 0.006168\n","2700it [01:09, 23.97it/s]Train epoch: 5 [batch #2700, batch_size 16, seq length 2491]\tLoss: 0.006238\n","2724it [01:11, 23.28it/s]Train epoch: 5 [batch #2725, batch_size 16, seq length 2500]\tLoss: 0.006353\n","2748it [01:12, 23.43it/s]Train epoch: 5 [batch #2750, batch_size 16, seq length 2500]\tLoss: 0.006291\n","2775it [01:13, 23.26it/s]Train epoch: 5 [batch #2775, batch_size 16, seq length 2500]\tLoss: 0.006593\n","2799it [01:14, 22.83it/s]Train epoch: 5 [batch #2800, batch_size 16, seq length 2500]\tLoss: 0.006671\n","2823it [01:15, 22.40it/s]Train epoch: 5 [batch #2825, batch_size 16, seq length 2500]\tLoss: 0.006526\n","2850it [01:16, 22.41it/s]Train epoch: 5 [batch #2850, batch_size 16, seq length 2500]\tLoss: 0.006818\n","2874it [01:17, 22.16it/s]Train epoch: 5 [batch #2875, batch_size 16, seq length 2500]\tLoss: 0.007595\n","2898it [01:18, 21.41it/s]Train epoch: 5 [batch #2900, batch_size 16, seq length 2500]\tLoss: 0.007431\n","2925it [01:20, 20.90it/s]Train epoch: 5 [batch #2925, batch_size 16, seq length 2500]\tLoss: 0.008105\n","2949it [01:21, 19.70it/s]Train epoch: 5 [batch #2950, batch_size 16, seq length 2500]\tLoss: 0.007859\n","2974it [01:22, 18.24it/s]Train epoch: 5 [batch #2975, batch_size 16, seq length 2500]\tLoss: 0.009463\n","2983it [01:23, 35.90it/s]\n","epoch loss: 0.004703319042064362\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:03, 437.10it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0290, 0.0465, 0.0462, 0.0463, 0.8317\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2703, 0.4617, 0.3947, 0.4256, 0.9706\n","rec_at_8: 0.3113\n","prec_at_8: 0.5773\n","rec_at_15: 0.4258\n","prec_at_15: 0.4410\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/cnn_vanilla_May_02_220247\n","\n","EPOCH 6\n","0it [00:00, ?it/s]Train epoch: 6 [batch #0, batch_size 16, seq length 117]\tLoss: 0.007360\n","22it [00:00, 67.36it/s]Train epoch: 6 [batch #25, batch_size 16, seq length 337]\tLoss: 0.006746\n","48it [00:00, 77.36it/s]Train epoch: 6 [batch #50, batch_size 16, seq length 402]\tLoss: 0.003544\n","75it [00:00, 79.64it/s]Train epoch: 6 [batch #75, batch_size 16, seq length 452]\tLoss: 0.003131\n","99it [00:01, 78.52it/s]Train epoch: 6 [batch #100, batch_size 16, seq length 490]\tLoss: 0.002875\n","123it [00:01, 73.50it/s]Train epoch: 6 [batch #125, batch_size 16, seq length 520]\tLoss: 0.003239\n","147it [00:01, 70.93it/s]Train epoch: 6 [batch #150, batch_size 16, seq length 548]\tLoss: 0.003331\n","171it [00:02, 69.32it/s]Train epoch: 6 [batch #175, batch_size 16, seq length 574]\tLoss: 0.003027\n","199it [00:02, 67.61it/s]Train epoch: 6 [batch #200, batch_size 16, seq length 596]\tLoss: 0.003242\n","220it [00:03, 66.77it/s]Train epoch: 6 [batch #225, batch_size 16, seq length 618]\tLoss: 0.003305\n","248it [00:03, 65.44it/s]Train epoch: 6 [batch #250, batch_size 16, seq length 638]\tLoss: 0.003085\n","269it [00:03, 63.51it/s]Train epoch: 6 [batch #275, batch_size 16, seq length 656]\tLoss: 0.003394\n","297it [00:04, 61.74it/s]Train epoch: 6 [batch #300, batch_size 16, seq length 674]\tLoss: 0.003442\n","325it [00:04, 60.97it/s]Train epoch: 6 [batch #325, batch_size 16, seq length 693]\tLoss: 0.003192\n","346it [00:05, 60.80it/s]Train epoch: 6 [batch #350, batch_size 16, seq length 710]\tLoss: 0.003382\n","374it [00:05, 60.21it/s]Train epoch: 6 [batch #375, batch_size 16, seq length 726]\tLoss: 0.003761\n","394it [00:05, 58.48it/s]Train epoch: 6 [batch #400, batch_size 16, seq length 743]\tLoss: 0.003255\n","421it [00:06, 59.63it/s]Train epoch: 6 [batch #425, batch_size 16, seq length 758]\tLoss: 0.003350\n","445it [00:06, 57.57it/s]Train epoch: 6 [batch #450, batch_size 16, seq length 773]\tLoss: 0.003608\n","475it [00:07, 56.29it/s]Train epoch: 6 [batch #475, batch_size 16, seq length 789]\tLoss: 0.003117\n","499it [00:07, 54.98it/s]Train epoch: 6 [batch #500, batch_size 16, seq length 803]\tLoss: 0.003503\n","523it [00:08, 55.42it/s]Train epoch: 6 [batch #525, batch_size 16, seq length 818]\tLoss: 0.003419\n","547it [00:08, 55.37it/s]Train epoch: 6 [batch #550, batch_size 16, seq length 833]\tLoss: 0.003431\n","571it [00:09, 54.27it/s]Train epoch: 6 [batch #575, batch_size 16, seq length 846]\tLoss: 0.003390\n","595it [00:09, 55.18it/s]Train epoch: 6 [batch #600, batch_size 16, seq length 860]\tLoss: 0.003863\n","625it [00:10, 53.11it/s]Train epoch: 6 [batch #625, batch_size 16, seq length 874]\tLoss: 0.003693\n","649it [00:10, 53.25it/s]Train epoch: 6 [batch #650, batch_size 16, seq length 887]\tLoss: 0.003400\n","673it [00:10, 53.03it/s]Train epoch: 6 [batch #675, batch_size 16, seq length 901]\tLoss: 0.003518\n","697it [00:11, 52.86it/s]Train epoch: 6 [batch #700, batch_size 16, seq length 915]\tLoss: 0.003284\n","721it [00:11, 51.52it/s]Train epoch: 6 [batch #725, batch_size 16, seq length 929]\tLoss: 0.003562\n","745it [00:12, 52.13it/s]Train epoch: 6 [batch #750, batch_size 16, seq length 942]\tLoss: 0.003728\n","775it [00:12, 50.33it/s]Train epoch: 6 [batch #775, batch_size 16, seq length 954]\tLoss: 0.003643\n","797it [00:13, 49.79it/s]Train epoch: 6 [batch #800, batch_size 16, seq length 968]\tLoss: 0.003722\n","824it [00:13, 48.86it/s]Train epoch: 6 [batch #825, batch_size 16, seq length 981]\tLoss: 0.003677\n","850it [00:14, 49.46it/s]Train epoch: 6 [batch #850, batch_size 16, seq length 994]\tLoss: 0.003732\n","871it [00:14, 48.42it/s]Train epoch: 6 [batch #875, batch_size 16, seq length 1006]\tLoss: 0.003575\n","896it [00:15, 47.80it/s]Train epoch: 6 [batch #900, batch_size 16, seq length 1020]\tLoss: 0.003963\n","921it [00:15, 45.53it/s]Train epoch: 6 [batch #925, batch_size 16, seq length 1033]\tLoss: 0.003612\n","946it [00:16, 45.91it/s]Train epoch: 6 [batch #950, batch_size 16, seq length 1046]\tLoss: 0.003879\n","971it [00:17, 44.34it/s]Train epoch: 6 [batch #975, batch_size 16, seq length 1059]\tLoss: 0.003749\n","996it [00:17, 45.36it/s]Train epoch: 6 [batch #1000, batch_size 16, seq length 1073]\tLoss: 0.003813\n","1021it [00:18, 44.92it/s]Train epoch: 6 [batch #1025, batch_size 16, seq length 1085]\tLoss: 0.003924\n","1046it [00:18, 43.07it/s]Train epoch: 6 [batch #1050, batch_size 16, seq length 1099]\tLoss: 0.004019\n","1071it [00:19, 43.83it/s]Train epoch: 6 [batch #1075, batch_size 16, seq length 1113]\tLoss: 0.003962\n","1096it [00:19, 43.64it/s]Train epoch: 6 [batch #1100, batch_size 16, seq length 1126]\tLoss: 0.003811\n","1121it [00:20, 42.84it/s]Train epoch: 6 [batch #1125, batch_size 16, seq length 1140]\tLoss: 0.004085\n","1146it [00:21, 42.62it/s]Train epoch: 6 [batch #1150, batch_size 16, seq length 1154]\tLoss: 0.003978\n","1171it [00:21, 42.89it/s]Train epoch: 6 [batch #1175, batch_size 16, seq length 1167]\tLoss: 0.004088\n","1196it [00:22, 42.62it/s]Train epoch: 6 [batch #1200, batch_size 16, seq length 1180]\tLoss: 0.003916\n","1221it [00:22, 41.59it/s]Train epoch: 6 [batch #1225, batch_size 16, seq length 1195]\tLoss: 0.003960\n","1246it [00:23, 40.95it/s]Train epoch: 6 [batch #1250, batch_size 16, seq length 1208]\tLoss: 0.003861\n","1271it [00:24, 40.52it/s]Train epoch: 6 [batch #1275, batch_size 16, seq length 1220]\tLoss: 0.003903\n","1296it [00:24, 40.86it/s]Train epoch: 6 [batch #1300, batch_size 16, seq length 1234]\tLoss: 0.004134\n","1321it [00:25, 39.95it/s]Train epoch: 6 [batch #1325, batch_size 16, seq length 1248]\tLoss: 0.003888\n","1350it [00:26, 39.42it/s]Train epoch: 6 [batch #1350, batch_size 16, seq length 1262]\tLoss: 0.004230\n","1375it [00:26, 38.39it/s]Train epoch: 6 [batch #1375, batch_size 16, seq length 1276]\tLoss: 0.004522\n","1399it [00:27, 38.24it/s]Train epoch: 6 [batch #1400, batch_size 16, seq length 1290]\tLoss: 0.004446\n","1423it [00:27, 38.44it/s]Train epoch: 6 [batch #1425, batch_size 16, seq length 1304]\tLoss: 0.004474\n","1447it [00:28, 37.81it/s]Train epoch: 6 [batch #1450, batch_size 16, seq length 1319]\tLoss: 0.004207\n","1475it [00:29, 38.18it/s]Train epoch: 6 [batch #1475, batch_size 16, seq length 1332]\tLoss: 0.004460\n","1499it [00:30, 37.62it/s]Train epoch: 6 [batch #1500, batch_size 16, seq length 1346]\tLoss: 0.004351\n","1523it [00:30, 37.32it/s]Train epoch: 6 [batch #1525, batch_size 16, seq length 1362]\tLoss: 0.003817\n","1547it [00:31, 37.19it/s]Train epoch: 6 [batch #1550, batch_size 16, seq length 1377]\tLoss: 0.004450\n","1575it [00:32, 37.01it/s]Train epoch: 6 [batch #1575, batch_size 16, seq length 1392]\tLoss: 0.003958\n","1599it [00:32, 36.83it/s]Train epoch: 6 [batch #1600, batch_size 16, seq length 1406]\tLoss: 0.004101\n","1623it [00:33, 35.85it/s]Train epoch: 6 [batch #1625, batch_size 16, seq length 1422]\tLoss: 0.004658\n","1647it [00:34, 36.21it/s]Train epoch: 6 [batch #1650, batch_size 16, seq length 1437]\tLoss: 0.004426\n","1675it [00:34, 35.69it/s]Train epoch: 6 [batch #1675, batch_size 16, seq length 1454]\tLoss: 0.004538\n","1699it [00:35, 35.22it/s]Train epoch: 6 [batch #1700, batch_size 16, seq length 1469]\tLoss: 0.004763\n","1723it [00:36, 35.81it/s]Train epoch: 6 [batch #1725, batch_size 16, seq length 1484]\tLoss: 0.004299\n","1747it [00:36, 35.33it/s]Train epoch: 6 [batch #1750, batch_size 16, seq length 1501]\tLoss: 0.004188\n","1775it [00:37, 35.23it/s]Train epoch: 6 [batch #1775, batch_size 16, seq length 1518]\tLoss: 0.004624\n","1799it [00:38, 34.12it/s]Train epoch: 6 [batch #1800, batch_size 16, seq length 1533]\tLoss: 0.004657\n","1823it [00:39, 33.86it/s]Train epoch: 6 [batch #1825, batch_size 16, seq length 1548]\tLoss: 0.004765\n","1847it [00:39, 33.79it/s]Train epoch: 6 [batch #1850, batch_size 16, seq length 1566]\tLoss: 0.004423\n","1875it [00:40, 33.28it/s]Train epoch: 6 [batch #1875, batch_size 16, seq length 1582]\tLoss: 0.004618\n","1899it [00:41, 33.12it/s]Train epoch: 6 [batch #1900, batch_size 16, seq length 1599]\tLoss: 0.004904\n","1923it [00:42, 33.46it/s]Train epoch: 6 [batch #1925, batch_size 16, seq length 1616]\tLoss: 0.004698\n","1947it [00:42, 32.69it/s]Train epoch: 6 [batch #1950, batch_size 16, seq length 1633]\tLoss: 0.004617\n","1975it [00:43, 32.56it/s]Train epoch: 6 [batch #1975, batch_size 16, seq length 1651]\tLoss: 0.004678\n","1999it [00:44, 32.21it/s]Train epoch: 6 [batch #2000, batch_size 16, seq length 1670]\tLoss: 0.004593\n","2023it [00:45, 32.17it/s]Train epoch: 6 [batch #2025, batch_size 16, seq length 1688]\tLoss: 0.004551\n","2047it [00:45, 32.47it/s]Train epoch: 6 [batch #2050, batch_size 16, seq length 1707]\tLoss: 0.004688\n","2075it [00:46, 31.70it/s]Train epoch: 6 [batch #2075, batch_size 16, seq length 1728]\tLoss: 0.004862\n","2099it [00:47, 30.89it/s]Train epoch: 6 [batch #2100, batch_size 16, seq length 1748]\tLoss: 0.004533\n","2123it [00:48, 30.84it/s]Train epoch: 6 [batch #2125, batch_size 16, seq length 1768]\tLoss: 0.005161\n","2147it [00:49, 30.70it/s]Train epoch: 6 [batch #2150, batch_size 16, seq length 1787]\tLoss: 0.004509\n","2175it [00:50, 29.91it/s]Train epoch: 6 [batch #2175, batch_size 16, seq length 1809]\tLoss: 0.005213\n","2200it [00:50, 29.95it/s]Train epoch: 6 [batch #2200, batch_size 16, seq length 1833]\tLoss: 0.004803\n","2222it [00:51, 29.66it/s]Train epoch: 6 [batch #2225, batch_size 16, seq length 1855]\tLoss: 0.004808\n","2249it [00:52, 29.26it/s]Train epoch: 6 [batch #2250, batch_size 16, seq length 1878]\tLoss: 0.005281\n","2274it [00:53, 28.99it/s]Train epoch: 6 [batch #2275, batch_size 16, seq length 1901]\tLoss: 0.005045\n","2298it [00:54, 28.07it/s]Train epoch: 6 [batch #2300, batch_size 16, seq length 1927]\tLoss: 0.004955\n","2325it [00:55, 28.57it/s]Train epoch: 6 [batch #2325, batch_size 16, seq length 1953]\tLoss: 0.005320\n","2349it [00:56, 27.72it/s]Train epoch: 6 [batch #2350, batch_size 16, seq length 1980]\tLoss: 0.005021\n","2373it [00:56, 28.10it/s]Train epoch: 6 [batch #2375, batch_size 16, seq length 2006]\tLoss: 0.005283\n","2400it [00:57, 27.33it/s]Train epoch: 6 [batch #2400, batch_size 16, seq length 2034]\tLoss: 0.005258\n","2424it [00:58, 26.94it/s]Train epoch: 6 [batch #2425, batch_size 16, seq length 2064]\tLoss: 0.005108\n","2448it [00:59, 26.92it/s]Train epoch: 6 [batch #2450, batch_size 16, seq length 2094]\tLoss: 0.005406\n","2475it [01:00, 26.52it/s]Train epoch: 6 [batch #2475, batch_size 16, seq length 2126]\tLoss: 0.005545\n","2499it [01:01, 26.47it/s]Train epoch: 6 [batch #2500, batch_size 16, seq length 2160]\tLoss: 0.005427\n","2523it [01:02, 26.46it/s]Train epoch: 6 [batch #2525, batch_size 16, seq length 2192]\tLoss: 0.005613\n","2550it [01:03, 26.47it/s]Train epoch: 6 [batch #2550, batch_size 16, seq length 2228]\tLoss: 0.005539\n","2574it [01:04, 25.62it/s]Train epoch: 6 [batch #2575, batch_size 16, seq length 2266]\tLoss: 0.005905\n","2598it [01:05, 24.89it/s]Train epoch: 6 [batch #2600, batch_size 16, seq length 2307]\tLoss: 0.005948\n","2625it [01:06, 24.71it/s]Train epoch: 6 [batch #2625, batch_size 16, seq length 2350]\tLoss: 0.005712\n","2649it [01:07, 24.52it/s]Train epoch: 6 [batch #2650, batch_size 16, seq length 2392]\tLoss: 0.005597\n","2673it [01:08, 24.32it/s]Train epoch: 6 [batch #2675, batch_size 16, seq length 2439]\tLoss: 0.005923\n","2700it [01:09, 23.54it/s]Train epoch: 6 [batch #2700, batch_size 16, seq length 2491]\tLoss: 0.006019\n","2724it [01:10, 23.46it/s]Train epoch: 6 [batch #2725, batch_size 16, seq length 2500]\tLoss: 0.006107\n","2748it [01:11, 23.05it/s]Train epoch: 6 [batch #2750, batch_size 16, seq length 2500]\tLoss: 0.006237\n","2775it [01:12, 22.51it/s]Train epoch: 6 [batch #2775, batch_size 16, seq length 2500]\tLoss: 0.006355\n","2799it [01:13, 22.11it/s]Train epoch: 6 [batch #2800, batch_size 16, seq length 2500]\tLoss: 0.006398\n","2823it [01:15, 22.31it/s]Train epoch: 6 [batch #2825, batch_size 16, seq length 2500]\tLoss: 0.006282\n","2850it [01:16, 22.16it/s]Train epoch: 6 [batch #2850, batch_size 16, seq length 2500]\tLoss: 0.006586\n","2874it [01:17, 22.05it/s]Train epoch: 6 [batch #2875, batch_size 16, seq length 2500]\tLoss: 0.007333\n","2898it [01:18, 21.69it/s]Train epoch: 6 [batch #2900, batch_size 16, seq length 2500]\tLoss: 0.007266\n","2925it [01:19, 20.51it/s]Train epoch: 6 [batch #2925, batch_size 16, seq length 2500]\tLoss: 0.007792\n","2949it [01:20, 20.00it/s]Train epoch: 6 [batch #2950, batch_size 16, seq length 2500]\tLoss: 0.007628\n","2975it [01:22, 17.66it/s]Train epoch: 6 [batch #2975, batch_size 16, seq length 2500]\tLoss: 0.009160\n","2983it [01:22, 36.02it/s]\n","epoch loss: 0.004546238585121423\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:03, 435.03it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0273, 0.0459, 0.0412, 0.0434, 0.8351\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2798, 0.5145, 0.3801, 0.4372, 0.9724\n","rec_at_8: 0.3202\n","prec_at_8: 0.5940\n","rec_at_15: 0.4406\n","prec_at_15: 0.4554\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/cnn_vanilla_May_02_220247\n","\n","EPOCH 7\n","0it [00:00, ?it/s]Train epoch: 7 [batch #0, batch_size 16, seq length 117]\tLoss: 0.006217\n","21it [00:00, 65.87it/s]Train epoch: 7 [batch #25, batch_size 16, seq length 337]\tLoss: 0.004582\n","48it [00:00, 78.68it/s]Train epoch: 7 [batch #50, batch_size 16, seq length 402]\tLoss: 0.003310\n","75it [00:00, 80.29it/s]Train epoch: 7 [batch #75, batch_size 16, seq length 452]\tLoss: 0.002876\n","100it [00:01, 78.99it/s]Train epoch: 7 [batch #100, batch_size 16, seq length 490]\tLoss: 0.002779\n","124it [00:01, 76.08it/s]Train epoch: 7 [batch #125, batch_size 16, seq length 520]\tLoss: 0.003110\n","148it [00:01, 71.81it/s]Train epoch: 7 [batch #150, batch_size 16, seq length 548]\tLoss: 0.003209\n","172it [00:02, 68.66it/s]Train epoch: 7 [batch #175, batch_size 16, seq length 574]\tLoss: 0.002891\n","200it [00:02, 66.57it/s]Train epoch: 7 [batch #200, batch_size 16, seq length 596]\tLoss: 0.003088\n","221it [00:03, 66.73it/s]Train epoch: 7 [batch #225, batch_size 16, seq length 618]\tLoss: 0.003172\n","249it [00:03, 65.43it/s]Train epoch: 7 [batch #250, batch_size 16, seq length 638]\tLoss: 0.002920\n","270it [00:03, 63.80it/s]Train epoch: 7 [batch #275, batch_size 16, seq length 656]\tLoss: 0.003221\n","298it [00:04, 63.64it/s]Train epoch: 7 [batch #300, batch_size 16, seq length 674]\tLoss: 0.003297\n","319it [00:04, 61.67it/s]Train epoch: 7 [batch #325, batch_size 16, seq length 693]\tLoss: 0.003050\n","347it [00:05, 61.73it/s]Train epoch: 7 [batch #350, batch_size 16, seq length 710]\tLoss: 0.003253\n","375it [00:05, 59.65it/s]Train epoch: 7 [batch #375, batch_size 16, seq length 726]\tLoss: 0.003612\n","395it [00:05, 58.30it/s]Train epoch: 7 [batch #400, batch_size 16, seq length 743]\tLoss: 0.003109\n","421it [00:06, 59.05it/s]Train epoch: 7 [batch #425, batch_size 16, seq length 758]\tLoss: 0.003216\n","446it [00:06, 58.33it/s]Train epoch: 7 [batch #450, batch_size 16, seq length 773]\tLoss: 0.003468\n","470it [00:07, 57.27it/s]Train epoch: 7 [batch #475, batch_size 16, seq length 789]\tLoss: 0.003084\n","500it [00:07, 55.60it/s]Train epoch: 7 [batch #500, batch_size 16, seq length 803]\tLoss: 0.003367\n","524it [00:08, 56.00it/s]Train epoch: 7 [batch #525, batch_size 16, seq length 818]\tLoss: 0.003304\n","548it [00:08, 54.23it/s]Train epoch: 7 [batch #550, batch_size 16, seq length 833]\tLoss: 0.003262\n","572it [00:09, 54.99it/s]Train epoch: 7 [batch #575, batch_size 16, seq length 846]\tLoss: 0.003225\n","596it [00:09, 54.42it/s]Train epoch: 7 [batch #600, batch_size 16, seq length 860]\tLoss: 0.003638\n","620it [00:09, 54.04it/s]Train epoch: 7 [batch #625, batch_size 16, seq length 874]\tLoss: 0.003582\n","650it [00:10, 54.10it/s]Train epoch: 7 [batch #650, batch_size 16, seq length 887]\tLoss: 0.003291\n","674it [00:10, 53.17it/s]Train epoch: 7 [batch #675, batch_size 16, seq length 901]\tLoss: 0.003360\n","698it [00:11, 52.70it/s]Train epoch: 7 [batch #700, batch_size 16, seq length 915]\tLoss: 0.003183\n","722it [00:11, 52.03it/s]Train epoch: 7 [batch #725, batch_size 16, seq length 929]\tLoss: 0.003349\n","746it [00:12, 52.26it/s]Train epoch: 7 [batch #750, batch_size 16, seq length 942]\tLoss: 0.003633\n","775it [00:12, 49.25it/s]Train epoch: 7 [batch #775, batch_size 16, seq length 954]\tLoss: 0.003527\n","797it [00:13, 48.88it/s]Train epoch: 7 [batch #800, batch_size 16, seq length 968]\tLoss: 0.003597\n","823it [00:13, 47.88it/s]Train epoch: 7 [batch #825, batch_size 16, seq length 981]\tLoss: 0.003545\n","849it [00:14, 48.37it/s]Train epoch: 7 [batch #850, batch_size 16, seq length 994]\tLoss: 0.003606\n","875it [00:14, 48.01it/s]Train epoch: 7 [batch #875, batch_size 16, seq length 1006]\tLoss: 0.003486\n","900it [00:15, 46.23it/s]Train epoch: 7 [batch #900, batch_size 16, seq length 1020]\tLoss: 0.003760\n","925it [00:16, 45.17it/s]Train epoch: 7 [batch #925, batch_size 16, seq length 1033]\tLoss: 0.003492\n","950it [00:16, 45.41it/s]Train epoch: 7 [batch #950, batch_size 16, seq length 1046]\tLoss: 0.003790\n","975it [00:17, 44.67it/s]Train epoch: 7 [batch #975, batch_size 16, seq length 1059]\tLoss: 0.003640\n","1000it [00:17, 44.21it/s]Train epoch: 7 [batch #1000, batch_size 16, seq length 1073]\tLoss: 0.003672\n","1025it [00:18, 44.45it/s]Train epoch: 7 [batch #1025, batch_size 16, seq length 1085]\tLoss: 0.003830\n","1050it [00:18, 44.02it/s]Train epoch: 7 [batch #1050, batch_size 16, seq length 1099]\tLoss: 0.003862\n","1075it [00:19, 44.48it/s]Train epoch: 7 [batch #1075, batch_size 16, seq length 1113]\tLoss: 0.003885\n","1100it [00:20, 43.52it/s]Train epoch: 7 [batch #1100, batch_size 16, seq length 1126]\tLoss: 0.003669\n","1125it [00:20, 43.20it/s]Train epoch: 7 [batch #1125, batch_size 16, seq length 1140]\tLoss: 0.003932\n","1150it [00:21, 42.07it/s]Train epoch: 7 [batch #1150, batch_size 16, seq length 1154]\tLoss: 0.003831\n","1175it [00:21, 42.33it/s]Train epoch: 7 [batch #1175, batch_size 16, seq length 1167]\tLoss: 0.003955\n","1200it [00:22, 41.52it/s]Train epoch: 7 [batch #1200, batch_size 16, seq length 1180]\tLoss: 0.003777\n","1225it [00:22, 40.80it/s]Train epoch: 7 [batch #1225, batch_size 16, seq length 1195]\tLoss: 0.003855\n","1250it [00:23, 41.23it/s]Train epoch: 7 [batch #1250, batch_size 16, seq length 1208]\tLoss: 0.003710\n","1275it [00:24, 40.45it/s]Train epoch: 7 [batch #1275, batch_size 16, seq length 1220]\tLoss: 0.003786\n","1300it [00:24, 40.30it/s]Train epoch: 7 [batch #1300, batch_size 16, seq length 1234]\tLoss: 0.003922\n","1325it [00:25, 40.25it/s]Train epoch: 7 [batch #1325, batch_size 16, seq length 1248]\tLoss: 0.003802\n","1349it [00:26, 40.07it/s]Train epoch: 7 [batch #1350, batch_size 16, seq length 1262]\tLoss: 0.004082\n","1372it [00:26, 39.32it/s]Train epoch: 7 [batch #1375, batch_size 16, seq length 1276]\tLoss: 0.004336\n","1400it [00:27, 38.79it/s]Train epoch: 7 [batch #1400, batch_size 16, seq length 1290]\tLoss: 0.004293\n","1424it [00:27, 37.98it/s]Train epoch: 7 [batch #1425, batch_size 16, seq length 1304]\tLoss: 0.004402\n","1448it [00:28, 37.87it/s]Train epoch: 7 [batch #1450, batch_size 16, seq length 1319]\tLoss: 0.004063\n","1472it [00:29, 37.45it/s]Train epoch: 7 [batch #1475, batch_size 16, seq length 1332]\tLoss: 0.004291\n","1500it [00:29, 36.92it/s]Train epoch: 7 [batch #1500, batch_size 16, seq length 1346]\tLoss: 0.004242\n","1524it [00:30, 37.53it/s]Train epoch: 7 [batch #1525, batch_size 16, seq length 1362]\tLoss: 0.003738\n","1548it [00:31, 36.75it/s]Train epoch: 7 [batch #1550, batch_size 16, seq length 1377]\tLoss: 0.004415\n","1572it [00:31, 35.86it/s]Train epoch: 7 [batch #1575, batch_size 16, seq length 1392]\tLoss: 0.003768\n","1600it [00:32, 36.01it/s]Train epoch: 7 [batch #1600, batch_size 16, seq length 1406]\tLoss: 0.003982\n","1624it [00:33, 35.86it/s]Train epoch: 7 [batch #1625, batch_size 16, seq length 1422]\tLoss: 0.004444\n","1648it [00:34, 35.52it/s]Train epoch: 7 [batch #1650, batch_size 16, seq length 1437]\tLoss: 0.004248\n","1672it [00:34, 35.58it/s]Train epoch: 7 [batch #1675, batch_size 16, seq length 1454]\tLoss: 0.004338\n","1700it [00:35, 35.91it/s]Train epoch: 7 [batch #1700, batch_size 16, seq length 1469]\tLoss: 0.004617\n","1724it [00:36, 35.42it/s]Train epoch: 7 [batch #1725, batch_size 16, seq length 1484]\tLoss: 0.004160\n","1748it [00:36, 35.61it/s]Train epoch: 7 [batch #1750, batch_size 16, seq length 1501]\tLoss: 0.003995\n","1772it [00:37, 35.21it/s]Train epoch: 7 [batch #1775, batch_size 16, seq length 1518]\tLoss: 0.004554\n","1800it [00:38, 33.59it/s]Train epoch: 7 [batch #1800, batch_size 16, seq length 1533]\tLoss: 0.004546\n","1824it [00:39, 32.88it/s]Train epoch: 7 [batch #1825, batch_size 16, seq length 1548]\tLoss: 0.004554\n","1848it [00:39, 33.32it/s]Train epoch: 7 [batch #1850, batch_size 16, seq length 1566]\tLoss: 0.004216\n","1872it [00:40, 33.11it/s]Train epoch: 7 [batch #1875, batch_size 16, seq length 1582]\tLoss: 0.004531\n","1900it [00:41, 32.78it/s]Train epoch: 7 [batch #1900, batch_size 16, seq length 1599]\tLoss: 0.004726\n","1924it [00:42, 32.59it/s]Train epoch: 7 [batch #1925, batch_size 16, seq length 1616]\tLoss: 0.004518\n","1948it [00:42, 32.10it/s]Train epoch: 7 [batch #1950, batch_size 16, seq length 1633]\tLoss: 0.004521\n","1972it [00:43, 32.27it/s]Train epoch: 7 [batch #1975, batch_size 16, seq length 1651]\tLoss: 0.004455\n","2000it [00:44, 31.87it/s]Train epoch: 7 [batch #2000, batch_size 16, seq length 1670]\tLoss: 0.004444\n","2024it [00:45, 32.04it/s]Train epoch: 7 [batch #2025, batch_size 16, seq length 1688]\tLoss: 0.004452\n","2048it [00:46, 31.93it/s]Train epoch: 7 [batch #2050, batch_size 16, seq length 1707]\tLoss: 0.004569\n","2072it [00:46, 30.59it/s]Train epoch: 7 [batch #2075, batch_size 16, seq length 1728]\tLoss: 0.004778\n","2098it [00:47, 28.97it/s]Train epoch: 7 [batch #2100, batch_size 16, seq length 1748]\tLoss: 0.004458\n","2125it [00:48, 30.43it/s]Train epoch: 7 [batch #2125, batch_size 16, seq length 1768]\tLoss: 0.004944\n","2149it [00:49, 30.26it/s]Train epoch: 7 [batch #2150, batch_size 16, seq length 1787]\tLoss: 0.004431\n","2173it [00:50, 30.25it/s]Train epoch: 7 [batch #2175, batch_size 16, seq length 1809]\tLoss: 0.005066\n","2198it [00:51, 29.61it/s]Train epoch: 7 [batch #2200, batch_size 16, seq length 1833]\tLoss: 0.004638\n","2225it [00:51, 29.62it/s]Train epoch: 7 [batch #2225, batch_size 16, seq length 1855]\tLoss: 0.004669\n","2250it [00:52, 29.17it/s]Train epoch: 7 [batch #2250, batch_size 16, seq length 1878]\tLoss: 0.005069\n","2274it [00:53, 28.88it/s]Train epoch: 7 [batch #2275, batch_size 16, seq length 1901]\tLoss: 0.004918\n","2298it [00:54, 28.39it/s]Train epoch: 7 [batch #2300, batch_size 16, seq length 1927]\tLoss: 0.004848\n","2325it [00:55, 28.82it/s]Train epoch: 7 [batch #2325, batch_size 16, seq length 1953]\tLoss: 0.005214\n","2349it [00:56, 28.79it/s]Train epoch: 7 [batch #2350, batch_size 16, seq length 1980]\tLoss: 0.004930\n","2373it [00:57, 27.83it/s]Train epoch: 7 [batch #2375, batch_size 16, seq length 2006]\tLoss: 0.005136\n","2400it [00:58, 28.08it/s]Train epoch: 7 [batch #2400, batch_size 16, seq length 2034]\tLoss: 0.005119\n","2424it [00:58, 26.60it/s]Train epoch: 7 [batch #2425, batch_size 16, seq length 2064]\tLoss: 0.004995\n","2448it [00:59, 26.58it/s]Train epoch: 7 [batch #2450, batch_size 16, seq length 2094]\tLoss: 0.005243\n","2475it [01:00, 25.89it/s]Train epoch: 7 [batch #2475, batch_size 16, seq length 2126]\tLoss: 0.005477\n","2499it [01:01, 26.10it/s]Train epoch: 7 [batch #2500, batch_size 16, seq length 2160]\tLoss: 0.005347\n","2523it [01:02, 26.30it/s]Train epoch: 7 [batch #2525, batch_size 16, seq length 2192]\tLoss: 0.005545\n","2550it [01:03, 25.85it/s]Train epoch: 7 [batch #2550, batch_size 16, seq length 2228]\tLoss: 0.005429\n","2574it [01:04, 25.59it/s]Train epoch: 7 [batch #2575, batch_size 16, seq length 2266]\tLoss: 0.005788\n","2598it [01:05, 24.41it/s]Train epoch: 7 [batch #2600, batch_size 16, seq length 2307]\tLoss: 0.005757\n","2625it [01:06, 24.13it/s]Train epoch: 7 [batch #2625, batch_size 16, seq length 2350]\tLoss: 0.005579\n","2649it [01:07, 24.41it/s]Train epoch: 7 [batch #2650, batch_size 16, seq length 2392]\tLoss: 0.005552\n","2673it [01:08, 23.77it/s]Train epoch: 7 [batch #2675, batch_size 16, seq length 2439]\tLoss: 0.005843\n","2700it [01:09, 23.02it/s]Train epoch: 7 [batch #2700, batch_size 16, seq length 2491]\tLoss: 0.005952\n","2724it [01:10, 22.91it/s]Train epoch: 7 [batch #2725, batch_size 16, seq length 2500]\tLoss: 0.006025\n","2748it [01:12, 22.79it/s]Train epoch: 7 [batch #2750, batch_size 16, seq length 2500]\tLoss: 0.006165\n","2775it [01:13, 22.90it/s]Train epoch: 7 [batch #2775, batch_size 16, seq length 2500]\tLoss: 0.006336\n","2799it [01:14, 22.69it/s]Train epoch: 7 [batch #2800, batch_size 16, seq length 2500]\tLoss: 0.006259\n","2823it [01:15, 22.53it/s]Train epoch: 7 [batch #2825, batch_size 16, seq length 2500]\tLoss: 0.006235\n","2850it [01:16, 22.34it/s]Train epoch: 7 [batch #2850, batch_size 16, seq length 2500]\tLoss: 0.006430\n","2874it [01:17, 21.71it/s]Train epoch: 7 [batch #2875, batch_size 16, seq length 2500]\tLoss: 0.007130\n","2898it [01:18, 21.25it/s]Train epoch: 7 [batch #2900, batch_size 16, seq length 2500]\tLoss: 0.007108\n","2925it [01:20, 20.62it/s]Train epoch: 7 [batch #2925, batch_size 16, seq length 2500]\tLoss: 0.007586\n","2949it [01:21, 19.75it/s]Train epoch: 7 [batch #2950, batch_size 16, seq length 2500]\tLoss: 0.007505\n","2975it [01:22, 18.09it/s]Train epoch: 7 [batch #2975, batch_size 16, seq length 2500]\tLoss: 0.009001\n","2983it [01:23, 35.91it/s]\n","epoch loss: 0.004397805363567065\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:03, 440.87it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0287, 0.0467, 0.0447, 0.0457, 0.8325\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2771, 0.4851, 0.3926, 0.4340, 0.9722\n","rec_at_8: 0.3159\n","prec_at_8: 0.5825\n","rec_at_15: 0.4327\n","prec_at_15: 0.4464\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/cnn_vanilla_May_02_220247\n","\n","EPOCH 8\n","0it [00:00, ?it/s]Train epoch: 8 [batch #0, batch_size 16, seq length 117]\tLoss: 0.006678\n","22it [00:00, 72.43it/s]Train epoch: 8 [batch #25, batch_size 16, seq length 337]\tLoss: 0.003997\n","48it [00:00, 80.49it/s]Train epoch: 8 [batch #50, batch_size 16, seq length 402]\tLoss: 0.003168\n","75it [00:00, 79.87it/s]Train epoch: 8 [batch #75, batch_size 16, seq length 452]\tLoss: 0.002853\n","99it [00:01, 78.54it/s]Train epoch: 8 [batch #100, batch_size 16, seq length 490]\tLoss: 0.002656\n","123it [00:01, 75.70it/s]Train epoch: 8 [batch #125, batch_size 16, seq length 520]\tLoss: 0.002975\n","147it [00:01, 70.94it/s]Train epoch: 8 [batch #150, batch_size 16, seq length 548]\tLoss: 0.003103\n","170it [00:02, 69.41it/s]Train epoch: 8 [batch #175, batch_size 16, seq length 574]\tLoss: 0.002849\n","198it [00:02, 67.19it/s]Train epoch: 8 [batch #200, batch_size 16, seq length 596]\tLoss: 0.003024\n","219it [00:03, 65.99it/s]Train epoch: 8 [batch #225, batch_size 16, seq length 618]\tLoss: 0.003102\n","247it [00:03, 65.87it/s]Train epoch: 8 [batch #250, batch_size 16, seq length 638]\tLoss: 0.002856\n","275it [00:03, 63.09it/s]Train epoch: 8 [batch #275, batch_size 16, seq length 656]\tLoss: 0.003162\n","296it [00:04, 62.96it/s]Train epoch: 8 [batch #300, batch_size 16, seq length 674]\tLoss: 0.003220\n","324it [00:04, 61.87it/s]Train epoch: 8 [batch #325, batch_size 16, seq length 693]\tLoss: 0.003018\n","345it [00:05, 60.43it/s]Train epoch: 8 [batch #350, batch_size 16, seq length 710]\tLoss: 0.003154\n","373it [00:05, 60.02it/s]Train epoch: 8 [batch #375, batch_size 16, seq length 726]\tLoss: 0.003543\n","399it [00:05, 57.81it/s]Train epoch: 8 [batch #400, batch_size 16, seq length 743]\tLoss: 0.003018\n","424it [00:06, 59.23it/s]Train epoch: 8 [batch #425, batch_size 16, seq length 758]\tLoss: 0.003130\n","448it [00:06, 58.84it/s]Train epoch: 8 [batch #450, batch_size 16, seq length 773]\tLoss: 0.003332\n","472it [00:07, 56.41it/s]Train epoch: 8 [batch #475, batch_size 16, seq length 789]\tLoss: 0.002957\n","496it [00:07, 56.64it/s]Train epoch: 8 [batch #500, batch_size 16, seq length 803]\tLoss: 0.003302\n","520it [00:08, 55.26it/s]Train epoch: 8 [batch #525, batch_size 16, seq length 818]\tLoss: 0.003196\n","550it [00:08, 53.31it/s]Train epoch: 8 [batch #550, batch_size 16, seq length 833]\tLoss: 0.003146\n","574it [00:09, 54.40it/s]Train epoch: 8 [batch #575, batch_size 16, seq length 846]\tLoss: 0.003136\n","598it [00:09, 53.85it/s]Train epoch: 8 [batch #600, batch_size 16, seq length 860]\tLoss: 0.003514\n","622it [00:09, 52.04it/s]Train epoch: 8 [batch #625, batch_size 16, seq length 874]\tLoss: 0.003441\n","646it [00:10, 54.08it/s]Train epoch: 8 [batch #650, batch_size 16, seq length 887]\tLoss: 0.003177\n","670it [00:10, 52.40it/s]Train epoch: 8 [batch #675, batch_size 16, seq length 901]\tLoss: 0.003296\n","700it [00:11, 51.92it/s]Train epoch: 8 [batch #700, batch_size 16, seq length 915]\tLoss: 0.003077\n","724it [00:11, 52.21it/s]Train epoch: 8 [batch #725, batch_size 16, seq length 929]\tLoss: 0.003288\n","748it [00:12, 52.02it/s]Train epoch: 8 [batch #750, batch_size 16, seq length 942]\tLoss: 0.003600\n","772it [00:12, 50.06it/s]Train epoch: 8 [batch #775, batch_size 16, seq length 954]\tLoss: 0.003396\n","799it [00:13, 49.07it/s]Train epoch: 8 [batch #800, batch_size 16, seq length 968]\tLoss: 0.003494\n","821it [00:13, 49.43it/s]Train epoch: 8 [batch #825, batch_size 16, seq length 981]\tLoss: 0.003460\n","848it [00:14, 48.04it/s]Train epoch: 8 [batch #850, batch_size 16, seq length 994]\tLoss: 0.003575\n","873it [00:14, 48.27it/s]Train epoch: 8 [batch #875, batch_size 16, seq length 1006]\tLoss: 0.003373\n","899it [00:15, 47.86it/s]Train epoch: 8 [batch #900, batch_size 16, seq length 1020]\tLoss: 0.003668\n","924it [00:16, 45.03it/s]Train epoch: 8 [batch #925, batch_size 16, seq length 1033]\tLoss: 0.003364\n","949it [00:16, 46.18it/s]Train epoch: 8 [batch #950, batch_size 16, seq length 1046]\tLoss: 0.003638\n","974it [00:17, 45.01it/s]Train epoch: 8 [batch #975, batch_size 16, seq length 1059]\tLoss: 0.003533\n","999it [00:17, 45.33it/s]Train epoch: 8 [batch #1000, batch_size 16, seq length 1073]\tLoss: 0.003551\n","1024it [00:18, 44.17it/s]Train epoch: 8 [batch #1025, batch_size 16, seq length 1085]\tLoss: 0.003776\n","1049it [00:18, 44.26it/s]Train epoch: 8 [batch #1050, batch_size 16, seq length 1099]\tLoss: 0.003725\n","1074it [00:19, 43.80it/s]Train epoch: 8 [batch #1075, batch_size 16, seq length 1113]\tLoss: 0.003681\n","1099it [00:19, 43.82it/s]Train epoch: 8 [batch #1100, batch_size 16, seq length 1126]\tLoss: 0.003649\n","1124it [00:20, 42.42it/s]Train epoch: 8 [batch #1125, batch_size 16, seq length 1140]\tLoss: 0.003866\n","1149it [00:21, 42.11it/s]Train epoch: 8 [batch #1150, batch_size 16, seq length 1154]\tLoss: 0.003771\n","1174it [00:21, 42.91it/s]Train epoch: 8 [batch #1175, batch_size 16, seq length 1167]\tLoss: 0.003864\n","1199it [00:22, 42.40it/s]Train epoch: 8 [batch #1200, batch_size 16, seq length 1180]\tLoss: 0.003645\n","1224it [00:22, 41.80it/s]Train epoch: 8 [batch #1225, batch_size 16, seq length 1195]\tLoss: 0.003784\n","1249it [00:23, 41.32it/s]Train epoch: 8 [batch #1250, batch_size 16, seq length 1208]\tLoss: 0.003638\n","1274it [00:24, 41.05it/s]Train epoch: 8 [batch #1275, batch_size 16, seq length 1220]\tLoss: 0.003655\n","1299it [00:24, 40.43it/s]Train epoch: 8 [batch #1300, batch_size 16, seq length 1234]\tLoss: 0.003897\n","1323it [00:25, 39.28it/s]Train epoch: 8 [batch #1325, batch_size 16, seq length 1248]\tLoss: 0.003724\n","1346it [00:25, 40.15it/s]Train epoch: 8 [batch #1350, batch_size 16, seq length 1262]\tLoss: 0.004000\n","1372it [00:26, 39.08it/s]Train epoch: 8 [batch #1375, batch_size 16, seq length 1276]\tLoss: 0.004312\n","1400it [00:27, 38.20it/s]Train epoch: 8 [batch #1400, batch_size 16, seq length 1290]\tLoss: 0.004206\n","1424it [00:27, 38.47it/s]Train epoch: 8 [batch #1425, batch_size 16, seq length 1304]\tLoss: 0.004277\n","1448it [00:28, 37.92it/s]Train epoch: 8 [batch #1450, batch_size 16, seq length 1319]\tLoss: 0.003968\n","1472it [00:29, 38.38it/s]Train epoch: 8 [batch #1475, batch_size 16, seq length 1332]\tLoss: 0.004266\n","1500it [00:30, 37.10it/s]Train epoch: 8 [batch #1500, batch_size 16, seq length 1346]\tLoss: 0.004135\n","1524it [00:30, 36.89it/s]Train epoch: 8 [batch #1525, batch_size 16, seq length 1362]\tLoss: 0.003632\n","1548it [00:31, 37.34it/s]Train epoch: 8 [batch #1550, batch_size 16, seq length 1377]\tLoss: 0.004283\n","1572it [00:31, 36.31it/s]Train epoch: 8 [batch #1575, batch_size 16, seq length 1392]\tLoss: 0.003720\n","1600it [00:32, 35.88it/s]Train epoch: 8 [batch #1600, batch_size 16, seq length 1406]\tLoss: 0.003865\n","1624it [00:33, 35.90it/s]Train epoch: 8 [batch #1625, batch_size 16, seq length 1422]\tLoss: 0.004339\n","1648it [00:34, 36.57it/s]Train epoch: 8 [batch #1650, batch_size 16, seq length 1437]\tLoss: 0.004141\n","1672it [00:34, 36.03it/s]Train epoch: 8 [batch #1675, batch_size 16, seq length 1454]\tLoss: 0.004220\n","1700it [00:35, 34.35it/s]Train epoch: 8 [batch #1700, batch_size 16, seq length 1469]\tLoss: 0.004541\n","1724it [00:36, 35.50it/s]Train epoch: 8 [batch #1725, batch_size 16, seq length 1484]\tLoss: 0.004038\n","1748it [00:36, 35.18it/s]Train epoch: 8 [batch #1750, batch_size 16, seq length 1501]\tLoss: 0.004013\n","1772it [00:37, 34.72it/s]Train epoch: 8 [batch #1775, batch_size 16, seq length 1518]\tLoss: 0.004477\n","1800it [00:38, 34.07it/s]Train epoch: 8 [batch #1800, batch_size 16, seq length 1533]\tLoss: 0.004481\n","1824it [00:39, 33.31it/s]Train epoch: 8 [batch #1825, batch_size 16, seq length 1548]\tLoss: 0.004506\n","1848it [00:39, 33.42it/s]Train epoch: 8 [batch #1850, batch_size 16, seq length 1566]\tLoss: 0.004183\n","1872it [00:40, 32.19it/s]Train epoch: 8 [batch #1875, batch_size 16, seq length 1582]\tLoss: 0.004440\n","1900it [00:41, 31.98it/s]Train epoch: 8 [batch #1900, batch_size 16, seq length 1599]\tLoss: 0.004606\n","1924it [00:42, 32.44it/s]Train epoch: 8 [batch #1925, batch_size 16, seq length 1616]\tLoss: 0.004393\n","1948it [00:42, 32.41it/s]Train epoch: 8 [batch #1950, batch_size 16, seq length 1633]\tLoss: 0.004407\n","1972it [00:43, 30.78it/s]Train epoch: 8 [batch #1975, batch_size 16, seq length 1651]\tLoss: 0.004399\n","2000it [00:44, 30.20it/s]Train epoch: 8 [batch #2000, batch_size 16, seq length 1670]\tLoss: 0.004284\n","2024it [00:45, 32.13it/s]Train epoch: 8 [batch #2025, batch_size 16, seq length 1688]\tLoss: 0.004302\n","2048it [00:46, 31.58it/s]Train epoch: 8 [batch #2050, batch_size 16, seq length 1707]\tLoss: 0.004476\n","2072it [00:46, 31.32it/s]Train epoch: 8 [batch #2075, batch_size 16, seq length 1728]\tLoss: 0.004606\n","2100it [00:47, 31.01it/s]Train epoch: 8 [batch #2100, batch_size 16, seq length 1748]\tLoss: 0.004345\n","2124it [00:48, 30.65it/s]Train epoch: 8 [batch #2125, batch_size 16, seq length 1768]\tLoss: 0.004880\n","2148it [00:49, 30.26it/s]Train epoch: 8 [batch #2150, batch_size 16, seq length 1787]\tLoss: 0.004370\n","2173it [00:50, 29.76it/s]Train epoch: 8 [batch #2175, batch_size 16, seq length 1809]\tLoss: 0.005022\n","2198it [00:51, 29.82it/s]Train epoch: 8 [batch #2200, batch_size 16, seq length 1833]\tLoss: 0.004516\n","2224it [00:51, 29.66it/s]Train epoch: 8 [batch #2225, batch_size 16, seq length 1855]\tLoss: 0.004517\n","2250it [00:52, 29.23it/s]Train epoch: 8 [batch #2250, batch_size 16, seq length 1878]\tLoss: 0.004997\n","2273it [00:53, 29.20it/s]Train epoch: 8 [batch #2275, batch_size 16, seq length 1901]\tLoss: 0.004837\n","2300it [00:54, 28.23it/s]Train epoch: 8 [batch #2300, batch_size 16, seq length 1927]\tLoss: 0.004707\n","2324it [00:55, 28.26it/s]Train epoch: 8 [batch #2325, batch_size 16, seq length 1953]\tLoss: 0.005120\n","2348it [00:56, 28.70it/s]Train epoch: 8 [batch #2350, batch_size 16, seq length 1980]\tLoss: 0.004836\n","2375it [00:57, 28.11it/s]Train epoch: 8 [batch #2375, batch_size 16, seq length 2006]\tLoss: 0.005068\n","2399it [00:58, 27.55it/s]Train epoch: 8 [batch #2400, batch_size 16, seq length 2034]\tLoss: 0.005081\n","2423it [00:58, 27.60it/s]Train epoch: 8 [batch #2425, batch_size 16, seq length 2064]\tLoss: 0.004885\n","2450it [00:59, 26.49it/s]Train epoch: 8 [batch #2450, batch_size 16, seq length 2094]\tLoss: 0.005145\n","2474it [01:00, 26.53it/s]Train epoch: 8 [batch #2475, batch_size 16, seq length 2126]\tLoss: 0.005273\n","2498it [01:01, 26.47it/s]Train epoch: 8 [batch #2500, batch_size 16, seq length 2160]\tLoss: 0.005169\n","2525it [01:02, 26.20it/s]Train epoch: 8 [batch #2525, batch_size 16, seq length 2192]\tLoss: 0.005519\n","2549it [01:03, 26.45it/s]Train epoch: 8 [batch #2550, batch_size 16, seq length 2228]\tLoss: 0.005218\n","2573it [01:04, 25.56it/s]Train epoch: 8 [batch #2575, batch_size 16, seq length 2266]\tLoss: 0.005726\n","2600it [01:05, 25.03it/s]Train epoch: 8 [batch #2600, batch_size 16, seq length 2307]\tLoss: 0.005703\n","2624it [01:06, 24.23it/s]Train epoch: 8 [batch #2625, batch_size 16, seq length 2350]\tLoss: 0.005413\n","2648it [01:07, 23.66it/s]Train epoch: 8 [batch #2650, batch_size 16, seq length 2392]\tLoss: 0.005469\n","2675it [01:08, 22.93it/s]Train epoch: 8 [batch #2675, batch_size 16, seq length 2439]\tLoss: 0.005753\n","2699it [01:09, 23.43it/s]Train epoch: 8 [batch #2700, batch_size 16, seq length 2491]\tLoss: 0.005777\n","2723it [01:10, 22.97it/s]Train epoch: 8 [batch #2725, batch_size 16, seq length 2500]\tLoss: 0.006021\n","2750it [01:12, 22.41it/s]Train epoch: 8 [batch #2750, batch_size 16, seq length 2500]\tLoss: 0.006073\n","2774it [01:13, 23.11it/s]Train epoch: 8 [batch #2775, batch_size 16, seq length 2500]\tLoss: 0.006238\n","2798it [01:14, 22.81it/s]Train epoch: 8 [batch #2800, batch_size 16, seq length 2500]\tLoss: 0.006240\n","2825it [01:15, 22.61it/s]Train epoch: 8 [batch #2825, batch_size 16, seq length 2500]\tLoss: 0.006193\n","2849it [01:16, 22.19it/s]Train epoch: 8 [batch #2850, batch_size 16, seq length 2500]\tLoss: 0.006295\n","2873it [01:17, 21.85it/s]Train epoch: 8 [batch #2875, batch_size 16, seq length 2500]\tLoss: 0.006953\n","2900it [01:18, 21.30it/s]Train epoch: 8 [batch #2900, batch_size 16, seq length 2500]\tLoss: 0.006955\n","2924it [01:20, 20.86it/s]Train epoch: 8 [batch #2925, batch_size 16, seq length 2500]\tLoss: 0.007552\n","2948it [01:21, 20.22it/s]Train epoch: 8 [batch #2950, batch_size 16, seq length 2500]\tLoss: 0.007353\n","2974it [01:22, 18.03it/s]Train epoch: 8 [batch #2975, batch_size 16, seq length 2500]\tLoss: 0.008872\n","2983it [01:23, 35.89it/s]\n","epoch loss: 0.004290492060081299\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:03, 439.09it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0269, 0.0456, 0.0396, 0.0424, 0.8259\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2817, 0.5097, 0.3864, 0.4396, 0.9710\n","rec_at_8: 0.3173\n","prec_at_8: 0.5911\n","rec_at_15: 0.4382\n","prec_at_15: 0.4538\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/cnn_vanilla_May_02_220247\n","\n","EPOCH 9\n","0it [00:00, ?it/s]Train epoch: 9 [batch #0, batch_size 16, seq length 117]\tLoss: 0.006367\n","22it [00:00, 69.83it/s]Train epoch: 9 [batch #25, batch_size 16, seq length 337]\tLoss: 0.003770\n","49it [00:00, 80.21it/s]Train epoch: 9 [batch #50, batch_size 16, seq length 402]\tLoss: 0.003097\n","67it [00:00, 80.47it/s]Train epoch: 9 [batch #75, batch_size 16, seq length 452]\tLoss: 0.002760\n","93it [00:01, 78.33it/s]Train epoch: 9 [batch #100, batch_size 16, seq length 490]\tLoss: 0.002858\n","125it [00:01, 75.78it/s]Train epoch: 9 [batch #125, batch_size 16, seq length 520]\tLoss: 0.002993\n","149it [00:01, 69.27it/s]Train epoch: 9 [batch #150, batch_size 16, seq length 548]\tLoss: 0.003025\n","171it [00:02, 68.08it/s]Train epoch: 9 [batch #175, batch_size 16, seq length 574]\tLoss: 0.002766\n","199it [00:02, 67.13it/s]Train epoch: 9 [batch #200, batch_size 16, seq length 596]\tLoss: 0.003006\n","220it [00:03, 64.41it/s]Train epoch: 9 [batch #225, batch_size 16, seq length 618]\tLoss: 0.003005\n","248it [00:03, 62.68it/s]Train epoch: 9 [batch #250, batch_size 16, seq length 638]\tLoss: 0.003152\n","269it [00:03, 61.53it/s]Train epoch: 9 [batch #275, batch_size 16, seq length 656]\tLoss: 0.003288\n","297it [00:04, 62.09it/s]Train epoch: 9 [batch #300, batch_size 16, seq length 674]\tLoss: 0.003245\n","325it [00:04, 61.79it/s]Train epoch: 9 [batch #325, batch_size 16, seq length 693]\tLoss: 0.002999\n","346it [00:05, 60.35it/s]Train epoch: 9 [batch #350, batch_size 16, seq length 710]\tLoss: 0.003144\n","374it [00:05, 60.20it/s]Train epoch: 9 [batch #375, batch_size 16, seq length 726]\tLoss: 0.003415\n","395it [00:05, 60.19it/s]Train epoch: 9 [batch #400, batch_size 16, seq length 743]\tLoss: 0.002948\n","421it [00:06, 59.10it/s]Train epoch: 9 [batch #425, batch_size 16, seq length 758]\tLoss: 0.003038\n","446it [00:06, 58.62it/s]Train epoch: 9 [batch #450, batch_size 16, seq length 773]\tLoss: 0.003328\n","470it [00:07, 55.37it/s]Train epoch: 9 [batch #475, batch_size 16, seq length 789]\tLoss: 0.002898\n","500it [00:07, 55.76it/s]Train epoch: 9 [batch #500, batch_size 16, seq length 803]\tLoss: 0.003220\n","524it [00:08, 54.78it/s]Train epoch: 9 [batch #525, batch_size 16, seq length 818]\tLoss: 0.003216\n","548it [00:08, 55.10it/s]Train epoch: 9 [batch #550, batch_size 16, seq length 833]\tLoss: 0.003145\n","572it [00:09, 54.20it/s]Train epoch: 9 [batch #575, batch_size 16, seq length 846]\tLoss: 0.003106\n","596it [00:09, 54.00it/s]Train epoch: 9 [batch #600, batch_size 16, seq length 860]\tLoss: 0.003491\n","620it [00:09, 53.92it/s]Train epoch: 9 [batch #625, batch_size 16, seq length 874]\tLoss: 0.003362\n","650it [00:10, 53.83it/s]Train epoch: 9 [batch #650, batch_size 16, seq length 887]\tLoss: 0.003141\n","674it [00:10, 53.18it/s]Train epoch: 9 [batch #675, batch_size 16, seq length 901]\tLoss: 0.003172\n","698it [00:11, 52.36it/s]Train epoch: 9 [batch #700, batch_size 16, seq length 915]\tLoss: 0.003124\n","722it [00:11, 52.25it/s]Train epoch: 9 [batch #725, batch_size 16, seq length 929]\tLoss: 0.003203\n","746it [00:12, 50.91it/s]Train epoch: 9 [batch #750, batch_size 16, seq length 942]\tLoss: 0.003453\n","770it [00:12, 51.18it/s]Train epoch: 9 [batch #775, batch_size 16, seq length 954]\tLoss: 0.003320\n","799it [00:13, 47.76it/s]Train epoch: 9 [batch #800, batch_size 16, seq length 968]\tLoss: 0.003436\n","821it [00:13, 48.69it/s]Train epoch: 9 [batch #825, batch_size 16, seq length 981]\tLoss: 0.003385\n","847it [00:14, 48.02it/s]Train epoch: 9 [batch #850, batch_size 16, seq length 994]\tLoss: 0.003437\n","873it [00:14, 47.80it/s]Train epoch: 9 [batch #875, batch_size 16, seq length 1006]\tLoss: 0.003278\n","898it [00:15, 46.63it/s]Train epoch: 9 [batch #900, batch_size 16, seq length 1020]\tLoss: 0.003564\n","923it [00:16, 45.95it/s]Train epoch: 9 [batch #925, batch_size 16, seq length 1033]\tLoss: 0.003304\n","948it [00:16, 45.37it/s]Train epoch: 9 [batch #950, batch_size 16, seq length 1046]\tLoss: 0.003541\n","973it [00:17, 44.61it/s]Train epoch: 9 [batch #975, batch_size 16, seq length 1059]\tLoss: 0.003521\n","998it [00:17, 44.63it/s]Train epoch: 9 [batch #1000, batch_size 16, seq length 1073]\tLoss: 0.003523\n","1023it [00:18, 44.36it/s]Train epoch: 9 [batch #1025, batch_size 16, seq length 1085]\tLoss: 0.003749\n","1048it [00:18, 43.51it/s]Train epoch: 9 [batch #1050, batch_size 16, seq length 1099]\tLoss: 0.003636\n","1073it [00:19, 42.75it/s]Train epoch: 9 [batch #1075, batch_size 16, seq length 1113]\tLoss: 0.003625\n","1098it [00:20, 42.48it/s]Train epoch: 9 [batch #1100, batch_size 16, seq length 1126]\tLoss: 0.003539\n","1123it [00:20, 42.32it/s]Train epoch: 9 [batch #1125, batch_size 16, seq length 1140]\tLoss: 0.003783\n","1148it [00:21, 42.16it/s]Train epoch: 9 [batch #1150, batch_size 16, seq length 1154]\tLoss: 0.003735\n","1173it [00:21, 41.86it/s]Train epoch: 9 [batch #1175, batch_size 16, seq length 1167]\tLoss: 0.003798\n","1198it [00:22, 42.03it/s]Train epoch: 9 [batch #1200, batch_size 16, seq length 1180]\tLoss: 0.003576\n","1223it [00:23, 41.23it/s]Train epoch: 9 [batch #1225, batch_size 16, seq length 1195]\tLoss: 0.003664\n","1248it [00:23, 40.04it/s]Train epoch: 9 [batch #1250, batch_size 16, seq length 1208]\tLoss: 0.003520\n","1273it [00:24, 39.98it/s]Train epoch: 9 [batch #1275, batch_size 16, seq length 1220]\tLoss: 0.003631\n","1298it [00:24, 40.46it/s]Train epoch: 9 [batch #1300, batch_size 16, seq length 1234]\tLoss: 0.003757\n","1323it [00:25, 39.87it/s]Train epoch: 9 [batch #1325, batch_size 16, seq length 1248]\tLoss: 0.003624\n","1348it [00:26, 40.07it/s]Train epoch: 9 [batch #1350, batch_size 16, seq length 1262]\tLoss: 0.003921\n","1374it [00:26, 39.15it/s]Train epoch: 9 [batch #1375, batch_size 16, seq length 1276]\tLoss: 0.004198\n","1398it [00:27, 38.59it/s]Train epoch: 9 [batch #1400, batch_size 16, seq length 1290]\tLoss: 0.004137\n","1422it [00:28, 38.55it/s]Train epoch: 9 [batch #1425, batch_size 16, seq length 1304]\tLoss: 0.004226\n","1450it [00:28, 36.50it/s]Train epoch: 9 [batch #1450, batch_size 16, seq length 1319]\tLoss: 0.003942\n","1474it [00:29, 37.20it/s]Train epoch: 9 [batch #1475, batch_size 16, seq length 1332]\tLoss: 0.004174\n","1498it [00:30, 37.36it/s]Train epoch: 9 [batch #1500, batch_size 16, seq length 1346]\tLoss: 0.004011\n","1522it [00:30, 37.21it/s]Train epoch: 9 [batch #1525, batch_size 16, seq length 1362]\tLoss: 0.003581\n","1550it [00:31, 37.80it/s]Train epoch: 9 [batch #1550, batch_size 16, seq length 1377]\tLoss: 0.004176\n","1574it [00:32, 36.78it/s]Train epoch: 9 [batch #1575, batch_size 16, seq length 1392]\tLoss: 0.003591\n","1598it [00:32, 36.73it/s]Train epoch: 9 [batch #1600, batch_size 16, seq length 1406]\tLoss: 0.003825\n","1622it [00:33, 36.61it/s]Train epoch: 9 [batch #1625, batch_size 16, seq length 1422]\tLoss: 0.004271\n","1650it [00:34, 35.56it/s]Train epoch: 9 [batch #1650, batch_size 16, seq length 1437]\tLoss: 0.004117\n","1674it [00:34, 35.00it/s]Train epoch: 9 [batch #1675, batch_size 16, seq length 1454]\tLoss: 0.004128\n","1698it [00:35, 35.89it/s]Train epoch: 9 [batch #1700, batch_size 16, seq length 1469]\tLoss: 0.004445\n","1722it [00:36, 35.11it/s]Train epoch: 9 [batch #1725, batch_size 16, seq length 1484]\tLoss: 0.004038\n","1750it [00:37, 35.25it/s]Train epoch: 9 [batch #1750, batch_size 16, seq length 1501]\tLoss: 0.003890\n","1774it [00:37, 33.79it/s]Train epoch: 9 [batch #1775, batch_size 16, seq length 1518]\tLoss: 0.004353\n","1798it [00:38, 34.08it/s]Train epoch: 9 [batch #1800, batch_size 16, seq length 1533]\tLoss: 0.004378\n","1822it [00:39, 33.46it/s]Train epoch: 9 [batch #1825, batch_size 16, seq length 1548]\tLoss: 0.004403\n","1850it [00:40, 33.56it/s]Train epoch: 9 [batch #1850, batch_size 16, seq length 1566]\tLoss: 0.004150\n","1874it [00:40, 33.60it/s]Train epoch: 9 [batch #1875, batch_size 16, seq length 1582]\tLoss: 0.004333\n","1898it [00:41, 33.33it/s]Train epoch: 9 [batch #1900, batch_size 16, seq length 1599]\tLoss: 0.004519\n","1922it [00:42, 33.05it/s]Train epoch: 9 [batch #1925, batch_size 16, seq length 1616]\tLoss: 0.004374\n","1950it [00:43, 32.42it/s]Train epoch: 9 [batch #1950, batch_size 16, seq length 1633]\tLoss: 0.004324\n","1974it [00:43, 32.54it/s]Train epoch: 9 [batch #1975, batch_size 16, seq length 1651]\tLoss: 0.004293\n","1998it [00:44, 32.15it/s]Train epoch: 9 [batch #2000, batch_size 16, seq length 1670]\tLoss: 0.004219\n","2022it [00:45, 32.08it/s]Train epoch: 9 [batch #2025, batch_size 16, seq length 1688]\tLoss: 0.004248\n","2050it [00:46, 32.03it/s]Train epoch: 9 [batch #2050, batch_size 16, seq length 1707]\tLoss: 0.004340\n","2074it [00:46, 31.57it/s]Train epoch: 9 [batch #2075, batch_size 16, seq length 1728]\tLoss: 0.004533\n","2098it [00:47, 31.60it/s]Train epoch: 9 [batch #2100, batch_size 16, seq length 1748]\tLoss: 0.004295\n","2122it [00:48, 30.97it/s]Train epoch: 9 [batch #2125, batch_size 16, seq length 1768]\tLoss: 0.004819\n","2150it [00:49, 30.32it/s]Train epoch: 9 [batch #2150, batch_size 16, seq length 1787]\tLoss: 0.004322\n","2174it [00:50, 29.79it/s]Train epoch: 9 [batch #2175, batch_size 16, seq length 1809]\tLoss: 0.004970\n","2198it [00:51, 29.34it/s]Train epoch: 9 [batch #2200, batch_size 16, seq length 1833]\tLoss: 0.004531\n","2223it [00:51, 29.78it/s]Train epoch: 9 [batch #2225, batch_size 16, seq length 1855]\tLoss: 0.004542\n","2247it [00:52, 29.20it/s]Train epoch: 9 [batch #2250, batch_size 16, seq length 1878]\tLoss: 0.004870\n","2272it [00:53, 29.17it/s]Train epoch: 9 [batch #2275, batch_size 16, seq length 1901]\tLoss: 0.004750\n","2300it [00:54, 28.83it/s]Train epoch: 9 [batch #2300, batch_size 16, seq length 1927]\tLoss: 0.004645\n","2324it [00:55, 28.42it/s]Train epoch: 9 [batch #2325, batch_size 16, seq length 1953]\tLoss: 0.005039\n","2348it [00:56, 27.98it/s]Train epoch: 9 [batch #2350, batch_size 16, seq length 1980]\tLoss: 0.004711\n","2375it [00:57, 27.75it/s]Train epoch: 9 [batch #2375, batch_size 16, seq length 2006]\tLoss: 0.005020\n","2399it [00:58, 27.78it/s]Train epoch: 9 [batch #2400, batch_size 16, seq length 2034]\tLoss: 0.004889\n","2423it [00:58, 26.76it/s]Train epoch: 9 [batch #2425, batch_size 16, seq length 2064]\tLoss: 0.004799\n","2450it [00:59, 27.01it/s]Train epoch: 9 [batch #2450, batch_size 16, seq length 2094]\tLoss: 0.005089\n","2474it [01:00, 26.79it/s]Train epoch: 9 [batch #2475, batch_size 16, seq length 2126]\tLoss: 0.005195\n","2498it [01:01, 26.56it/s]Train epoch: 9 [batch #2500, batch_size 16, seq length 2160]\tLoss: 0.005096\n","2525it [01:02, 25.92it/s]Train epoch: 9 [batch #2525, batch_size 16, seq length 2192]\tLoss: 0.005349\n","2549it [01:03, 26.31it/s]Train epoch: 9 [batch #2550, batch_size 16, seq length 2228]\tLoss: 0.005109\n","2573it [01:04, 25.65it/s]Train epoch: 9 [batch #2575, batch_size 16, seq length 2266]\tLoss: 0.005713\n","2600it [01:05, 24.21it/s]Train epoch: 9 [batch #2600, batch_size 16, seq length 2307]\tLoss: 0.005623\n","2624it [01:06, 24.28it/s]Train epoch: 9 [batch #2625, batch_size 16, seq length 2350]\tLoss: 0.005363\n","2648it [01:07, 23.61it/s]Train epoch: 9 [batch #2650, batch_size 16, seq length 2392]\tLoss: 0.005297\n","2675it [01:08, 24.03it/s]Train epoch: 9 [batch #2675, batch_size 16, seq length 2439]\tLoss: 0.005680\n","2699it [01:09, 23.44it/s]Train epoch: 9 [batch #2700, batch_size 16, seq length 2491]\tLoss: 0.005653\n","2723it [01:10, 22.89it/s]Train epoch: 9 [batch #2725, batch_size 16, seq length 2500]\tLoss: 0.005877\n","2750it [01:12, 22.24it/s]Train epoch: 9 [batch #2750, batch_size 16, seq length 2500]\tLoss: 0.005957\n","2774it [01:13, 22.79it/s]Train epoch: 9 [batch #2775, batch_size 16, seq length 2500]\tLoss: 0.006114\n","2798it [01:14, 22.39it/s]Train epoch: 9 [batch #2800, batch_size 16, seq length 2500]\tLoss: 0.005981\n","2825it [01:15, 22.64it/s]Train epoch: 9 [batch #2825, batch_size 16, seq length 2500]\tLoss: 0.005908\n","2849it [01:16, 22.16it/s]Train epoch: 9 [batch #2850, batch_size 16, seq length 2500]\tLoss: 0.006221\n","2873it [01:17, 21.90it/s]Train epoch: 9 [batch #2875, batch_size 16, seq length 2500]\tLoss: 0.006872\n","2900it [01:18, 21.51it/s]Train epoch: 9 [batch #2900, batch_size 16, seq length 2500]\tLoss: 0.006897\n","2924it [01:20, 21.02it/s]Train epoch: 9 [batch #2925, batch_size 16, seq length 2500]\tLoss: 0.007371\n","2948it [01:21, 20.16it/s]Train epoch: 9 [batch #2950, batch_size 16, seq length 2500]\tLoss: 0.007304\n","2974it [01:22, 18.08it/s]Train epoch: 9 [batch #2975, batch_size 16, seq length 2500]\tLoss: 0.008675\n","2983it [01:23, 35.89it/s]\n","epoch loss: 0.004222882775049076\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:03, 439.83it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0283, 0.0439, 0.0449, 0.0444, 0.8245\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2792, 0.4708, 0.4069, 0.4365, 0.9708\n","rec_at_8: 0.3120\n","prec_at_8: 0.5779\n","rec_at_15: 0.4350\n","prec_at_15: 0.4488\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/cnn_vanilla_May_02_220247\n","\n","EPOCH 10\n","0it [00:00, ?it/s]Train epoch: 10 [batch #0, batch_size 16, seq length 117]\tLoss: 0.006262\n","22it [00:00, 72.93it/s]Train epoch: 10 [batch #25, batch_size 16, seq length 337]\tLoss: 0.004880\n","48it [00:00, 79.33it/s]Train epoch: 10 [batch #50, batch_size 16, seq length 402]\tLoss: 0.003456\n","75it [00:00, 79.40it/s]Train epoch: 10 [batch #75, batch_size 16, seq length 452]\tLoss: 0.002881\n","99it [00:01, 77.27it/s]Train epoch: 10 [batch #100, batch_size 16, seq length 490]\tLoss: 0.002729\n","123it [00:01, 73.81it/s]Train epoch: 10 [batch #125, batch_size 16, seq length 520]\tLoss: 0.002996\n","147it [00:01, 71.08it/s]Train epoch: 10 [batch #150, batch_size 16, seq length 548]\tLoss: 0.002985\n","169it [00:02, 68.56it/s]Train epoch: 10 [batch #175, batch_size 16, seq length 574]\tLoss: 0.002766\n","197it [00:02, 67.46it/s]Train epoch: 10 [batch #200, batch_size 16, seq length 596]\tLoss: 0.002956\n","225it [00:03, 64.51it/s]Train epoch: 10 [batch #225, batch_size 16, seq length 618]\tLoss: 0.002940\n","246it [00:03, 64.92it/s]Train epoch: 10 [batch #250, batch_size 16, seq length 638]\tLoss: 0.002740\n","274it [00:03, 62.94it/s]Train epoch: 10 [batch #275, batch_size 16, seq length 656]\tLoss: 0.003107\n","295it [00:04, 62.45it/s]Train epoch: 10 [batch #300, batch_size 16, seq length 674]\tLoss: 0.003106\n","323it [00:04, 60.83it/s]Train epoch: 10 [batch #325, batch_size 16, seq length 693]\tLoss: 0.002938\n","344it [00:05, 59.66it/s]Train epoch: 10 [batch #350, batch_size 16, seq length 710]\tLoss: 0.003024\n","372it [00:05, 60.07it/s]Train epoch: 10 [batch #375, batch_size 16, seq length 726]\tLoss: 0.003405\n","398it [00:05, 58.82it/s]Train epoch: 10 [batch #400, batch_size 16, seq length 743]\tLoss: 0.002933\n","422it [00:06, 58.18it/s]Train epoch: 10 [batch #425, batch_size 16, seq length 758]\tLoss: 0.003061\n","446it [00:06, 58.35it/s]Train epoch: 10 [batch #450, batch_size 16, seq length 773]\tLoss: 0.003235\n","470it [00:07, 56.01it/s]Train epoch: 10 [batch #475, batch_size 16, seq length 789]\tLoss: 0.002940\n","500it [00:07, 55.13it/s]Train epoch: 10 [batch #500, batch_size 16, seq length 803]\tLoss: 0.003195\n","524it [00:08, 55.21it/s]Train epoch: 10 [batch #525, batch_size 16, seq length 818]\tLoss: 0.003247\n","548it [00:08, 55.09it/s]Train epoch: 10 [batch #550, batch_size 16, seq length 833]\tLoss: 0.003170\n","572it [00:09, 54.88it/s]Train epoch: 10 [batch #575, batch_size 16, seq length 846]\tLoss: 0.003102\n","596it [00:09, 54.37it/s]Train epoch: 10 [batch #600, batch_size 16, seq length 860]\tLoss: 0.003412\n","620it [00:09, 53.11it/s]Train epoch: 10 [batch #625, batch_size 16, seq length 874]\tLoss: 0.003304\n","650it [00:10, 54.11it/s]Train epoch: 10 [batch #650, batch_size 16, seq length 887]\tLoss: 0.003079\n","674it [00:11, 53.23it/s]Train epoch: 10 [batch #675, batch_size 16, seq length 901]\tLoss: 0.003205\n","698it [00:11, 53.11it/s]Train epoch: 10 [batch #700, batch_size 16, seq length 915]\tLoss: 0.003038\n","722it [00:11, 52.16it/s]Train epoch: 10 [batch #725, batch_size 16, seq length 929]\tLoss: 0.003174\n","746it [00:12, 51.35it/s]Train epoch: 10 [batch #750, batch_size 16, seq length 942]\tLoss: 0.003435\n","770it [00:12, 49.30it/s]Train epoch: 10 [batch #775, batch_size 16, seq length 954]\tLoss: 0.003322\n","798it [00:13, 49.17it/s]Train epoch: 10 [batch #800, batch_size 16, seq length 968]\tLoss: 0.003418\n","825it [00:14, 48.22it/s]Train epoch: 10 [batch #825, batch_size 16, seq length 981]\tLoss: 0.003294\n","850it [00:14, 47.48it/s]Train epoch: 10 [batch #850, batch_size 16, seq length 994]\tLoss: 0.003421\n","870it [00:14, 46.26it/s]Train epoch: 10 [batch #875, batch_size 16, seq length 1006]\tLoss: 0.003304\n","896it [00:15, 47.46it/s]Train epoch: 10 [batch #900, batch_size 16, seq length 1020]\tLoss: 0.003532\n","921it [00:16, 46.06it/s]Train epoch: 10 [batch #925, batch_size 16, seq length 1033]\tLoss: 0.003283\n","946it [00:16, 45.30it/s]Train epoch: 10 [batch #950, batch_size 16, seq length 1046]\tLoss: 0.003543\n","971it [00:17, 45.43it/s]Train epoch: 10 [batch #975, batch_size 16, seq length 1059]\tLoss: 0.003407\n","996it [00:17, 44.96it/s]Train epoch: 10 [batch #1000, batch_size 16, seq length 1073]\tLoss: 0.003447\n","1021it [00:18, 44.06it/s]Train epoch: 10 [batch #1025, batch_size 16, seq length 1085]\tLoss: 0.003609\n","1046it [00:18, 43.60it/s]Train epoch: 10 [batch #1050, batch_size 16, seq length 1099]\tLoss: 0.003651\n","1071it [00:19, 44.11it/s]Train epoch: 10 [batch #1075, batch_size 16, seq length 1113]\tLoss: 0.003549\n","1096it [00:20, 42.86it/s]Train epoch: 10 [batch #1100, batch_size 16, seq length 1126]\tLoss: 0.003496\n","1121it [00:20, 43.56it/s]Train epoch: 10 [batch #1125, batch_size 16, seq length 1140]\tLoss: 0.003688\n","1146it [00:21, 42.95it/s]Train epoch: 10 [batch #1150, batch_size 16, seq length 1154]\tLoss: 0.003648\n","1171it [00:21, 42.13it/s]Train epoch: 10 [batch #1175, batch_size 16, seq length 1167]\tLoss: 0.003715\n","1196it [00:22, 41.68it/s]Train epoch: 10 [batch #1200, batch_size 16, seq length 1180]\tLoss: 0.003547\n","1221it [00:22, 41.33it/s]Train epoch: 10 [batch #1225, batch_size 16, seq length 1195]\tLoss: 0.003605\n","1246it [00:23, 40.78it/s]Train epoch: 10 [batch #1250, batch_size 16, seq length 1208]\tLoss: 0.003507\n","1271it [00:24, 40.80it/s]Train epoch: 10 [batch #1275, batch_size 16, seq length 1220]\tLoss: 0.003506\n","1296it [00:24, 40.97it/s]Train epoch: 10 [batch #1300, batch_size 16, seq length 1234]\tLoss: 0.003675\n","1321it [00:25, 40.14it/s]Train epoch: 10 [batch #1325, batch_size 16, seq length 1248]\tLoss: 0.003600\n","1346it [00:26, 40.45it/s]Train epoch: 10 [batch #1350, batch_size 16, seq length 1262]\tLoss: 0.003885\n","1372it [00:26, 39.49it/s]Train epoch: 10 [batch #1375, batch_size 16, seq length 1276]\tLoss: 0.004057\n","1400it [00:27, 38.82it/s]Train epoch: 10 [batch #1400, batch_size 16, seq length 1290]\tLoss: 0.004089\n","1425it [00:28, 38.34it/s]Train epoch: 10 [batch #1425, batch_size 16, seq length 1304]\tLoss: 0.004175\n","1449it [00:28, 37.01it/s]Train epoch: 10 [batch #1450, batch_size 16, seq length 1319]\tLoss: 0.003816\n","1473it [00:29, 38.07it/s]Train epoch: 10 [batch #1475, batch_size 16, seq length 1332]\tLoss: 0.004123\n","1497it [00:30, 36.41it/s]Train epoch: 10 [batch #1500, batch_size 16, seq length 1346]\tLoss: 0.003972\n","1525it [00:30, 37.80it/s]Train epoch: 10 [batch #1525, batch_size 16, seq length 1362]\tLoss: 0.003452\n","1549it [00:31, 37.24it/s]Train epoch: 10 [batch #1550, batch_size 16, seq length 1377]\tLoss: 0.004066\n","1573it [00:32, 36.53it/s]Train epoch: 10 [batch #1575, batch_size 16, seq length 1392]\tLoss: 0.003527\n","1597it [00:32, 36.46it/s]Train epoch: 10 [batch #1600, batch_size 16, seq length 1406]\tLoss: 0.003725\n","1625it [00:33, 36.61it/s]Train epoch: 10 [batch #1625, batch_size 16, seq length 1422]\tLoss: 0.004211\n","1649it [00:34, 35.98it/s]Train epoch: 10 [batch #1650, batch_size 16, seq length 1437]\tLoss: 0.004071\n","1673it [00:34, 35.97it/s]Train epoch: 10 [batch #1675, batch_size 16, seq length 1454]\tLoss: 0.004002\n","1697it [00:35, 35.42it/s]Train epoch: 10 [batch #1700, batch_size 16, seq length 1469]\tLoss: 0.004312\n","1725it [00:36, 35.55it/s]Train epoch: 10 [batch #1725, batch_size 16, seq length 1484]\tLoss: 0.003933\n","1749it [00:36, 35.54it/s]Train epoch: 10 [batch #1750, batch_size 16, seq length 1501]\tLoss: 0.003780\n","1773it [00:37, 34.89it/s]Train epoch: 10 [batch #1775, batch_size 16, seq length 1518]\tLoss: 0.004289\n","1797it [00:38, 33.95it/s]Train epoch: 10 [batch #1800, batch_size 16, seq length 1533]\tLoss: 0.004279\n","1825it [00:39, 32.74it/s]Train epoch: 10 [batch #1825, batch_size 16, seq length 1548]\tLoss: 0.004360\n","1849it [00:39, 33.75it/s]Train epoch: 10 [batch #1850, batch_size 16, seq length 1566]\tLoss: 0.004007\n","1873it [00:40, 32.63it/s]Train epoch: 10 [batch #1875, batch_size 16, seq length 1582]\tLoss: 0.004270\n","1897it [00:41, 33.04it/s]Train epoch: 10 [batch #1900, batch_size 16, seq length 1599]\tLoss: 0.004480\n","1925it [00:42, 32.70it/s]Train epoch: 10 [batch #1925, batch_size 16, seq length 1616]\tLoss: 0.004252\n","1949it [00:43, 31.97it/s]Train epoch: 10 [batch #1950, batch_size 16, seq length 1633]\tLoss: 0.004281\n","1973it [00:43, 32.53it/s]Train epoch: 10 [batch #1975, batch_size 16, seq length 1651]\tLoss: 0.004264\n","1997it [00:44, 32.26it/s]Train epoch: 10 [batch #2000, batch_size 16, seq length 1670]\tLoss: 0.004108\n","2025it [00:45, 32.37it/s]Train epoch: 10 [batch #2025, batch_size 16, seq length 1688]\tLoss: 0.004197\n","2049it [00:46, 32.04it/s]Train epoch: 10 [batch #2050, batch_size 16, seq length 1707]\tLoss: 0.004173\n","2073it [00:46, 31.91it/s]Train epoch: 10 [batch #2075, batch_size 16, seq length 1728]\tLoss: 0.004494\n","2097it [00:47, 31.04it/s]Train epoch: 10 [batch #2100, batch_size 16, seq length 1748]\tLoss: 0.004209\n","2125it [00:48, 30.51it/s]Train epoch: 10 [batch #2125, batch_size 16, seq length 1768]\tLoss: 0.004692\n","2149it [00:49, 29.23it/s]Train epoch: 10 [batch #2150, batch_size 16, seq length 1787]\tLoss: 0.004231\n","2172it [00:50, 29.02it/s]Train epoch: 10 [batch #2175, batch_size 16, seq length 1809]\tLoss: 0.004887\n","2198it [00:51, 29.16it/s]Train epoch: 10 [batch #2200, batch_size 16, seq length 1833]\tLoss: 0.004428\n","2223it [00:51, 29.86it/s]Train epoch: 10 [batch #2225, batch_size 16, seq length 1855]\tLoss: 0.004460\n","2250it [00:52, 29.32it/s]Train epoch: 10 [batch #2250, batch_size 16, seq length 1878]\tLoss: 0.004710\n","2275it [00:53, 28.75it/s]Train epoch: 10 [batch #2275, batch_size 16, seq length 1901]\tLoss: 0.004657\n","2299it [00:54, 28.46it/s]Train epoch: 10 [batch #2300, batch_size 16, seq length 1927]\tLoss: 0.004569\n","2323it [00:55, 29.21it/s]Train epoch: 10 [batch #2325, batch_size 16, seq length 1953]\tLoss: 0.004910\n","2350it [00:56, 28.42it/s]Train epoch: 10 [batch #2350, batch_size 16, seq length 1980]\tLoss: 0.004595\n","2374it [00:57, 27.96it/s]Train epoch: 10 [batch #2375, batch_size 16, seq length 2006]\tLoss: 0.004935\n","2398it [00:58, 27.71it/s]Train epoch: 10 [batch #2400, batch_size 16, seq length 2034]\tLoss: 0.004860\n","2425it [00:59, 26.59it/s]Train epoch: 10 [batch #2425, batch_size 16, seq length 2064]\tLoss: 0.004775\n","2449it [00:59, 25.75it/s]Train epoch: 10 [batch #2450, batch_size 16, seq length 2094]\tLoss: 0.004970\n","2473it [01:00, 25.74it/s]Train epoch: 10 [batch #2475, batch_size 16, seq length 2126]\tLoss: 0.005087\n","2500it [01:01, 26.41it/s]Train epoch: 10 [batch #2500, batch_size 16, seq length 2160]\tLoss: 0.005002\n","2524it [01:02, 25.67it/s]Train epoch: 10 [batch #2525, batch_size 16, seq length 2192]\tLoss: 0.005329\n","2548it [01:03, 26.11it/s]Train epoch: 10 [batch #2550, batch_size 16, seq length 2228]\tLoss: 0.005086\n","2575it [01:04, 25.61it/s]Train epoch: 10 [batch #2575, batch_size 16, seq length 2266]\tLoss: 0.005522\n","2599it [01:05, 24.47it/s]Train epoch: 10 [batch #2600, batch_size 16, seq length 2307]\tLoss: 0.005490\n","2623it [01:06, 24.48it/s]Train epoch: 10 [batch #2625, batch_size 16, seq length 2350]\tLoss: 0.005282\n","2650it [01:07, 23.77it/s]Train epoch: 10 [batch #2650, batch_size 16, seq length 2392]\tLoss: 0.005240\n","2674it [01:08, 23.44it/s]Train epoch: 10 [batch #2675, batch_size 16, seq length 2439]\tLoss: 0.005540\n","2698it [01:09, 23.62it/s]Train epoch: 10 [batch #2700, batch_size 16, seq length 2491]\tLoss: 0.005619\n","2725it [01:11, 22.91it/s]Train epoch: 10 [batch #2725, batch_size 16, seq length 2500]\tLoss: 0.005789\n","2749it [01:12, 23.04it/s]Train epoch: 10 [batch #2750, batch_size 16, seq length 2500]\tLoss: 0.005754\n","2773it [01:13, 22.59it/s]Train epoch: 10 [batch #2775, batch_size 16, seq length 2500]\tLoss: 0.006063\n","2800it [01:14, 22.28it/s]Train epoch: 10 [batch #2800, batch_size 16, seq length 2500]\tLoss: 0.005815\n","2824it [01:15, 22.44it/s]Train epoch: 10 [batch #2825, batch_size 16, seq length 2500]\tLoss: 0.005931\n","2848it [01:16, 22.48it/s]Train epoch: 10 [batch #2850, batch_size 16, seq length 2500]\tLoss: 0.006127\n","2875it [01:17, 21.99it/s]Train epoch: 10 [batch #2875, batch_size 16, seq length 2500]\tLoss: 0.006801\n","2899it [01:18, 21.56it/s]Train epoch: 10 [batch #2900, batch_size 16, seq length 2500]\tLoss: 0.006815\n","2923it [01:19, 20.49it/s]Train epoch: 10 [batch #2925, batch_size 16, seq length 2500]\tLoss: 0.007180\n","2948it [01:21, 19.65it/s]Train epoch: 10 [batch #2950, batch_size 16, seq length 2500]\tLoss: 0.007246\n","2974it [01:22, 17.78it/s]Train epoch: 10 [batch #2975, batch_size 16, seq length 2500]\tLoss: 0.008511\n","2983it [01:23, 35.88it/s]\n","epoch loss: 0.00418118223093826\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:03, 438.77it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0274, 0.0445, 0.0431, 0.0438, 0.8257\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2759, 0.4740, 0.3977, 0.4325, 0.9709\n","rec_at_8: 0.3109\n","prec_at_8: 0.5733\n","rec_at_15: 0.4318\n","prec_at_15: 0.4443\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/cnn_vanilla_May_02_220247\n","\n","EPOCH 11\n","0it [00:00, ?it/s]Train epoch: 11 [batch #0, batch_size 16, seq length 117]\tLoss: 0.005732\n","21it [00:00, 67.02it/s]Train epoch: 11 [batch #25, batch_size 16, seq length 337]\tLoss: 0.003662\n","47it [00:00, 77.27it/s]Train epoch: 11 [batch #50, batch_size 16, seq length 402]\tLoss: 0.002988\n","72it [00:00, 77.19it/s]Train epoch: 11 [batch #75, batch_size 16, seq length 452]\tLoss: 0.002703\n","96it [00:01, 76.71it/s]Train epoch: 11 [batch #100, batch_size 16, seq length 490]\tLoss: 0.002554\n","120it [00:01, 75.17it/s]Train epoch: 11 [batch #125, batch_size 16, seq length 520]\tLoss: 0.002880\n","144it [00:01, 71.18it/s]Train epoch: 11 [batch #150, batch_size 16, seq length 548]\tLoss: 0.002967\n","174it [00:02, 68.93it/s]Train epoch: 11 [batch #175, batch_size 16, seq length 574]\tLoss: 0.002833\n","195it [00:02, 65.75it/s]Train epoch: 11 [batch #200, batch_size 16, seq length 596]\tLoss: 0.002939\n","223it [00:03, 64.02it/s]Train epoch: 11 [batch #225, batch_size 16, seq length 618]\tLoss: 0.002879\n","244it [00:03, 64.97it/s]Train epoch: 11 [batch #250, batch_size 16, seq length 638]\tLoss: 0.002747\n","272it [00:03, 62.21it/s]Train epoch: 11 [batch #275, batch_size 16, seq length 656]\tLoss: 0.003072\n","300it [00:04, 62.83it/s]Train epoch: 11 [batch #300, batch_size 16, seq length 674]\tLoss: 0.003086\n","321it [00:04, 62.56it/s]Train epoch: 11 [batch #325, batch_size 16, seq length 693]\tLoss: 0.002818\n","349it [00:05, 60.24it/s]Train epoch: 11 [batch #350, batch_size 16, seq length 710]\tLoss: 0.003021\n","370it [00:05, 60.40it/s]Train epoch: 11 [batch #375, batch_size 16, seq length 726]\tLoss: 0.003335\n","396it [00:05, 58.43it/s]Train epoch: 11 [batch #400, batch_size 16, seq length 743]\tLoss: 0.002846\n","422it [00:06, 58.48it/s]Train epoch: 11 [batch #425, batch_size 16, seq length 758]\tLoss: 0.002997\n","447it [00:06, 58.98it/s]Train epoch: 11 [batch #450, batch_size 16, seq length 773]\tLoss: 0.003234\n","471it [00:07, 55.73it/s]Train epoch: 11 [batch #475, batch_size 16, seq length 789]\tLoss: 0.002848\n","495it [00:07, 56.37it/s]Train epoch: 11 [batch #500, batch_size 16, seq length 803]\tLoss: 0.003143\n","525it [00:08, 55.05it/s]Train epoch: 11 [batch #525, batch_size 16, seq length 818]\tLoss: 0.003060\n","549it [00:08, 53.74it/s]Train epoch: 11 [batch #550, batch_size 16, seq length 833]\tLoss: 0.002995\n","573it [00:09, 53.65it/s]Train epoch: 11 [batch #575, batch_size 16, seq length 846]\tLoss: 0.002929\n","597it [00:09, 54.37it/s]Train epoch: 11 [batch #600, batch_size 16, seq length 860]\tLoss: 0.003347\n","621it [00:10, 51.62it/s]Train epoch: 11 [batch #625, batch_size 16, seq length 874]\tLoss: 0.003233\n","645it [00:10, 52.78it/s]Train epoch: 11 [batch #650, batch_size 16, seq length 887]\tLoss: 0.003069\n","675it [00:11, 53.42it/s]Train epoch: 11 [batch #675, batch_size 16, seq length 901]\tLoss: 0.003094\n","699it [00:11, 52.60it/s]Train epoch: 11 [batch #700, batch_size 16, seq length 915]\tLoss: 0.003031\n","723it [00:11, 52.44it/s]Train epoch: 11 [batch #725, batch_size 16, seq length 929]\tLoss: 0.003163\n","747it [00:12, 52.16it/s]Train epoch: 11 [batch #750, batch_size 16, seq length 942]\tLoss: 0.003358\n","771it [00:12, 48.76it/s]Train epoch: 11 [batch #775, batch_size 16, seq length 954]\tLoss: 0.003258\n","797it [00:13, 48.31it/s]Train epoch: 11 [batch #800, batch_size 16, seq length 968]\tLoss: 0.003341\n","825it [00:14, 48.58it/s]Train epoch: 11 [batch #825, batch_size 16, seq length 981]\tLoss: 0.003249\n","850it [00:14, 46.93it/s]Train epoch: 11 [batch #850, batch_size 16, seq length 994]\tLoss: 0.003334\n","875it [00:15, 46.46it/s]Train epoch: 11 [batch #875, batch_size 16, seq length 1006]\tLoss: 0.003226\n","900it [00:15, 47.93it/s]Train epoch: 11 [batch #900, batch_size 16, seq length 1020]\tLoss: 0.003506\n","925it [00:16, 45.21it/s]Train epoch: 11 [batch #925, batch_size 16, seq length 1033]\tLoss: 0.003231\n","950it [00:16, 45.09it/s]Train epoch: 11 [batch #950, batch_size 16, seq length 1046]\tLoss: 0.003490\n","975it [00:17, 44.53it/s]Train epoch: 11 [batch #975, batch_size 16, seq length 1059]\tLoss: 0.003367\n","1000it [00:17, 44.29it/s]Train epoch: 11 [batch #1000, batch_size 16, seq length 1073]\tLoss: 0.003382\n","1025it [00:18, 45.32it/s]Train epoch: 11 [batch #1025, batch_size 16, seq length 1085]\tLoss: 0.003558\n","1050it [00:18, 44.66it/s]Train epoch: 11 [batch #1050, batch_size 16, seq length 1099]\tLoss: 0.003576\n","1075it [00:19, 44.20it/s]Train epoch: 11 [batch #1075, batch_size 16, seq length 1113]\tLoss: 0.003561\n","1100it [00:20, 43.99it/s]Train epoch: 11 [batch #1100, batch_size 16, seq length 1126]\tLoss: 0.003501\n","1125it [00:20, 42.92it/s]Train epoch: 11 [batch #1125, batch_size 16, seq length 1140]\tLoss: 0.003667\n","1150it [00:21, 41.41it/s]Train epoch: 11 [batch #1150, batch_size 16, seq length 1154]\tLoss: 0.003556\n","1175it [00:21, 41.94it/s]Train epoch: 11 [batch #1175, batch_size 16, seq length 1167]\tLoss: 0.003710\n","1200it [00:22, 41.42it/s]Train epoch: 11 [batch #1200, batch_size 16, seq length 1180]\tLoss: 0.003497\n","1225it [00:23, 41.00it/s]Train epoch: 11 [batch #1225, batch_size 16, seq length 1195]\tLoss: 0.003574\n","1249it [00:23, 39.70it/s]Train epoch: 11 [batch #1250, batch_size 16, seq length 1208]\tLoss: 0.003440\n","1274it [00:24, 40.20it/s]Train epoch: 11 [batch #1275, batch_size 16, seq length 1220]\tLoss: 0.003513\n","1299it [00:24, 40.50it/s]Train epoch: 11 [batch #1300, batch_size 16, seq length 1234]\tLoss: 0.003652\n","1324it [00:25, 39.88it/s]Train epoch: 11 [batch #1325, batch_size 16, seq length 1248]\tLoss: 0.003560\n","1349it [00:26, 40.39it/s]Train epoch: 11 [batch #1350, batch_size 16, seq length 1262]\tLoss: 0.003812\n","1372it [00:26, 39.27it/s]Train epoch: 11 [batch #1375, batch_size 16, seq length 1276]\tLoss: 0.004062\n","1398it [00:27, 38.30it/s]Train epoch: 11 [batch #1400, batch_size 16, seq length 1290]\tLoss: 0.003992\n","1422it [00:28, 38.26it/s]Train epoch: 11 [batch #1425, batch_size 16, seq length 1304]\tLoss: 0.004096\n","1450it [00:28, 38.03it/s]Train epoch: 11 [batch #1450, batch_size 16, seq length 1319]\tLoss: 0.003878\n","1474it [00:29, 38.08it/s]Train epoch: 11 [batch #1475, batch_size 16, seq length 1332]\tLoss: 0.004014\n","1498it [00:30, 38.21it/s]Train epoch: 11 [batch #1500, batch_size 16, seq length 1346]\tLoss: 0.003896\n","1522it [00:30, 37.36it/s]Train epoch: 11 [batch #1525, batch_size 16, seq length 1362]\tLoss: 0.003526\n","1550it [00:31, 37.47it/s]Train epoch: 11 [batch #1550, batch_size 16, seq length 1377]\tLoss: 0.004065\n","1574it [00:32, 36.41it/s]Train epoch: 11 [batch #1575, batch_size 16, seq length 1392]\tLoss: 0.003505\n","1598it [00:32, 36.77it/s]Train epoch: 11 [batch #1600, batch_size 16, seq length 1406]\tLoss: 0.003680\n","1622it [00:33, 36.11it/s]Train epoch: 11 [batch #1625, batch_size 16, seq length 1422]\tLoss: 0.004110\n","1650it [00:34, 36.30it/s]Train epoch: 11 [batch #1650, batch_size 16, seq length 1437]\tLoss: 0.004009\n","1674it [00:34, 36.36it/s]Train epoch: 11 [batch #1675, batch_size 16, seq length 1454]\tLoss: 0.003997\n","1698it [00:35, 35.43it/s]Train epoch: 11 [batch #1700, batch_size 16, seq length 1469]\tLoss: 0.004339\n","1722it [00:36, 35.96it/s]Train epoch: 11 [batch #1725, batch_size 16, seq length 1484]\tLoss: 0.003922\n","1750it [00:37, 35.44it/s]Train epoch: 11 [batch #1750, batch_size 16, seq length 1501]\tLoss: 0.003788\n","1774it [00:37, 34.59it/s]Train epoch: 11 [batch #1775, batch_size 16, seq length 1518]\tLoss: 0.004248\n","1798it [00:38, 33.84it/s]Train epoch: 11 [batch #1800, batch_size 16, seq length 1533]\tLoss: 0.004215\n","1822it [00:39, 33.71it/s]Train epoch: 11 [batch #1825, batch_size 16, seq length 1548]\tLoss: 0.004273\n","1850it [00:39, 33.92it/s]Train epoch: 11 [batch #1850, batch_size 16, seq length 1566]\tLoss: 0.003995\n","1874it [00:40, 33.09it/s]Train epoch: 11 [batch #1875, batch_size 16, seq length 1582]\tLoss: 0.004237\n","1898it [00:41, 33.02it/s]Train epoch: 11 [batch #1900, batch_size 16, seq length 1599]\tLoss: 0.004467\n","1922it [00:42, 32.95it/s]Train epoch: 11 [batch #1925, batch_size 16, seq length 1616]\tLoss: 0.004227\n","1950it [00:43, 32.29it/s]Train epoch: 11 [batch #1950, batch_size 16, seq length 1633]\tLoss: 0.004294\n","1974it [00:43, 32.32it/s]Train epoch: 11 [batch #1975, batch_size 16, seq length 1651]\tLoss: 0.004188\n","1998it [00:44, 31.64it/s]Train epoch: 11 [batch #2000, batch_size 16, seq length 1670]\tLoss: 0.004109\n","2022it [00:45, 30.43it/s]Train epoch: 11 [batch #2025, batch_size 16, seq length 1688]\tLoss: 0.004212\n","2050it [00:46, 30.96it/s]Train epoch: 11 [batch #2050, batch_size 16, seq length 1707]\tLoss: 0.004289\n","2074it [00:47, 30.90it/s]Train epoch: 11 [batch #2075, batch_size 16, seq length 1728]\tLoss: 0.004503\n","2098it [00:47, 31.27it/s]Train epoch: 11 [batch #2100, batch_size 16, seq length 1748]\tLoss: 0.004146\n","2122it [00:48, 30.77it/s]Train epoch: 11 [batch #2125, batch_size 16, seq length 1768]\tLoss: 0.004727\n","2150it [00:49, 30.35it/s]Train epoch: 11 [batch #2150, batch_size 16, seq length 1787]\tLoss: 0.004168\n","2174it [00:50, 30.03it/s]Train epoch: 11 [batch #2175, batch_size 16, seq length 1809]\tLoss: 0.004863\n","2198it [00:51, 29.20it/s]Train epoch: 11 [batch #2200, batch_size 16, seq length 1833]\tLoss: 0.004389\n","2223it [00:51, 29.33it/s]Train epoch: 11 [batch #2225, batch_size 16, seq length 1855]\tLoss: 0.004397\n","2249it [00:52, 28.88it/s]Train epoch: 11 [batch #2250, batch_size 16, seq length 1878]\tLoss: 0.004774\n","2275it [00:53, 28.98it/s]Train epoch: 11 [batch #2275, batch_size 16, seq length 1901]\tLoss: 0.004606\n","2299it [00:54, 28.13it/s]Train epoch: 11 [batch #2300, batch_size 16, seq length 1927]\tLoss: 0.004490\n","2323it [00:55, 28.61it/s]Train epoch: 11 [batch #2325, batch_size 16, seq length 1953]\tLoss: 0.004897\n","2350it [00:56, 28.31it/s]Train epoch: 11 [batch #2350, batch_size 16, seq length 1980]\tLoss: 0.004511\n","2374it [00:57, 28.11it/s]Train epoch: 11 [batch #2375, batch_size 16, seq length 2006]\tLoss: 0.004832\n","2398it [00:58, 27.40it/s]Train epoch: 11 [batch #2400, batch_size 16, seq length 2034]\tLoss: 0.004858\n","2425it [00:59, 27.25it/s]Train epoch: 11 [batch #2425, batch_size 16, seq length 2064]\tLoss: 0.004787\n","2449it [01:00, 26.43it/s]Train epoch: 11 [batch #2450, batch_size 16, seq length 2094]\tLoss: 0.004974\n","2473it [01:00, 26.82it/s]Train epoch: 11 [batch #2475, batch_size 16, seq length 2126]\tLoss: 0.005049\n","2500it [01:01, 26.47it/s]Train epoch: 11 [batch #2500, batch_size 16, seq length 2160]\tLoss: 0.005078\n","2524it [01:02, 26.35it/s]Train epoch: 11 [batch #2525, batch_size 16, seq length 2192]\tLoss: 0.005256\n","2548it [01:03, 26.03it/s]Train epoch: 11 [batch #2550, batch_size 16, seq length 2228]\tLoss: 0.005028\n","2575it [01:04, 25.43it/s]Train epoch: 11 [batch #2575, batch_size 16, seq length 2266]\tLoss: 0.005499\n","2599it [01:05, 25.19it/s]Train epoch: 11 [batch #2600, batch_size 16, seq length 2307]\tLoss: 0.005439\n","2623it [01:06, 24.23it/s]Train epoch: 11 [batch #2625, batch_size 16, seq length 2350]\tLoss: 0.005221\n","2650it [01:07, 23.95it/s]Train epoch: 11 [batch #2650, batch_size 16, seq length 2392]\tLoss: 0.005278\n","2674it [01:08, 24.00it/s]Train epoch: 11 [batch #2675, batch_size 16, seq length 2439]\tLoss: 0.005548\n","2698it [01:10, 22.32it/s]Train epoch: 11 [batch #2700, batch_size 16, seq length 2491]\tLoss: 0.005533\n","2725it [01:11, 23.10it/s]Train epoch: 11 [batch #2725, batch_size 16, seq length 2500]\tLoss: 0.005723\n","2749it [01:12, 23.16it/s]Train epoch: 11 [batch #2750, batch_size 16, seq length 2500]\tLoss: 0.005790\n","2773it [01:13, 22.36it/s]Train epoch: 11 [batch #2775, batch_size 16, seq length 2500]\tLoss: 0.005978\n","2800it [01:14, 22.84it/s]Train epoch: 11 [batch #2800, batch_size 16, seq length 2500]\tLoss: 0.005897\n","2824it [01:15, 22.64it/s]Train epoch: 11 [batch #2825, batch_size 16, seq length 2500]\tLoss: 0.005914\n","2848it [01:16, 22.14it/s]Train epoch: 11 [batch #2850, batch_size 16, seq length 2500]\tLoss: 0.006108\n","2875it [01:17, 21.55it/s]Train epoch: 11 [batch #2875, batch_size 16, seq length 2500]\tLoss: 0.006735\n","2899it [01:19, 21.46it/s]Train epoch: 11 [batch #2900, batch_size 16, seq length 2500]\tLoss: 0.006809\n","2923it [01:20, 20.98it/s]Train epoch: 11 [batch #2925, batch_size 16, seq length 2500]\tLoss: 0.007222\n","2950it [01:21, 19.86it/s]Train epoch: 11 [batch #2950, batch_size 16, seq length 2500]\tLoss: 0.007299\n","2975it [01:22, 18.04it/s]Train epoch: 11 [batch #2975, batch_size 16, seq length 2500]\tLoss: 0.008441\n","2983it [01:23, 35.77it/s]\n","epoch loss: 0.0041058014844070605\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:03, 435.44it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0278, 0.0466, 0.0397, 0.0429, 0.8240\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2839, 0.5269, 0.3810, 0.4422, 0.9702\n","rec_at_8: 0.3195\n","prec_at_8: 0.5915\n","rec_at_15: 0.4434\n","prec_at_15: 0.4569\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/cnn_vanilla_May_02_220247\n","\n","EPOCH 12\n","0it [00:00, ?it/s]Train epoch: 12 [batch #0, batch_size 16, seq length 117]\tLoss: 0.005496\n","22it [00:00, 73.38it/s]Train epoch: 12 [batch #25, batch_size 16, seq length 337]\tLoss: 0.003550\n","48it [00:00, 81.18it/s]Train epoch: 12 [batch #50, batch_size 16, seq length 402]\tLoss: 0.003005\n","75it [00:00, 79.75it/s]Train epoch: 12 [batch #75, batch_size 16, seq length 452]\tLoss: 0.002669\n","99it [00:01, 77.14it/s]Train epoch: 12 [batch #100, batch_size 16, seq length 490]\tLoss: 0.002561\n","123it [00:01, 75.06it/s]Train epoch: 12 [batch #125, batch_size 16, seq length 520]\tLoss: 0.002849\n","147it [00:01, 70.37it/s]Train epoch: 12 [batch #150, batch_size 16, seq length 548]\tLoss: 0.002950\n","169it [00:02, 68.05it/s]Train epoch: 12 [batch #175, batch_size 16, seq length 574]\tLoss: 0.002671\n","197it [00:02, 67.30it/s]Train epoch: 12 [batch #200, batch_size 16, seq length 596]\tLoss: 0.002885\n","225it [00:03, 64.38it/s]Train epoch: 12 [batch #225, batch_size 16, seq length 618]\tLoss: 0.002818\n","246it [00:03, 64.81it/s]Train epoch: 12 [batch #250, batch_size 16, seq length 638]\tLoss: 0.002695\n","274it [00:03, 62.64it/s]Train epoch: 12 [batch #275, batch_size 16, seq length 656]\tLoss: 0.002972\n","295it [00:04, 62.07it/s]Train epoch: 12 [batch #300, batch_size 16, seq length 674]\tLoss: 0.003025\n","323it [00:04, 61.88it/s]Train epoch: 12 [batch #325, batch_size 16, seq length 693]\tLoss: 0.002783\n","344it [00:05, 59.99it/s]Train epoch: 12 [batch #350, batch_size 16, seq length 710]\tLoss: 0.002952\n","372it [00:05, 60.36it/s]Train epoch: 12 [batch #375, batch_size 16, seq length 726]\tLoss: 0.003319\n","398it [00:05, 58.35it/s]Train epoch: 12 [batch #400, batch_size 16, seq length 743]\tLoss: 0.002810\n","424it [00:06, 58.63it/s]Train epoch: 12 [batch #425, batch_size 16, seq length 758]\tLoss: 0.003009\n","449it [00:06, 58.14it/s]Train epoch: 12 [batch #450, batch_size 16, seq length 773]\tLoss: 0.003177\n","473it [00:07, 53.36it/s]Train epoch: 12 [batch #475, batch_size 16, seq length 789]\tLoss: 0.002855\n","497it [00:07, 54.11it/s]Train epoch: 12 [batch #500, batch_size 16, seq length 803]\tLoss: 0.003107\n","521it [00:08, 54.56it/s]Train epoch: 12 [batch #525, batch_size 16, seq length 818]\tLoss: 0.003033\n","545it [00:08, 54.64it/s]Train epoch: 12 [batch #550, batch_size 16, seq length 833]\tLoss: 0.002986\n","575it [00:09, 53.43it/s]Train epoch: 12 [batch #575, batch_size 16, seq length 846]\tLoss: 0.002911\n","599it [00:09, 54.33it/s]Train epoch: 12 [batch #600, batch_size 16, seq length 860]\tLoss: 0.003399\n","623it [00:10, 53.86it/s]Train epoch: 12 [batch #625, batch_size 16, seq length 874]\tLoss: 0.003198\n","647it [00:10, 53.19it/s]Train epoch: 12 [batch #650, batch_size 16, seq length 887]\tLoss: 0.002970\n","671it [00:10, 52.94it/s]Train epoch: 12 [batch #675, batch_size 16, seq length 901]\tLoss: 0.003019\n","695it [00:11, 51.98it/s]Train epoch: 12 [batch #700, batch_size 16, seq length 915]\tLoss: 0.002903\n","725it [00:12, 51.18it/s]Train epoch: 12 [batch #725, batch_size 16, seq length 929]\tLoss: 0.003088\n","749it [00:12, 51.34it/s]Train epoch: 12 [batch #750, batch_size 16, seq length 942]\tLoss: 0.003313\n","773it [00:12, 49.99it/s]Train epoch: 12 [batch #775, batch_size 16, seq length 954]\tLoss: 0.003217\n","800it [00:13, 48.99it/s]Train epoch: 12 [batch #800, batch_size 16, seq length 968]\tLoss: 0.003279\n","822it [00:13, 48.01it/s]Train epoch: 12 [batch #825, batch_size 16, seq length 981]\tLoss: 0.003202\n","850it [00:14, 47.95it/s]Train epoch: 12 [batch #850, batch_size 16, seq length 994]\tLoss: 0.003340\n","871it [00:14, 48.85it/s]Train epoch: 12 [batch #875, batch_size 16, seq length 1006]\tLoss: 0.003166\n","896it [00:15, 46.26it/s]Train epoch: 12 [batch #900, batch_size 16, seq length 1020]\tLoss: 0.003437\n","921it [00:16, 45.06it/s]Train epoch: 12 [batch #925, batch_size 16, seq length 1033]\tLoss: 0.003174\n","946it [00:16, 45.21it/s]Train epoch: 12 [batch #950, batch_size 16, seq length 1046]\tLoss: 0.003391\n","971it [00:17, 44.75it/s]Train epoch: 12 [batch #975, batch_size 16, seq length 1059]\tLoss: 0.003358\n","996it [00:17, 44.57it/s]Train epoch: 12 [batch #1000, batch_size 16, seq length 1073]\tLoss: 0.003445\n","1021it [00:18, 44.08it/s]Train epoch: 12 [batch #1025, batch_size 16, seq length 1085]\tLoss: 0.003557\n","1046it [00:18, 43.80it/s]Train epoch: 12 [batch #1050, batch_size 16, seq length 1099]\tLoss: 0.003553\n","1071it [00:19, 42.95it/s]Train epoch: 12 [batch #1075, batch_size 16, seq length 1113]\tLoss: 0.003526\n","1096it [00:20, 43.27it/s]Train epoch: 12 [batch #1100, batch_size 16, seq length 1126]\tLoss: 0.003485\n","1121it [00:20, 43.19it/s]Train epoch: 12 [batch #1125, batch_size 16, seq length 1140]\tLoss: 0.003647\n","1146it [00:21, 42.31it/s]Train epoch: 12 [batch #1150, batch_size 16, seq length 1154]\tLoss: 0.003503\n","1171it [00:21, 41.37it/s]Train epoch: 12 [batch #1175, batch_size 16, seq length 1167]\tLoss: 0.003660\n","1196it [00:22, 41.04it/s]Train epoch: 12 [batch #1200, batch_size 16, seq length 1180]\tLoss: 0.003483\n","1221it [00:23, 41.85it/s]Train epoch: 12 [batch #1225, batch_size 16, seq length 1195]\tLoss: 0.003538\n","1246it [00:23, 40.05it/s]Train epoch: 12 [batch #1250, batch_size 16, seq length 1208]\tLoss: 0.003469\n","1271it [00:24, 40.76it/s]Train epoch: 12 [batch #1275, batch_size 16, seq length 1220]\tLoss: 0.003458\n","1296it [00:24, 40.50it/s]Train epoch: 12 [batch #1300, batch_size 16, seq length 1234]\tLoss: 0.003576\n","1321it [00:25, 39.56it/s]Train epoch: 12 [batch #1325, batch_size 16, seq length 1248]\tLoss: 0.003506\n","1346it [00:26, 40.41it/s]Train epoch: 12 [batch #1350, batch_size 16, seq length 1262]\tLoss: 0.003763\n","1375it [00:26, 39.21it/s]Train epoch: 12 [batch #1375, batch_size 16, seq length 1276]\tLoss: 0.004006\n","1400it [00:27, 38.00it/s]Train epoch: 12 [batch #1400, batch_size 16, seq length 1290]\tLoss: 0.003995\n","1424it [00:28, 37.88it/s]Train epoch: 12 [batch #1425, batch_size 16, seq length 1304]\tLoss: 0.004039\n","1448it [00:28, 37.70it/s]Train epoch: 12 [batch #1450, batch_size 16, seq length 1319]\tLoss: 0.003790\n","1472it [00:29, 37.65it/s]Train epoch: 12 [batch #1475, batch_size 16, seq length 1332]\tLoss: 0.004014\n","1500it [00:30, 36.88it/s]Train epoch: 12 [batch #1500, batch_size 16, seq length 1346]\tLoss: 0.003854\n","1524it [00:30, 36.58it/s]Train epoch: 12 [batch #1525, batch_size 16, seq length 1362]\tLoss: 0.003432\n","1548it [00:31, 37.18it/s]Train epoch: 12 [batch #1550, batch_size 16, seq length 1377]\tLoss: 0.004014\n","1572it [00:32, 37.17it/s]Train epoch: 12 [batch #1575, batch_size 16, seq length 1392]\tLoss: 0.003461\n","1600it [00:32, 36.23it/s]Train epoch: 12 [batch #1600, batch_size 16, seq length 1406]\tLoss: 0.003598\n","1624it [00:33, 36.32it/s]Train epoch: 12 [batch #1625, batch_size 16, seq length 1422]\tLoss: 0.004325\n","1648it [00:34, 37.01it/s]Train epoch: 12 [batch #1650, batch_size 16, seq length 1437]\tLoss: 0.004002\n","1672it [00:34, 36.12it/s]Train epoch: 12 [batch #1675, batch_size 16, seq length 1454]\tLoss: 0.004016\n","1700it [00:35, 35.66it/s]Train epoch: 12 [batch #1700, batch_size 16, seq length 1469]\tLoss: 0.004306\n","1724it [00:36, 35.78it/s]Train epoch: 12 [batch #1725, batch_size 16, seq length 1484]\tLoss: 0.003866\n","1748it [00:37, 35.28it/s]Train epoch: 12 [batch #1750, batch_size 16, seq length 1501]\tLoss: 0.003769\n","1772it [00:37, 34.34it/s]Train epoch: 12 [batch #1775, batch_size 16, seq length 1518]\tLoss: 0.004212\n","1800it [00:38, 33.65it/s]Train epoch: 12 [batch #1800, batch_size 16, seq length 1533]\tLoss: 0.004211\n","1824it [00:39, 33.60it/s]Train epoch: 12 [batch #1825, batch_size 16, seq length 1548]\tLoss: 0.004279\n","1848it [00:39, 33.56it/s]Train epoch: 12 [batch #1850, batch_size 16, seq length 1566]\tLoss: 0.003932\n","1872it [00:40, 33.32it/s]Train epoch: 12 [batch #1875, batch_size 16, seq length 1582]\tLoss: 0.004163\n","1900it [00:41, 33.22it/s]Train epoch: 12 [batch #1900, batch_size 16, seq length 1599]\tLoss: 0.004537\n","1924it [00:42, 33.11it/s]Train epoch: 12 [batch #1925, batch_size 16, seq length 1616]\tLoss: 0.004225\n","1948it [00:42, 32.80it/s]Train epoch: 12 [batch #1950, batch_size 16, seq length 1633]\tLoss: 0.004236\n","1972it [00:43, 32.01it/s]Train epoch: 12 [batch #1975, batch_size 16, seq length 1651]\tLoss: 0.004180\n","2000it [00:44, 32.33it/s]Train epoch: 12 [batch #2000, batch_size 16, seq length 1670]\tLoss: 0.004011\n","2024it [00:45, 32.07it/s]Train epoch: 12 [batch #2025, batch_size 16, seq length 1688]\tLoss: 0.004168\n","2048it [00:46, 32.07it/s]Train epoch: 12 [batch #2050, batch_size 16, seq length 1707]\tLoss: 0.004196\n","2072it [00:46, 31.07it/s]Train epoch: 12 [batch #2075, batch_size 16, seq length 1728]\tLoss: 0.004405\n","2100it [00:47, 30.61it/s]Train epoch: 12 [batch #2100, batch_size 16, seq length 1748]\tLoss: 0.004077\n","2124it [00:48, 30.84it/s]Train epoch: 12 [batch #2125, batch_size 16, seq length 1768]\tLoss: 0.004611\n","2148it [00:49, 30.56it/s]Train epoch: 12 [batch #2150, batch_size 16, seq length 1787]\tLoss: 0.004168\n","2172it [00:50, 29.80it/s]Train epoch: 12 [batch #2175, batch_size 16, seq length 1809]\tLoss: 0.004712\n","2199it [00:51, 29.57it/s]Train epoch: 12 [batch #2200, batch_size 16, seq length 1833]\tLoss: 0.004364\n","2225it [00:51, 29.73it/s]Train epoch: 12 [batch #2225, batch_size 16, seq length 1855]\tLoss: 0.004366\n","2249it [00:52, 29.68it/s]Train epoch: 12 [batch #2250, batch_size 16, seq length 1878]\tLoss: 0.004727\n","2273it [00:53, 28.30it/s]Train epoch: 12 [batch #2275, batch_size 16, seq length 1901]\tLoss: 0.004664\n","2299it [00:54, 28.63it/s]Train epoch: 12 [batch #2300, batch_size 16, seq length 1927]\tLoss: 0.004470\n","2323it [00:55, 28.46it/s]Train epoch: 12 [batch #2325, batch_size 16, seq length 1953]\tLoss: 0.004859\n","2350it [00:56, 28.17it/s]Train epoch: 12 [batch #2350, batch_size 16, seq length 1980]\tLoss: 0.004537\n","2374it [00:57, 28.47it/s]Train epoch: 12 [batch #2375, batch_size 16, seq length 2006]\tLoss: 0.004810\n","2398it [00:57, 27.86it/s]Train epoch: 12 [batch #2400, batch_size 16, seq length 2034]\tLoss: 0.004808\n","2425it [00:58, 27.38it/s]Train epoch: 12 [batch #2425, batch_size 16, seq length 2064]\tLoss: 0.004669\n","2449it [00:59, 26.54it/s]Train epoch: 12 [batch #2450, batch_size 16, seq length 2094]\tLoss: 0.004976\n","2473it [01:00, 27.01it/s]Train epoch: 12 [batch #2475, batch_size 16, seq length 2126]\tLoss: 0.005008\n","2500it [01:01, 26.26it/s]Train epoch: 12 [batch #2500, batch_size 16, seq length 2160]\tLoss: 0.004975\n","2524it [01:02, 26.26it/s]Train epoch: 12 [batch #2525, batch_size 16, seq length 2192]\tLoss: 0.005191\n","2548it [01:03, 26.16it/s]Train epoch: 12 [batch #2550, batch_size 16, seq length 2228]\tLoss: 0.005059\n","2575it [01:04, 25.76it/s]Train epoch: 12 [batch #2575, batch_size 16, seq length 2266]\tLoss: 0.005444\n","2599it [01:05, 24.50it/s]Train epoch: 12 [batch #2600, batch_size 16, seq length 2307]\tLoss: 0.005302\n","2623it [01:06, 24.48it/s]Train epoch: 12 [batch #2625, batch_size 16, seq length 2350]\tLoss: 0.005209\n","2650it [01:07, 24.61it/s]Train epoch: 12 [batch #2650, batch_size 16, seq length 2392]\tLoss: 0.005225\n","2674it [01:08, 23.88it/s]Train epoch: 12 [batch #2675, batch_size 16, seq length 2439]\tLoss: 0.005446\n","2698it [01:09, 23.30it/s]Train epoch: 12 [batch #2700, batch_size 16, seq length 2491]\tLoss: 0.005451\n","2725it [01:10, 22.97it/s]Train epoch: 12 [batch #2725, batch_size 16, seq length 2500]\tLoss: 0.005717\n","2749it [01:11, 22.81it/s]Train epoch: 12 [batch #2750, batch_size 16, seq length 2500]\tLoss: 0.005753\n","2773it [01:12, 23.35it/s]Train epoch: 12 [batch #2775, batch_size 16, seq length 2500]\tLoss: 0.005949\n","2800it [01:14, 22.62it/s]Train epoch: 12 [batch #2800, batch_size 16, seq length 2500]\tLoss: 0.005742\n","2824it [01:15, 22.42it/s]Train epoch: 12 [batch #2825, batch_size 16, seq length 2500]\tLoss: 0.005752\n","2848it [01:16, 22.48it/s]Train epoch: 12 [batch #2850, batch_size 16, seq length 2500]\tLoss: 0.005989\n","2875it [01:17, 22.16it/s]Train epoch: 12 [batch #2875, batch_size 16, seq length 2500]\tLoss: 0.006729\n","2899it [01:18, 20.85it/s]Train epoch: 12 [batch #2900, batch_size 16, seq length 2500]\tLoss: 0.006679\n","2923it [01:19, 20.69it/s]Train epoch: 12 [batch #2925, batch_size 16, seq length 2500]\tLoss: 0.007172\n","2950it [01:21, 20.34it/s]Train epoch: 12 [batch #2950, batch_size 16, seq length 2500]\tLoss: 0.007088\n","2974it [01:22, 18.22it/s]Train epoch: 12 [batch #2975, batch_size 16, seq length 2500]\tLoss: 0.008428\n","2983it [01:22, 35.94it/s]\n","epoch loss: 0.0040616054056890425\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:03, 441.82it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0280, 0.0457, 0.0415, 0.0435, 0.8218\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2771, 0.4917, 0.3884, 0.4339, 0.9698\n","rec_at_8: 0.3152\n","prec_at_8: 0.5791\n","rec_at_15: 0.4338\n","prec_at_15: 0.4466\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/cnn_vanilla_May_02_220247\n","\n","EPOCH 13\n","0it [00:00, ?it/s]Train epoch: 13 [batch #0, batch_size 16, seq length 117]\tLoss: 0.005650\n","22it [00:00, 68.34it/s]Train epoch: 13 [batch #25, batch_size 16, seq length 337]\tLoss: 0.003497\n","48it [00:00, 77.71it/s]Train epoch: 13 [batch #50, batch_size 16, seq length 402]\tLoss: 0.002939\n","73it [00:00, 78.75it/s]Train epoch: 13 [batch #75, batch_size 16, seq length 452]\tLoss: 0.002966\n","97it [00:01, 78.20it/s]Train epoch: 13 [batch #100, batch_size 16, seq length 490]\tLoss: 0.002561\n","121it [00:01, 76.15it/s]Train epoch: 13 [batch #125, batch_size 16, seq length 520]\tLoss: 0.002883\n","145it [00:01, 71.78it/s]Train epoch: 13 [batch #150, batch_size 16, seq length 548]\tLoss: 0.002909\n","175it [00:02, 68.91it/s]Train epoch: 13 [batch #175, batch_size 16, seq length 574]\tLoss: 0.002585\n","196it [00:02, 67.06it/s]Train epoch: 13 [batch #200, batch_size 16, seq length 596]\tLoss: 0.002837\n","224it [00:03, 66.89it/s]Train epoch: 13 [batch #225, batch_size 16, seq length 618]\tLoss: 0.002811\n","245it [00:03, 66.45it/s]Train epoch: 13 [batch #250, batch_size 16, seq length 638]\tLoss: 0.002647\n","273it [00:03, 63.09it/s]Train epoch: 13 [batch #275, batch_size 16, seq length 656]\tLoss: 0.002933\n","294it [00:04, 63.27it/s]Train epoch: 13 [batch #300, batch_size 16, seq length 674]\tLoss: 0.003008\n","322it [00:04, 61.81it/s]Train epoch: 13 [batch #325, batch_size 16, seq length 693]\tLoss: 0.002787\n","350it [00:05, 60.63it/s]Train epoch: 13 [batch #350, batch_size 16, seq length 710]\tLoss: 0.002919\n","371it [00:05, 60.12it/s]Train epoch: 13 [batch #375, batch_size 16, seq length 726]\tLoss: 0.003271\n","396it [00:05, 58.60it/s]Train epoch: 13 [batch #400, batch_size 16, seq length 743]\tLoss: 0.002783\n","422it [00:06, 58.37it/s]Train epoch: 13 [batch #425, batch_size 16, seq length 758]\tLoss: 0.003021\n","448it [00:06, 58.11it/s]Train epoch: 13 [batch #450, batch_size 16, seq length 773]\tLoss: 0.003113\n","472it [00:07, 57.21it/s]Train epoch: 13 [batch #475, batch_size 16, seq length 789]\tLoss: 0.002810\n","496it [00:07, 54.89it/s]Train epoch: 13 [batch #500, batch_size 16, seq length 803]\tLoss: 0.003086\n","520it [00:08, 54.86it/s]Train epoch: 13 [batch #525, batch_size 16, seq length 818]\tLoss: 0.002996\n","550it [00:08, 54.48it/s]Train epoch: 13 [batch #550, batch_size 16, seq length 833]\tLoss: 0.002839\n","574it [00:09, 52.99it/s]Train epoch: 13 [batch #575, batch_size 16, seq length 846]\tLoss: 0.002926\n","598it [00:09, 53.52it/s]Train epoch: 13 [batch #600, batch_size 16, seq length 860]\tLoss: 0.003293\n","622it [00:10, 52.32it/s]Train epoch: 13 [batch #625, batch_size 16, seq length 874]\tLoss: 0.003156\n","646it [00:10, 53.19it/s]Train epoch: 13 [batch #650, batch_size 16, seq length 887]\tLoss: 0.002960\n","670it [00:10, 54.09it/s]Train epoch: 13 [batch #675, batch_size 16, seq length 901]\tLoss: 0.003014\n","700it [00:11, 52.71it/s]Train epoch: 13 [batch #700, batch_size 16, seq length 915]\tLoss: 0.002818\n","724it [00:11, 52.86it/s]Train epoch: 13 [batch #725, batch_size 16, seq length 929]\tLoss: 0.003026\n","748it [00:12, 52.02it/s]Train epoch: 13 [batch #750, batch_size 16, seq length 942]\tLoss: 0.003267\n","772it [00:12, 49.84it/s]Train epoch: 13 [batch #775, batch_size 16, seq length 954]\tLoss: 0.003153\n","798it [00:13, 49.07it/s]Train epoch: 13 [batch #800, batch_size 16, seq length 968]\tLoss: 0.003230\n","825it [00:13, 49.00it/s]Train epoch: 13 [batch #825, batch_size 16, seq length 981]\tLoss: 0.003173\n","849it [00:14, 49.05it/s]Train epoch: 13 [batch #850, batch_size 16, seq length 994]\tLoss: 0.003346\n","874it [00:14, 46.73it/s]Train epoch: 13 [batch #875, batch_size 16, seq length 1006]\tLoss: 0.003130\n","899it [00:15, 47.07it/s]Train epoch: 13 [batch #900, batch_size 16, seq length 1020]\tLoss: 0.003416\n","924it [00:16, 44.67it/s]Train epoch: 13 [batch #925, batch_size 16, seq length 1033]\tLoss: 0.003171\n","949it [00:16, 44.63it/s]Train epoch: 13 [batch #950, batch_size 16, seq length 1046]\tLoss: 0.003351\n","974it [00:17, 45.06it/s]Train epoch: 13 [batch #975, batch_size 16, seq length 1059]\tLoss: 0.003365\n","999it [00:17, 44.47it/s]Train epoch: 13 [batch #1000, batch_size 16, seq length 1073]\tLoss: 0.003343\n","1024it [00:18, 43.91it/s]Train epoch: 13 [batch #1025, batch_size 16, seq length 1085]\tLoss: 0.003446\n","1049it [00:18, 43.57it/s]Train epoch: 13 [batch #1050, batch_size 16, seq length 1099]\tLoss: 0.003579\n","1074it [00:19, 43.56it/s]Train epoch: 13 [batch #1075, batch_size 16, seq length 1113]\tLoss: 0.003472\n","1099it [00:20, 43.06it/s]Train epoch: 13 [batch #1100, batch_size 16, seq length 1126]\tLoss: 0.003389\n","1124it [00:20, 42.80it/s]Train epoch: 13 [batch #1125, batch_size 16, seq length 1140]\tLoss: 0.003580\n","1149it [00:21, 42.11it/s]Train epoch: 13 [batch #1150, batch_size 16, seq length 1154]\tLoss: 0.003457\n","1174it [00:21, 41.69it/s]Train epoch: 13 [batch #1175, batch_size 16, seq length 1167]\tLoss: 0.003577\n","1199it [00:22, 42.12it/s]Train epoch: 13 [batch #1200, batch_size 16, seq length 1180]\tLoss: 0.003452\n","1224it [00:22, 40.97it/s]Train epoch: 13 [batch #1225, batch_size 16, seq length 1195]\tLoss: 0.003513\n","1249it [00:23, 41.51it/s]Train epoch: 13 [batch #1250, batch_size 16, seq length 1208]\tLoss: 0.003331\n","1274it [00:24, 40.55it/s]Train epoch: 13 [batch #1275, batch_size 16, seq length 1220]\tLoss: 0.003459\n","1299it [00:24, 40.83it/s]Train epoch: 13 [batch #1300, batch_size 16, seq length 1234]\tLoss: 0.003590\n","1324it [00:25, 40.67it/s]Train epoch: 13 [batch #1325, batch_size 16, seq length 1248]\tLoss: 0.003449\n","1349it [00:26, 39.45it/s]Train epoch: 13 [batch #1350, batch_size 16, seq length 1262]\tLoss: 0.003776\n","1372it [00:26, 39.06it/s]Train epoch: 13 [batch #1375, batch_size 16, seq length 1276]\tLoss: 0.003929\n","1397it [00:27, 38.74it/s]Train epoch: 13 [batch #1400, batch_size 16, seq length 1290]\tLoss: 0.003894\n","1425it [00:28, 38.17it/s]Train epoch: 13 [batch #1425, batch_size 16, seq length 1304]\tLoss: 0.004030\n","1449it [00:28, 37.55it/s]Train epoch: 13 [batch #1450, batch_size 16, seq length 1319]\tLoss: 0.003761\n","1473it [00:29, 37.75it/s]Train epoch: 13 [batch #1475, batch_size 16, seq length 1332]\tLoss: 0.003985\n","1497it [00:29, 38.17it/s]Train epoch: 13 [batch #1500, batch_size 16, seq length 1346]\tLoss: 0.003823\n","1525it [00:30, 37.33it/s]Train epoch: 13 [batch #1525, batch_size 16, seq length 1362]\tLoss: 0.003399\n","1549it [00:31, 37.31it/s]Train epoch: 13 [batch #1550, batch_size 16, seq length 1377]\tLoss: 0.003931\n","1573it [00:31, 37.06it/s]Train epoch: 13 [batch #1575, batch_size 16, seq length 1392]\tLoss: 0.003414\n","1597it [00:32, 36.61it/s]Train epoch: 13 [batch #1600, batch_size 16, seq length 1406]\tLoss: 0.003598\n","1625it [00:33, 36.35it/s]Train epoch: 13 [batch #1625, batch_size 16, seq length 1422]\tLoss: 0.004047\n","1649it [00:34, 36.80it/s]Train epoch: 13 [batch #1650, batch_size 16, seq length 1437]\tLoss: 0.003908\n","1673it [00:34, 35.83it/s]Train epoch: 13 [batch #1675, batch_size 16, seq length 1454]\tLoss: 0.003893\n","1697it [00:35, 35.74it/s]Train epoch: 13 [batch #1700, batch_size 16, seq length 1469]\tLoss: 0.004327\n","1725it [00:36, 35.65it/s]Train epoch: 13 [batch #1725, batch_size 16, seq length 1484]\tLoss: 0.003854\n","1749it [00:36, 35.70it/s]Train epoch: 13 [batch #1750, batch_size 16, seq length 1501]\tLoss: 0.003703\n","1773it [00:37, 34.92it/s]Train epoch: 13 [batch #1775, batch_size 16, seq length 1518]\tLoss: 0.004219\n","1797it [00:38, 33.65it/s]Train epoch: 13 [batch #1800, batch_size 16, seq length 1533]\tLoss: 0.004151\n","1825it [00:39, 33.58it/s]Train epoch: 13 [batch #1825, batch_size 16, seq length 1548]\tLoss: 0.004251\n","1849it [00:39, 33.82it/s]Train epoch: 13 [batch #1850, batch_size 16, seq length 1566]\tLoss: 0.003867\n","1873it [00:40, 32.49it/s]Train epoch: 13 [batch #1875, batch_size 16, seq length 1582]\tLoss: 0.004092\n","1897it [00:41, 32.60it/s]Train epoch: 13 [batch #1900, batch_size 16, seq length 1599]\tLoss: 0.004280\n","1925it [00:42, 32.62it/s]Train epoch: 13 [batch #1925, batch_size 16, seq length 1616]\tLoss: 0.004107\n","1949it [00:42, 32.79it/s]Train epoch: 13 [batch #1950, batch_size 16, seq length 1633]\tLoss: 0.004114\n","1973it [00:43, 32.72it/s]Train epoch: 13 [batch #1975, batch_size 16, seq length 1651]\tLoss: 0.004119\n","1997it [00:44, 31.69it/s]Train epoch: 13 [batch #2000, batch_size 16, seq length 1670]\tLoss: 0.004033\n","2025it [00:45, 31.86it/s]Train epoch: 13 [batch #2025, batch_size 16, seq length 1688]\tLoss: 0.004078\n","2049it [00:46, 31.55it/s]Train epoch: 13 [batch #2050, batch_size 16, seq length 1707]\tLoss: 0.004136\n","2073it [00:46, 31.91it/s]Train epoch: 13 [batch #2075, batch_size 16, seq length 1728]\tLoss: 0.004455\n","2097it [00:47, 31.27it/s]Train epoch: 13 [batch #2100, batch_size 16, seq length 1748]\tLoss: 0.004090\n","2125it [00:48, 31.00it/s]Train epoch: 13 [batch #2125, batch_size 16, seq length 1768]\tLoss: 0.004595\n","2149it [00:49, 29.99it/s]Train epoch: 13 [batch #2150, batch_size 16, seq length 1787]\tLoss: 0.004150\n","2173it [00:50, 29.89it/s]Train epoch: 13 [batch #2175, batch_size 16, seq length 1809]\tLoss: 0.004743\n","2198it [00:50, 28.72it/s]Train epoch: 13 [batch #2200, batch_size 16, seq length 1833]\tLoss: 0.004335\n","2225it [00:51, 29.05it/s]Train epoch: 13 [batch #2225, batch_size 16, seq length 1855]\tLoss: 0.004312\n","2249it [00:52, 28.76it/s]Train epoch: 13 [batch #2250, batch_size 16, seq length 1878]\tLoss: 0.004697\n","2275it [00:53, 28.96it/s]Train epoch: 13 [batch #2275, batch_size 16, seq length 1901]\tLoss: 0.004550\n","2299it [00:54, 28.96it/s]Train epoch: 13 [batch #2300, batch_size 16, seq length 1927]\tLoss: 0.004446\n","2323it [00:55, 28.61it/s]Train epoch: 13 [batch #2325, batch_size 16, seq length 1953]\tLoss: 0.004825\n","2350it [00:56, 28.50it/s]Train epoch: 13 [batch #2350, batch_size 16, seq length 1980]\tLoss: 0.004533\n","2374it [00:57, 28.21it/s]Train epoch: 13 [batch #2375, batch_size 16, seq length 2006]\tLoss: 0.004739\n","2398it [00:57, 27.21it/s]Train epoch: 13 [batch #2400, batch_size 16, seq length 2034]\tLoss: 0.004767\n","2425it [00:58, 26.83it/s]Train epoch: 13 [batch #2425, batch_size 16, seq length 2064]\tLoss: 0.004694\n","2449it [00:59, 26.80it/s]Train epoch: 13 [batch #2450, batch_size 16, seq length 2094]\tLoss: 0.004848\n","2473it [01:00, 26.51it/s]Train epoch: 13 [batch #2475, batch_size 16, seq length 2126]\tLoss: 0.004940\n","2500it [01:01, 26.39it/s]Train epoch: 13 [batch #2500, batch_size 16, seq length 2160]\tLoss: 0.004866\n","2524it [01:02, 25.91it/s]Train epoch: 13 [batch #2525, batch_size 16, seq length 2192]\tLoss: 0.005185\n","2548it [01:03, 25.74it/s]Train epoch: 13 [batch #2550, batch_size 16, seq length 2228]\tLoss: 0.004901\n","2575it [01:04, 25.98it/s]Train epoch: 13 [batch #2575, batch_size 16, seq length 2266]\tLoss: 0.005386\n","2599it [01:05, 24.89it/s]Train epoch: 13 [batch #2600, batch_size 16, seq length 2307]\tLoss: 0.005289\n","2623it [01:06, 24.82it/s]Train epoch: 13 [batch #2625, batch_size 16, seq length 2350]\tLoss: 0.005147\n","2650it [01:07, 24.11it/s]Train epoch: 13 [batch #2650, batch_size 16, seq length 2392]\tLoss: 0.005183\n","2674it [01:08, 23.12it/s]Train epoch: 13 [batch #2675, batch_size 16, seq length 2439]\tLoss: 0.005433\n","2698it [01:09, 23.51it/s]Train epoch: 13 [batch #2700, batch_size 16, seq length 2491]\tLoss: 0.005436\n","2725it [01:10, 22.73it/s]Train epoch: 13 [batch #2725, batch_size 16, seq length 2500]\tLoss: 0.005655\n","2749it [01:11, 22.43it/s]Train epoch: 13 [batch #2750, batch_size 16, seq length 2500]\tLoss: 0.005626\n","2773it [01:13, 22.55it/s]Train epoch: 13 [batch #2775, batch_size 16, seq length 2500]\tLoss: 0.005931\n","2800it [01:14, 22.63it/s]Train epoch: 13 [batch #2800, batch_size 16, seq length 2500]\tLoss: 0.005834\n","2824it [01:15, 22.53it/s]Train epoch: 13 [batch #2825, batch_size 16, seq length 2500]\tLoss: 0.005839\n","2848it [01:16, 22.56it/s]Train epoch: 13 [batch #2850, batch_size 16, seq length 2500]\tLoss: 0.005976\n","2875it [01:17, 21.85it/s]Train epoch: 13 [batch #2875, batch_size 16, seq length 2500]\tLoss: 0.006598\n","2899it [01:18, 21.58it/s]Train epoch: 13 [batch #2900, batch_size 16, seq length 2500]\tLoss: 0.006703\n","2923it [01:19, 20.56it/s]Train epoch: 13 [batch #2925, batch_size 16, seq length 2500]\tLoss: 0.007091\n","2949it [01:21, 19.93it/s]Train epoch: 13 [batch #2950, batch_size 16, seq length 2500]\tLoss: 0.007024\n","2974it [01:22, 18.21it/s]Train epoch: 13 [batch #2975, batch_size 16, seq length 2500]\tLoss: 0.008278\n","2983it [01:23, 35.93it/s]\n","epoch loss: 0.004025384683525386\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:03, 441.11it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0272, 0.0446, 0.0420, 0.0433, 0.8213\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2774, 0.4936, 0.3878, 0.4343, 0.9691\n","rec_at_8: 0.3137\n","prec_at_8: 0.5785\n","rec_at_15: 0.4356\n","prec_at_15: 0.4475\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/cnn_vanilla_May_02_220247\n","\n","EPOCH 14\n","0it [00:00, ?it/s]Train epoch: 14 [batch #0, batch_size 16, seq length 117]\tLoss: 0.005507\n","22it [00:00, 70.26it/s]Train epoch: 14 [batch #25, batch_size 16, seq length 337]\tLoss: 0.003528\n","49it [00:00, 80.15it/s]Train epoch: 14 [batch #50, batch_size 16, seq length 402]\tLoss: 0.002923\n","67it [00:00, 80.70it/s]Train epoch: 14 [batch #75, batch_size 16, seq length 452]\tLoss: 0.002620\n","100it [00:01, 77.51it/s]Train epoch: 14 [batch #100, batch_size 16, seq length 490]\tLoss: 0.002459\n","124it [00:01, 75.36it/s]Train epoch: 14 [batch #125, batch_size 16, seq length 520]\tLoss: 0.002843\n","148it [00:01, 70.69it/s]Train epoch: 14 [batch #150, batch_size 16, seq length 548]\tLoss: 0.002846\n","170it [00:02, 68.01it/s]Train epoch: 14 [batch #175, batch_size 16, seq length 574]\tLoss: 0.002658\n","198it [00:02, 64.97it/s]Train epoch: 14 [batch #200, batch_size 16, seq length 596]\tLoss: 0.002817\n","219it [00:03, 65.38it/s]Train epoch: 14 [batch #225, batch_size 16, seq length 618]\tLoss: 0.002782\n","247it [00:03, 64.93it/s]Train epoch: 14 [batch #250, batch_size 16, seq length 638]\tLoss: 0.002641\n","275it [00:03, 64.48it/s]Train epoch: 14 [batch #275, batch_size 16, seq length 656]\tLoss: 0.002955\n","296it [00:04, 62.04it/s]Train epoch: 14 [batch #300, batch_size 16, seq length 674]\tLoss: 0.003004\n","324it [00:04, 62.34it/s]Train epoch: 14 [batch #325, batch_size 16, seq length 693]\tLoss: 0.002826\n","345it [00:05, 61.05it/s]Train epoch: 14 [batch #350, batch_size 16, seq length 710]\tLoss: 0.002875\n","373it [00:05, 60.30it/s]Train epoch: 14 [batch #375, batch_size 16, seq length 726]\tLoss: 0.003260\n","394it [00:05, 60.14it/s]Train epoch: 14 [batch #400, batch_size 16, seq length 743]\tLoss: 0.002729\n","421it [00:06, 59.40it/s]Train epoch: 14 [batch #425, batch_size 16, seq length 758]\tLoss: 0.002869\n","446it [00:06, 57.35it/s]Train epoch: 14 [batch #450, batch_size 16, seq length 773]\tLoss: 0.003117\n","470it [00:07, 55.37it/s]Train epoch: 14 [batch #475, batch_size 16, seq length 789]\tLoss: 0.002777\n","500it [00:07, 55.32it/s]Train epoch: 14 [batch #500, batch_size 16, seq length 803]\tLoss: 0.003134\n","524it [00:08, 54.13it/s]Train epoch: 14 [batch #525, batch_size 16, seq length 818]\tLoss: 0.003040\n","548it [00:08, 54.85it/s]Train epoch: 14 [batch #550, batch_size 16, seq length 833]\tLoss: 0.002919\n","572it [00:09, 54.59it/s]Train epoch: 14 [batch #575, batch_size 16, seq length 846]\tLoss: 0.002844\n","596it [00:09, 54.21it/s]Train epoch: 14 [batch #600, batch_size 16, seq length 860]\tLoss: 0.003203\n","620it [00:09, 53.78it/s]Train epoch: 14 [batch #625, batch_size 16, seq length 874]\tLoss: 0.003202\n","650it [00:10, 52.53it/s]Train epoch: 14 [batch #650, batch_size 16, seq length 887]\tLoss: 0.002905\n","674it [00:10, 53.33it/s]Train epoch: 14 [batch #675, batch_size 16, seq length 901]\tLoss: 0.003044\n","698it [00:11, 51.20it/s]Train epoch: 14 [batch #700, batch_size 16, seq length 915]\tLoss: 0.002897\n","722it [00:11, 51.50it/s]Train epoch: 14 [batch #725, batch_size 16, seq length 929]\tLoss: 0.003081\n","746it [00:12, 50.96it/s]Train epoch: 14 [batch #750, batch_size 16, seq length 942]\tLoss: 0.003201\n","775it [00:12, 48.65it/s]Train epoch: 14 [batch #775, batch_size 16, seq length 954]\tLoss: 0.003120\n","797it [00:13, 48.74it/s]Train epoch: 14 [batch #800, batch_size 16, seq length 968]\tLoss: 0.003175\n","823it [00:13, 49.00it/s]Train epoch: 14 [batch #825, batch_size 16, seq length 981]\tLoss: 0.003176\n","849it [00:14, 47.26it/s]Train epoch: 14 [batch #850, batch_size 16, seq length 994]\tLoss: 0.003255\n","874it [00:15, 47.86it/s]Train epoch: 14 [batch #875, batch_size 16, seq length 1006]\tLoss: 0.003131\n","899it [00:15, 46.18it/s]Train epoch: 14 [batch #900, batch_size 16, seq length 1020]\tLoss: 0.003401\n","924it [00:16, 45.51it/s]Train epoch: 14 [batch #925, batch_size 16, seq length 1033]\tLoss: 0.003146\n","949it [00:16, 46.19it/s]Train epoch: 14 [batch #950, batch_size 16, seq length 1046]\tLoss: 0.003292\n","974it [00:17, 45.94it/s]Train epoch: 14 [batch #975, batch_size 16, seq length 1059]\tLoss: 0.003287\n","999it [00:17, 44.47it/s]Train epoch: 14 [batch #1000, batch_size 16, seq length 1073]\tLoss: 0.003309\n","1024it [00:18, 45.75it/s]Train epoch: 14 [batch #1025, batch_size 16, seq length 1085]\tLoss: 0.003475\n","1049it [00:18, 43.96it/s]Train epoch: 14 [batch #1050, batch_size 16, seq length 1099]\tLoss: 0.003483\n","1074it [00:19, 43.12it/s]Train epoch: 14 [batch #1075, batch_size 16, seq length 1113]\tLoss: 0.003404\n","1099it [00:20, 43.27it/s]Train epoch: 14 [batch #1100, batch_size 16, seq length 1126]\tLoss: 0.003323\n","1124it [00:20, 42.11it/s]Train epoch: 14 [batch #1125, batch_size 16, seq length 1140]\tLoss: 0.003556\n","1149it [00:21, 42.96it/s]Train epoch: 14 [batch #1150, batch_size 16, seq length 1154]\tLoss: 0.003474\n","1174it [00:21, 42.96it/s]Train epoch: 14 [batch #1175, batch_size 16, seq length 1167]\tLoss: 0.003592\n","1199it [00:22, 42.90it/s]Train epoch: 14 [batch #1200, batch_size 16, seq length 1180]\tLoss: 0.003442\n","1224it [00:22, 41.72it/s]Train epoch: 14 [batch #1225, batch_size 16, seq length 1195]\tLoss: 0.003496\n","1249it [00:23, 40.34it/s]Train epoch: 14 [batch #1250, batch_size 16, seq length 1208]\tLoss: 0.003317\n","1274it [00:24, 41.06it/s]Train epoch: 14 [batch #1275, batch_size 16, seq length 1220]\tLoss: 0.003393\n","1299it [00:24, 40.72it/s]Train epoch: 14 [batch #1300, batch_size 16, seq length 1234]\tLoss: 0.003533\n","1324it [00:25, 40.76it/s]Train epoch: 14 [batch #1325, batch_size 16, seq length 1248]\tLoss: 0.003473\n","1349it [00:26, 40.59it/s]Train epoch: 14 [batch #1350, batch_size 16, seq length 1262]\tLoss: 0.003748\n","1373it [00:26, 39.29it/s]Train epoch: 14 [batch #1375, batch_size 16, seq length 1276]\tLoss: 0.003954\n","1398it [00:27, 38.56it/s]Train epoch: 14 [batch #1400, batch_size 16, seq length 1290]\tLoss: 0.003902\n","1423it [00:27, 38.59it/s]Train epoch: 14 [batch #1425, batch_size 16, seq length 1304]\tLoss: 0.003972\n","1447it [00:28, 37.84it/s]Train epoch: 14 [batch #1450, batch_size 16, seq length 1319]\tLoss: 0.003727\n","1472it [00:29, 38.55it/s]Train epoch: 14 [batch #1475, batch_size 16, seq length 1332]\tLoss: 0.003935\n","1500it [00:29, 37.58it/s]Train epoch: 14 [batch #1500, batch_size 16, seq length 1346]\tLoss: 0.003827\n","1524it [00:30, 36.54it/s]Train epoch: 14 [batch #1525, batch_size 16, seq length 1362]\tLoss: 0.003376\n","1548it [00:31, 37.38it/s]Train epoch: 14 [batch #1550, batch_size 16, seq length 1377]\tLoss: 0.003954\n","1572it [00:31, 37.14it/s]Train epoch: 14 [batch #1575, batch_size 16, seq length 1392]\tLoss: 0.003448\n","1600it [00:32, 36.62it/s]Train epoch: 14 [batch #1600, batch_size 16, seq length 1406]\tLoss: 0.003594\n","1624it [00:33, 35.39it/s]Train epoch: 14 [batch #1625, batch_size 16, seq length 1422]\tLoss: 0.004053\n","1648it [00:34, 36.06it/s]Train epoch: 14 [batch #1650, batch_size 16, seq length 1437]\tLoss: 0.003836\n","1672it [00:34, 35.90it/s]Train epoch: 14 [batch #1675, batch_size 16, seq length 1454]\tLoss: 0.003841\n","1700it [00:35, 35.77it/s]Train epoch: 14 [batch #1700, batch_size 16, seq length 1469]\tLoss: 0.004251\n","1724it [00:36, 35.05it/s]Train epoch: 14 [batch #1725, batch_size 16, seq length 1484]\tLoss: 0.003813\n","1748it [00:36, 34.69it/s]Train epoch: 14 [batch #1750, batch_size 16, seq length 1501]\tLoss: 0.003705\n","1772it [00:37, 34.09it/s]Train epoch: 14 [batch #1775, batch_size 16, seq length 1518]\tLoss: 0.004175\n","1800it [00:38, 34.34it/s]Train epoch: 14 [batch #1800, batch_size 16, seq length 1533]\tLoss: 0.004162\n","1824it [00:39, 33.60it/s]Train epoch: 14 [batch #1825, batch_size 16, seq length 1548]\tLoss: 0.004224\n","1848it [00:39, 33.22it/s]Train epoch: 14 [batch #1850, batch_size 16, seq length 1566]\tLoss: 0.003840\n","1872it [00:40, 32.72it/s]Train epoch: 14 [batch #1875, batch_size 16, seq length 1582]\tLoss: 0.004066\n","1900it [00:41, 33.36it/s]Train epoch: 14 [batch #1900, batch_size 16, seq length 1599]\tLoss: 0.004276\n","1924it [00:42, 33.04it/s]Train epoch: 14 [batch #1925, batch_size 16, seq length 1616]\tLoss: 0.004095\n","1948it [00:42, 33.07it/s]Train epoch: 14 [batch #1950, batch_size 16, seq length 1633]\tLoss: 0.004073\n","1972it [00:43, 32.67it/s]Train epoch: 14 [batch #1975, batch_size 16, seq length 1651]\tLoss: 0.004061\n","2000it [00:44, 31.89it/s]Train epoch: 14 [batch #2000, batch_size 16, seq length 1670]\tLoss: 0.003945\n","2024it [00:45, 32.44it/s]Train epoch: 14 [batch #2025, batch_size 16, seq length 1688]\tLoss: 0.004035\n","2048it [00:45, 32.06it/s]Train epoch: 14 [batch #2050, batch_size 16, seq length 1707]\tLoss: 0.004070\n","2072it [00:46, 30.76it/s]Train epoch: 14 [batch #2075, batch_size 16, seq length 1728]\tLoss: 0.004334\n","2100it [00:47, 31.31it/s]Train epoch: 14 [batch #2100, batch_size 16, seq length 1748]\tLoss: 0.004051\n","2124it [00:48, 30.45it/s]Train epoch: 14 [batch #2125, batch_size 16, seq length 1768]\tLoss: 0.004605\n","2148it [00:49, 30.59it/s]Train epoch: 14 [batch #2150, batch_size 16, seq length 1787]\tLoss: 0.004024\n","2172it [00:49, 30.14it/s]Train epoch: 14 [batch #2175, batch_size 16, seq length 1809]\tLoss: 0.004581\n","2200it [00:50, 30.12it/s]Train epoch: 14 [batch #2200, batch_size 16, seq length 1833]\tLoss: 0.004197\n","2223it [00:51, 29.88it/s]Train epoch: 14 [batch #2225, batch_size 16, seq length 1855]\tLoss: 0.004327\n","2250it [00:52, 29.80it/s]Train epoch: 14 [batch #2250, batch_size 16, seq length 1878]\tLoss: 0.004626\n","2275it [00:53, 28.83it/s]Train epoch: 14 [batch #2275, batch_size 16, seq length 1901]\tLoss: 0.004551\n","2299it [00:54, 28.91it/s]Train epoch: 14 [batch #2300, batch_size 16, seq length 1927]\tLoss: 0.004461\n","2323it [00:55, 28.55it/s]Train epoch: 14 [batch #2325, batch_size 16, seq length 1953]\tLoss: 0.004867\n","2350it [00:56, 28.56it/s]Train epoch: 14 [batch #2350, batch_size 16, seq length 1980]\tLoss: 0.004510\n","2374it [00:56, 28.40it/s]Train epoch: 14 [batch #2375, batch_size 16, seq length 2006]\tLoss: 0.004689\n","2398it [00:57, 27.47it/s]Train epoch: 14 [batch #2400, batch_size 16, seq length 2034]\tLoss: 0.004763\n","2425it [00:58, 27.23it/s]Train epoch: 14 [batch #2425, batch_size 16, seq length 2064]\tLoss: 0.004679\n","2449it [00:59, 27.01it/s]Train epoch: 14 [batch #2450, batch_size 16, seq length 2094]\tLoss: 0.004848\n","2473it [01:00, 27.18it/s]Train epoch: 14 [batch #2475, batch_size 16, seq length 2126]\tLoss: 0.004948\n","2500it [01:01, 26.37it/s]Train epoch: 14 [batch #2500, batch_size 16, seq length 2160]\tLoss: 0.004900\n","2524it [01:02, 26.28it/s]Train epoch: 14 [batch #2525, batch_size 16, seq length 2192]\tLoss: 0.005176\n","2548it [01:03, 26.04it/s]Train epoch: 14 [batch #2550, batch_size 16, seq length 2228]\tLoss: 0.004950\n","2575it [01:04, 24.28it/s]Train epoch: 14 [batch #2575, batch_size 16, seq length 2266]\tLoss: 0.005369\n","2599it [01:05, 24.68it/s]Train epoch: 14 [batch #2600, batch_size 16, seq length 2307]\tLoss: 0.005248\n","2623it [01:06, 24.41it/s]Train epoch: 14 [batch #2625, batch_size 16, seq length 2350]\tLoss: 0.005171\n","2650it [01:07, 23.73it/s]Train epoch: 14 [batch #2650, batch_size 16, seq length 2392]\tLoss: 0.005097\n","2674it [01:08, 23.41it/s]Train epoch: 14 [batch #2675, batch_size 16, seq length 2439]\tLoss: 0.005345\n","2698it [01:09, 23.50it/s]Train epoch: 14 [batch #2700, batch_size 16, seq length 2491]\tLoss: 0.005430\n","2725it [01:10, 23.21it/s]Train epoch: 14 [batch #2725, batch_size 16, seq length 2500]\tLoss: 0.005706\n","2749it [01:11, 22.14it/s]Train epoch: 14 [batch #2750, batch_size 16, seq length 2500]\tLoss: 0.005641\n","2773it [01:12, 22.28it/s]Train epoch: 14 [batch #2775, batch_size 16, seq length 2500]\tLoss: 0.005852\n","2800it [01:14, 22.76it/s]Train epoch: 14 [batch #2800, batch_size 16, seq length 2500]\tLoss: 0.005743\n","2824it [01:15, 22.66it/s]Train epoch: 14 [batch #2825, batch_size 16, seq length 2500]\tLoss: 0.005810\n","2848it [01:16, 22.24it/s]Train epoch: 14 [batch #2850, batch_size 16, seq length 2500]\tLoss: 0.005860\n","2875it [01:17, 21.47it/s]Train epoch: 14 [batch #2875, batch_size 16, seq length 2500]\tLoss: 0.006530\n","2899it [01:18, 21.03it/s]Train epoch: 14 [batch #2900, batch_size 16, seq length 2500]\tLoss: 0.006810\n","2923it [01:19, 20.98it/s]Train epoch: 14 [batch #2925, batch_size 16, seq length 2500]\tLoss: 0.007177\n","2950it [01:21, 19.82it/s]Train epoch: 14 [batch #2950, batch_size 16, seq length 2500]\tLoss: 0.007172\n","2974it [01:22, 18.63it/s]Train epoch: 14 [batch #2975, batch_size 16, seq length 2500]\tLoss: 0.008386\n","2983it [01:22, 35.98it/s]\n","epoch loss: 0.0039970614958149886\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:03, 445.15it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0270, 0.0442, 0.0416, 0.0429, 0.8193\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2739, 0.4894, 0.3835, 0.4301, 0.9691\n","rec_at_8: 0.3123\n","prec_at_8: 0.5754\n","rec_at_15: 0.4311\n","prec_at_15: 0.4423\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/cnn_vanilla_May_02_220247\n","\n","prec_at_8 hasn't improved in 10 epochs, early stopping...\n","loading pretrained embeddings...\n","adding unk embedding\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:03, 414.28it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0243, 0.0425, 0.0357, 0.0388, 0.8376\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2809, 0.5512, 0.3642, 0.4386, 0.9724\n","rec_at_8: 0.3279\n","prec_at_8: 0.6087\n","rec_at_15: 0.4448\n","prec_at_15: 0.4612\n","\n","\n","evaluating on test\n","file for evaluation: ../../mimicdata/mimic3/test_full.csv\n","3372it [00:08, 412.24it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0235, 0.0463, 0.0345, 0.0395, 0.8232\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2705, 0.5477, 0.3483, 0.4258, 0.9709\n","rec_at_8: 0.3085\n","prec_at_8: 0.5959\n","rec_at_15: 0.4221\n","prec_at_15: 0.4536\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/cnn_vanilla_May_02_220247\n","\n","TOTAL ELAPSED TIME FOR cnn_vanilla MODEL AND 16 EPOCHS: 1561.879260\n"]}],"source":["%cd /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/predictions/CNN_mimic3_full/\n","!python ../../learn/training.py ../../mimicdata/mimic3/train_full.csv ../../mimicdata/mimic3/vocab.csv full cnn_vanilla 100 --filter-size 4 --num-filter-maps 500 --dropout 0.2 --lr 0.003 --embed-file ../../mimicdata/mimic3/processed_full.embed --patience 10 --criterion prec_at_8 --gpu "]},{"cell_type":"markdown","metadata":{"id":"Q1al_KzBnYyc"},"source":["## Train CNN_MIMIC3_50\n","No starting model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":169931,"status":"ok","timestamp":1651528861888,"user":{"displayName":"Chris Rock","userId":"13280703149428069160"},"user_tz":420},"id":"NdVJ686-nWrn","outputId":"dba66b5c-68a1-4279-ff20-c099923cad63"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/predictions/CNN_mimic3_50\n","ARGS: Namespace(Y='50', batch_size=16, bidirectional=None, cell_type='gru', code_emb=None, command='python ../../learn/training.py ../../mimicdata/mimic3/train_50.csv ../../mimicdata/mimic3/vocab.csv 50 cnn_vanilla 100 --filter-size 4 --num-filter-maps 500 --dropout 0.2 --lr 0.003 --embed-file ../../mimicdata/mimic3/processed_full.embed --gpu', criterion='f1_micro', data_path='../../mimicdata/mimic3/train_50.csv', dropout=0.2, embed_file='../../mimicdata/mimic3/processed_full.embed', embed_size=100, filter_size='4', gpu=True, lmbda=0, lr=0.003, model='cnn_vanilla', n_epochs=100, num_filter_maps=500, patience=3, pool=None, public_model=None, quiet=None, rnn_dim=128, rnn_layers=1, samples=None, stack_filters=None, test_model=None, version='mimic3', vocab='../../mimicdata/mimic3/vocab.csv', weight_decay=0)\n","loading lookups...\n","loading pretrained embeddings...\n","adding unk embedding\n","VanillaConv(\n","  (embed_drop): Dropout(p=0.2, inplace=False)\n","  (embed): Embedding(51919, 100, padding_idx=0)\n","  (conv): Conv1d(100, 500, kernel_size=(4,), stride=(1,))\n","  (fc): Linear(in_features=500, out_features=50, bias=True)\n",")\n","EPOCH 0\n","0it [00:00, ?it/s]Train epoch: 0 [batch #0, batch_size 16, seq length 212]\tLoss: 0.693943\n","20it [00:00, 65.31it/s]Train epoch: 0 [batch #25, batch_size 16, seq length 571]\tLoss: 0.249582\n","43it [00:00, 71.84it/s]Train epoch: 0 [batch #50, batch_size 16, seq length 709]\tLoss: 0.256195\n","75it [00:01, 72.52it/s]Train epoch: 0 [batch #75, batch_size 16, seq length 806]\tLoss: 0.275128\n","99it [00:01, 69.06it/s]Train epoch: 0 [batch #100, batch_size 16, seq length 892]\tLoss: 0.246556\n","120it [00:01, 63.96it/s]Train epoch: 0 [batch #125, batch_size 16, seq length 978]\tLoss: 0.247619\n","147it [00:02, 57.84it/s]Train epoch: 0 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.274149\n","171it [00:02, 53.79it/s]Train epoch: 0 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.249305\n","195it [00:03, 51.28it/s]Train epoch: 0 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.261430\n","222it [00:03, 48.18it/s]Train epoch: 0 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.265532\n","247it [00:04, 44.84it/s]Train epoch: 0 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.268887\n","272it [00:04, 42.25it/s]Train epoch: 0 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.284989\n","297it [00:05, 39.96it/s]Train epoch: 0 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.281184\n","322it [00:06, 38.24it/s]Train epoch: 0 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.267263\n","350it [00:06, 36.01it/s]Train epoch: 0 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.282372\n","374it [00:07, 33.84it/s]Train epoch: 0 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.271586\n","398it [00:08, 32.58it/s]Train epoch: 0 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.278478\n","422it [00:09, 30.33it/s]Train epoch: 0 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.275578\n","449it [00:10, 27.38it/s]Train epoch: 0 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.275039\n","473it [00:11, 25.42it/s]Train epoch: 0 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.302626\n","500it [00:12, 22.50it/s]Train epoch: 0 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.315030\n","505it [00:12, 40.63it/s]\n","epoch loss: 0.2722056303874101\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:03, 463.10it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3221, 0.4605, 0.4665, 0.4635, 0.8390\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3942, 0.5898, 0.5430, 0.5655, 0.8769\n","rec_at_5: 0.5399\n","prec_at_5: 0.5587\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/cnn_vanilla_May_02_215822\n","\n","EPOCH 1\n","0it [00:00, ?it/s]Train epoch: 1 [batch #0, batch_size 16, seq length 212]\tLoss: 0.301635\n","22it [00:00, 101.67it/s]Train epoch: 1 [batch #25, batch_size 16, seq length 571]\tLoss: 0.177381\n","43it [00:00, 86.61it/s]Train epoch: 1 [batch #50, batch_size 16, seq length 709]\tLoss: 0.172399\n","70it [00:00, 77.54it/s]Train epoch: 1 [batch #75, batch_size 16, seq length 806]\tLoss: 0.198360\n","94it [00:01, 70.78it/s]Train epoch: 1 [batch #100, batch_size 16, seq length 892]\tLoss: 0.173466\n","123it [00:01, 63.95it/s]Train epoch: 1 [batch #125, batch_size 16, seq length 978]\tLoss: 0.174566\n","150it [00:02, 57.91it/s]Train epoch: 1 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.195448\n","174it [00:02, 53.25it/s]Train epoch: 1 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.179367\n","198it [00:03, 51.68it/s]Train epoch: 1 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.193626\n","222it [00:03, 49.55it/s]Train epoch: 1 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.207796\n","247it [00:04, 44.99it/s]Train epoch: 1 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.199200\n","272it [00:04, 42.28it/s]Train epoch: 1 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.217524\n","297it [00:05, 40.73it/s]Train epoch: 1 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.219952\n","324it [00:06, 37.91it/s]Train epoch: 1 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.203716\n","348it [00:06, 36.13it/s]Train epoch: 1 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.214629\n","372it [00:07, 34.25it/s]Train epoch: 1 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.215445\n","400it [00:08, 32.70it/s]Train epoch: 1 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.223532\n","424it [00:09, 30.14it/s]Train epoch: 1 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.222642\n","449it [00:09, 25.41it/s]Train epoch: 1 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.226229\n","473it [00:10, 24.69it/s]Train epoch: 1 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.251211\n","500it [00:12, 22.18it/s]Train epoch: 1 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.269411\n","505it [00:12, 40.98it/s]\n","epoch loss: 0.20548386463139318\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:03, 489.73it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.4223, 0.5230, 0.6468, 0.5783, 0.8787\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4569, 0.5727, 0.6931, 0.6272, 0.9115\n","rec_at_5: 0.6021\n","prec_at_5: 0.6122\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/cnn_vanilla_May_02_215822\n","\n","EPOCH 2\n","0it [00:00, ?it/s]Train epoch: 2 [batch #0, batch_size 16, seq length 212]\tLoss: 0.262118\n","20it [00:00, 96.13it/s]Train epoch: 2 [batch #25, batch_size 16, seq length 571]\tLoss: 0.145655\n","49it [00:00, 84.48it/s]Train epoch: 2 [batch #50, batch_size 16, seq length 709]\tLoss: 0.141117\n","75it [00:00, 74.73it/s]Train epoch: 2 [batch #75, batch_size 16, seq length 806]\tLoss: 0.162895\n","99it [00:01, 69.50it/s]Train epoch: 2 [batch #100, batch_size 16, seq length 892]\tLoss: 0.144432\n","120it [00:01, 64.51it/s]Train epoch: 2 [batch #125, batch_size 16, seq length 978]\tLoss: 0.146719\n","147it [00:02, 58.01it/s]Train epoch: 2 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.166733\n","171it [00:02, 54.30it/s]Train epoch: 2 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.148328\n","195it [00:03, 50.20it/s]Train epoch: 2 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.163782\n","222it [00:03, 48.88it/s]Train epoch: 2 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.172672\n","247it [00:04, 45.05it/s]Train epoch: 2 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.171258\n","272it [00:04, 42.77it/s]Train epoch: 2 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.182278\n","297it [00:05, 40.62it/s]Train epoch: 2 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.187202\n","323it [00:06, 34.61it/s]Train epoch: 2 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.170386\n","347it [00:06, 36.02it/s]Train epoch: 2 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.181660\n","375it [00:07, 33.85it/s]Train epoch: 2 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.182550\n","399it [00:08, 32.19it/s]Train epoch: 2 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.194416\n","423it [00:09, 29.93it/s]Train epoch: 2 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.188723\n","448it [00:10, 27.42it/s]Train epoch: 2 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.193032\n","475it [00:11, 24.90it/s]Train epoch: 2 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.211101\n","499it [00:12, 22.53it/s]Train epoch: 2 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.226664\n","505it [00:12, 40.79it/s]\n","epoch loss: 0.172673699613845\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:03, 487.10it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.4389, 0.5276, 0.6872, 0.5969, 0.8883\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4611, 0.5600, 0.7230, 0.6312, 0.9150\n","rec_at_5: 0.6154\n","prec_at_5: 0.6202\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/cnn_vanilla_May_02_215822\n","\n","EPOCH 3\n","0it [00:00, ?it/s]Train epoch: 3 [batch #0, batch_size 16, seq length 212]\tLoss: 0.225029\n","20it [00:00, 97.68it/s]Train epoch: 3 [batch #25, batch_size 16, seq length 571]\tLoss: 0.128739\n","49it [00:00, 83.20it/s]Train epoch: 3 [batch #50, batch_size 16, seq length 709]\tLoss: 0.123660\n","74it [00:00, 75.09it/s]Train epoch: 3 [batch #75, batch_size 16, seq length 806]\tLoss: 0.139709\n","98it [00:01, 68.68it/s]Train epoch: 3 [batch #100, batch_size 16, seq length 892]\tLoss: 0.121506\n","119it [00:01, 65.25it/s]Train epoch: 3 [batch #125, batch_size 16, seq length 978]\tLoss: 0.119640\n","146it [00:02, 57.53it/s]Train epoch: 3 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.137758\n","170it [00:02, 52.88it/s]Train epoch: 3 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.122960\n","200it [00:03, 49.48it/s]Train epoch: 3 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.131776\n","225it [00:03, 47.62it/s]Train epoch: 3 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.141881\n","250it [00:04, 44.23it/s]Train epoch: 3 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.143579\n","275it [00:04, 41.88it/s]Train epoch: 3 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.146670\n","300it [00:05, 37.78it/s]Train epoch: 3 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.157117\n","324it [00:06, 36.61it/s]Train epoch: 3 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.138144\n","348it [00:06, 35.47it/s]Train epoch: 3 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.151410\n","372it [00:07, 33.47it/s]Train epoch: 3 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.148576\n","400it [00:08, 31.79it/s]Train epoch: 3 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.160010\n","424it [00:09, 29.99it/s]Train epoch: 3 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.144943\n","449it [00:10, 27.13it/s]Train epoch: 3 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.156960\n","473it [00:11, 24.80it/s]Train epoch: 3 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.169785\n","500it [00:12, 22.14it/s]Train epoch: 3 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.184763\n","505it [00:12, 40.27it/s]\n","epoch loss: 0.14261333930610431\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:03, 493.00it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.4368, 0.5235, 0.6942, 0.5969, 0.8871\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4611, 0.5571, 0.7280, 0.6312, 0.9127\n","rec_at_5: 0.6173\n","prec_at_5: 0.6168\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/cnn_vanilla_May_02_215822\n","\n","EPOCH 4\n","0it [00:00, ?it/s]Train epoch: 4 [batch #0, batch_size 16, seq length 212]\tLoss: 0.210157\n","22it [00:00, 96.43it/s] Train epoch: 4 [batch #25, batch_size 16, seq length 571]\tLoss: 0.104448\n","42it [00:00, 87.31it/s]Train epoch: 4 [batch #50, batch_size 16, seq length 709]\tLoss: 0.102652\n","69it [00:00, 77.12it/s]Train epoch: 4 [batch #75, batch_size 16, seq length 806]\tLoss: 0.110881\n","93it [00:01, 71.27it/s]Train epoch: 4 [batch #100, batch_size 16, seq length 892]\tLoss: 0.099091\n","122it [00:01, 63.29it/s]Train epoch: 4 [batch #125, batch_size 16, seq length 978]\tLoss: 0.098074\n","149it [00:02, 57.07it/s]Train epoch: 4 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.106591\n","173it [00:02, 52.43it/s]Train epoch: 4 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.099187\n","197it [00:03, 50.51it/s]Train epoch: 4 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.104450\n","224it [00:03, 46.51it/s]Train epoch: 4 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.113791\n","249it [00:04, 44.24it/s]Train epoch: 4 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.114001\n","274it [00:04, 41.54it/s]Train epoch: 4 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.112702\n","298it [00:05, 37.05it/s]Train epoch: 4 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.128200\n","322it [00:06, 36.68it/s]Train epoch: 4 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.111812\n","350it [00:06, 34.89it/s]Train epoch: 4 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.115352\n","374it [00:07, 33.37it/s]Train epoch: 4 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.116588\n","398it [00:08, 31.54it/s]Train epoch: 4 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.120719\n","425it [00:09, 29.23it/s]Train epoch: 4 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.112262\n","449it [00:10, 26.69it/s]Train epoch: 4 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.122061\n","473it [00:11, 24.82it/s]Train epoch: 4 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.131857\n","500it [00:12, 22.26it/s]Train epoch: 4 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.146327\n","505it [00:12, 40.06it/s]\n","epoch loss: 0.11315532110926538\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:03, 472.42it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.4335, 0.5240, 0.6777, 0.5910, 0.8818\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4543, 0.5545, 0.7154, 0.6247, 0.9085\n","rec_at_5: 0.6127\n","prec_at_5: 0.6155\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/cnn_vanilla_May_02_215822\n","\n","EPOCH 5\n","0it [00:00, ?it/s]Train epoch: 5 [batch #0, batch_size 16, seq length 212]\tLoss: 0.195731\n","22it [00:00, 97.78it/s] Train epoch: 5 [batch #25, batch_size 16, seq length 571]\tLoss: 0.091053\n","42it [00:00, 85.40it/s]Train epoch: 5 [batch #50, batch_size 16, seq length 709]\tLoss: 0.080802\n","68it [00:00, 76.44it/s]Train epoch: 5 [batch #75, batch_size 16, seq length 806]\tLoss: 0.091273\n","100it [00:01, 67.82it/s]Train epoch: 5 [batch #100, batch_size 16, seq length 892]\tLoss: 0.079476\n","121it [00:01, 62.90it/s]Train epoch: 5 [batch #125, batch_size 16, seq length 978]\tLoss: 0.075163\n","147it [00:02, 56.17it/s]Train epoch: 5 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.084187\n","171it [00:02, 50.86it/s]Train epoch: 5 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.079513\n","198it [00:03, 49.65it/s]Train epoch: 5 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.083646\n","223it [00:03, 46.86it/s]Train epoch: 5 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.091631\n","248it [00:04, 43.73it/s]Train epoch: 5 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.092393\n","273it [00:04, 41.58it/s]Train epoch: 5 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.090429\n","297it [00:05, 38.99it/s]Train epoch: 5 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.091148\n","325it [00:06, 37.40it/s]Train epoch: 5 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.083983\n","349it [00:06, 35.58it/s]Train epoch: 5 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.085915\n","373it [00:07, 32.51it/s]Train epoch: 5 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.088599\n","397it [00:08, 31.70it/s]Train epoch: 5 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.093788\n","423it [00:09, 29.70it/s]Train epoch: 5 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.085817\n","450it [00:10, 26.66it/s]Train epoch: 5 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.100078\n","474it [00:11, 25.14it/s]Train epoch: 5 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.106884\n","498it [00:12, 22.66it/s]Train epoch: 5 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.117469\n","505it [00:12, 40.04it/s]\n","epoch loss: 0.08853027042886703\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:03, 492.69it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.4354, 0.5656, 0.6289, 0.5956, 0.8781\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4639, 0.5955, 0.6774, 0.6338, 0.9033\n","rec_at_5: 0.6078\n","prec_at_5: 0.6118\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/cnn_vanilla_May_02_215822\n","\n","EPOCH 6\n","0it [00:00, ?it/s]Train epoch: 6 [batch #0, batch_size 16, seq length 212]\tLoss: 0.178864\n","22it [00:00, 98.80it/s] Train epoch: 6 [batch #25, batch_size 16, seq length 571]\tLoss: 0.077395\n","42it [00:00, 85.78it/s]Train epoch: 6 [batch #50, batch_size 16, seq length 709]\tLoss: 0.065028\n","68it [00:00, 76.09it/s]Train epoch: 6 [batch #75, batch_size 16, seq length 806]\tLoss: 0.068358\n","100it [00:01, 67.28it/s]Train epoch: 6 [batch #100, batch_size 16, seq length 892]\tLoss: 0.058334\n","121it [00:01, 60.81it/s]Train epoch: 6 [batch #125, batch_size 16, seq length 978]\tLoss: 0.058583\n","147it [00:02, 56.32it/s]Train epoch: 6 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.062477\n","171it [00:02, 50.66it/s]Train epoch: 6 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.060288\n","199it [00:03, 48.90it/s]Train epoch: 6 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.064204\n","224it [00:03, 45.78it/s]Train epoch: 6 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.068886\n","249it [00:04, 43.50it/s]Train epoch: 6 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.074944\n","274it [00:04, 41.42it/s]Train epoch: 6 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.067456\n","298it [00:05, 36.18it/s]Train epoch: 6 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.072856\n","322it [00:06, 36.58it/s]Train epoch: 6 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.062614\n","350it [00:07, 35.19it/s]Train epoch: 6 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.065630\n","374it [00:07, 32.94it/s]Train epoch: 6 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.069492\n","398it [00:08, 31.01it/s]Train epoch: 6 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.076644\n","425it [00:09, 28.94it/s]Train epoch: 6 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.068923\n","449it [00:10, 26.46it/s]Train epoch: 6 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.072227\n","473it [00:11, 24.08it/s]Train epoch: 6 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.084174\n","500it [00:12, 21.80it/s]Train epoch: 6 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.094236\n","505it [00:12, 39.45it/s]\n","epoch loss: 0.06916706589059812\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:03, 484.63it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.4293, 0.5533, 0.6277, 0.5882, 0.8763\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4545, 0.5813, 0.6756, 0.6249, 0.9009\n","rec_at_5: 0.6018\n","prec_at_5: 0.6065\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/cnn_vanilla_May_02_215822\n","\n","EPOCH 7\n","0it [00:00, ?it/s]Train epoch: 7 [batch #0, batch_size 16, seq length 212]\tLoss: 0.199126\n","22it [00:00, 98.16it/s] Train epoch: 7 [batch #25, batch_size 16, seq length 571]\tLoss: 0.058853\n","42it [00:00, 87.04it/s]Train epoch: 7 [batch #50, batch_size 16, seq length 709]\tLoss: 0.055238\n","69it [00:00, 76.57it/s]Train epoch: 7 [batch #75, batch_size 16, seq length 806]\tLoss: 0.058202\n","100it [00:01, 67.58it/s]Train epoch: 7 [batch #100, batch_size 16, seq length 892]\tLoss: 0.046673\n","121it [00:01, 63.46it/s]Train epoch: 7 [batch #125, batch_size 16, seq length 978]\tLoss: 0.046468\n","147it [00:02, 56.15it/s]Train epoch: 7 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.050262\n","171it [00:02, 51.03it/s]Train epoch: 7 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.045667\n","200it [00:03, 49.23it/s]Train epoch: 7 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.049422\n","225it [00:03, 45.58it/s]Train epoch: 7 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.051431\n","250it [00:04, 43.13it/s]Train epoch: 7 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.056353\n","275it [00:04, 41.19it/s]Train epoch: 7 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.052793\n","299it [00:05, 38.49it/s]Train epoch: 7 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.060741\n","323it [00:06, 35.61it/s]Train epoch: 7 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.050986\n","347it [00:06, 34.71it/s]Train epoch: 7 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.050703\n","375it [00:07, 32.56it/s]Train epoch: 7 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.050003\n","399it [00:08, 30.80it/s]Train epoch: 7 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.059021\n","424it [00:09, 28.57it/s]Train epoch: 7 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.053809\n","448it [00:10, 26.61it/s]Train epoch: 7 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.056281\n","475it [00:11, 24.05it/s]Train epoch: 7 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.070162\n","499it [00:12, 22.14it/s]Train epoch: 7 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.069717\n","505it [00:12, 39.74it/s]\n","epoch loss: 0.05524543272700198\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:03, 487.75it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.4250, 0.5141, 0.6730, 0.5829, 0.8765\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4441, 0.5417, 0.7114, 0.6150, 0.9010\n","rec_at_5: 0.5977\n","prec_at_5: 0.6045\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/cnn_vanilla_May_02_215822\n","\n","EPOCH 8\n","0it [00:00, ?it/s]Train epoch: 8 [batch #0, batch_size 16, seq length 212]\tLoss: 0.158145\n","22it [00:00, 97.24it/s] Train epoch: 8 [batch #25, batch_size 16, seq length 571]\tLoss: 0.048143\n","50it [00:00, 79.76it/s]Train epoch: 8 [batch #50, batch_size 16, seq length 709]\tLoss: 0.046114\n","75it [00:00, 71.84it/s]Train epoch: 8 [batch #75, batch_size 16, seq length 806]\tLoss: 0.043549\n","97it [00:01, 67.03it/s]Train epoch: 8 [batch #100, batch_size 16, seq length 892]\tLoss: 0.035609\n","125it [00:01, 60.90it/s]Train epoch: 8 [batch #125, batch_size 16, seq length 978]\tLoss: 0.041070\n","150it [00:02, 53.82it/s]Train epoch: 8 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.044257\n","174it [00:02, 50.55it/s]Train epoch: 8 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.039549\n","196it [00:03, 49.05it/s]Train epoch: 8 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.041610\n","221it [00:03, 47.20it/s]Train epoch: 8 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.042128\n","246it [00:04, 43.73it/s]Train epoch: 8 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.050361\n","271it [00:04, 40.91it/s]Train epoch: 8 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.041461\n","298it [00:05, 38.61it/s]Train epoch: 8 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.046135\n","322it [00:06, 36.34it/s]Train epoch: 8 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.038483\n","350it [00:07, 35.09it/s]Train epoch: 8 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.047899\n","374it [00:07, 32.49it/s]Train epoch: 8 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.045469\n","398it [00:08, 31.42it/s]Train epoch: 8 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.048360\n","423it [00:09, 28.97it/s]Train epoch: 8 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.049542\n","450it [00:10, 25.94it/s]Train epoch: 8 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.053082\n","474it [00:11, 24.17it/s]Train epoch: 8 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.060963\n","498it [00:12, 22.50it/s]Train epoch: 8 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.066432\n","505it [00:12, 39.51it/s]\n","epoch loss: 0.04624967127485145\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:03, 491.40it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.4346, 0.5480, 0.6403, 0.5906, 0.8753\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4600, 0.5801, 0.6895, 0.6301, 0.9024\n","rec_at_5: 0.5971\n","prec_at_5: 0.6028\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/cnn_vanilla_May_02_215822\n","\n","f1_micro hasn't improved in 3 epochs, early stopping...\n","loading pretrained embeddings...\n","adding unk embedding\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:03, 456.62it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.4354, 0.5656, 0.6289, 0.5956, 0.8781\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4639, 0.5955, 0.6774, 0.6338, 0.9033\n","rec_at_5: 0.6078\n","prec_at_5: 0.6118\n","\n","\n","evaluating on test\n","file for evaluation: ../../mimicdata/mimic3/test_50.csv\n","1729it [00:03, 442.60it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.4246, 0.5538, 0.6155, 0.5830, 0.8769\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4520, 0.5862, 0.6638, 0.6226, 0.9010\n","rec_at_5: 0.5822\n","prec_at_5: 0.6039\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/cnn_vanilla_May_02_215822\n","\n","TOTAL ELAPSED TIME FOR cnn_vanilla MODEL AND 10 EPOCHS: 167.543576\n"]}],"source":["%cd /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/predictions/CNN_mimic3_50/\n","!python ../../learn/training.py ../../mimicdata/mimic3/train_50.csv ../../mimicdata/mimic3/vocab.csv 50 cnn_vanilla 100 --filter-size 4 --num-filter-maps 500 --dropout 0.2 --lr 0.003 --embed-file ../../mimicdata/mimic3/processed_full.embed --gpu \n"]},{"cell_type":"markdown","source":["### GRU_mimic3_full\n","No starting model"],"metadata":{"id":"lQRBQ0fbudZl"}},{"cell_type":"code","source":["%cd /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/predictions/GRU_mimic3_full/\n","!python ../../learn/training.py ../../mimicdata/mimic3/train_full.csv ../../mimicdata/mimic3/vocab.csv full rnn 100 --rnn-dim 512 --cell-type gru --lr 0.003 --bidirectional --patience 10 --criterion prec_at_8 --embed-file ../../mimicdata/mimic3/processed_full.embed --gpu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"GgC9SZgFzLEw","executionInfo":{"status":"ok","timestamp":1651637591592,"user_tz":420,"elapsed":6017962,"user":{"displayName":"Chris Rock","userId":"13280703149428069160"}},"outputId":"7678f9d5-1640-474b-e2e9-3c2721d3b9b5"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/predictions/GRU_mimic3_full\n","ARGS: Namespace(Y='full', batch_size=16, bidirectional=True, cell_type='gru', code_emb=None, command='python ../../learn/training.py ../../mimicdata/mimic3/train_full.csv ../../mimicdata/mimic3/vocab.csv full rnn 100 --rnn-dim 512 --cell-type gru --lr 0.003 --bidirectional --patience 10 --criterion prec_at_8 --embed-file ../../mimicdata/mimic3/processed_full.embed --gpu', criterion='prec_at_8', data_path='../../mimicdata/mimic3/train_full.csv', dropout=0.5, embed_file='../../mimicdata/mimic3/processed_full.embed', embed_size=100, filter_size=4, gpu=True, lmbda=0, lr=0.003, model='rnn', n_epochs=100, num_filter_maps=50, patience=10, pool=None, public_model=None, quiet=None, rnn_dim=512, rnn_layers=1, samples=None, stack_filters=None, test_model=None, version='mimic3', vocab='../../mimicdata/mimic3/vocab.csv', weight_decay=0)\n","loading lookups...\n","loading pretrained embeddings...\n","adding unk embedding\n","VanillaRNN(\n","  (embed_drop): Dropout(p=0.5, inplace=False)\n","  (embed): Embedding(51919, 100, padding_idx=0)\n","  (rnn): GRU(100, 256, bidirectional=True)\n","  (final): Linear(in_features=512, out_features=8921, bias=True)\n",")\n","EPOCH 0\n","0it [00:00, ?it/s]Train epoch: 0 [batch #0, batch_size 16, seq length 117]\tLoss: 0.693339\n","22it [00:00, 35.17it/s]Train epoch: 0 [batch #25, batch_size 16, seq length 337]\tLoss: 0.008514\n","50it [00:01, 34.01it/s]Train epoch: 0 [batch #50, batch_size 16, seq length 402]\tLoss: 0.007428\n","74it [00:02, 31.37it/s]Train epoch: 0 [batch #75, batch_size 16, seq length 452]\tLoss: 0.006448\n","100it [00:03, 29.16it/s]Train epoch: 0 [batch #100, batch_size 16, seq length 490]\tLoss: 0.006394\n","124it [00:04, 27.45it/s]Train epoch: 0 [batch #125, batch_size 16, seq length 520]\tLoss: 0.006977\n","148it [00:04, 26.02it/s]Train epoch: 0 [batch #150, batch_size 16, seq length 548]\tLoss: 0.007269\n","175it [00:06, 24.80it/s]Train epoch: 0 [batch #175, batch_size 16, seq length 574]\tLoss: 0.006788\n","199it [00:07, 23.99it/s]Train epoch: 0 [batch #200, batch_size 16, seq length 596]\tLoss: 0.007181\n","223it [00:08, 22.99it/s]Train epoch: 0 [batch #225, batch_size 16, seq length 618]\tLoss: 0.007510\n","250it [00:09, 21.34it/s]Train epoch: 0 [batch #250, batch_size 16, seq length 638]\tLoss: 0.006873\n","274it [00:10, 21.53it/s]Train epoch: 0 [batch #275, batch_size 16, seq length 656]\tLoss: 0.007567\n","298it [00:11, 21.26it/s]Train epoch: 0 [batch #300, batch_size 16, seq length 674]\tLoss: 0.007754\n","325it [00:12, 20.73it/s]Train epoch: 0 [batch #325, batch_size 16, seq length 693]\tLoss: 0.007008\n","349it [00:14, 20.43it/s]Train epoch: 0 [batch #350, batch_size 16, seq length 710]\tLoss: 0.007286\n","375it [00:15, 20.03it/s]Train epoch: 0 [batch #375, batch_size 16, seq length 726]\tLoss: 0.008273\n","400it [00:16, 19.67it/s]Train epoch: 0 [batch #400, batch_size 16, seq length 743]\tLoss: 0.007195\n","424it [00:17, 19.14it/s]Train epoch: 0 [batch #425, batch_size 16, seq length 758]\tLoss: 0.007289\n","450it [00:19, 18.89it/s]Train epoch: 0 [batch #450, batch_size 16, seq length 773]\tLoss: 0.007597\n","474it [00:20, 18.05it/s]Train epoch: 0 [batch #475, batch_size 16, seq length 789]\tLoss: 0.006882\n","500it [00:22, 18.23it/s]Train epoch: 0 [batch #500, batch_size 16, seq length 803]\tLoss: 0.007429\n","524it [00:23, 17.80it/s]Train epoch: 0 [batch #525, batch_size 16, seq length 818]\tLoss: 0.007451\n","550it [00:24, 17.37it/s]Train epoch: 0 [batch #550, batch_size 16, seq length 833]\tLoss: 0.007356\n","574it [00:26, 17.43it/s]Train epoch: 0 [batch #575, batch_size 16, seq length 846]\tLoss: 0.007155\n","600it [00:27, 16.86it/s]Train epoch: 0 [batch #600, batch_size 16, seq length 860]\tLoss: 0.008318\n","624it [00:29, 16.68it/s]Train epoch: 0 [batch #625, batch_size 16, seq length 874]\tLoss: 0.008061\n","650it [00:30, 15.46it/s]Train epoch: 0 [batch #650, batch_size 16, seq length 887]\tLoss: 0.007240\n","674it [00:32, 15.96it/s]Train epoch: 0 [batch #675, batch_size 16, seq length 901]\tLoss: 0.007687\n","700it [00:33, 15.75it/s]Train epoch: 0 [batch #700, batch_size 16, seq length 915]\tLoss: 0.007289\n","724it [00:35, 15.71it/s]Train epoch: 0 [batch #725, batch_size 16, seq length 929]\tLoss: 0.007931\n","750it [00:37, 15.53it/s]Train epoch: 0 [batch #750, batch_size 16, seq length 942]\tLoss: 0.008099\n","774it [00:38, 15.55it/s]Train epoch: 0 [batch #775, batch_size 16, seq length 954]\tLoss: 0.008249\n","800it [00:40, 15.04it/s]Train epoch: 0 [batch #800, batch_size 16, seq length 968]\tLoss: 0.007892\n","824it [00:42, 14.52it/s]Train epoch: 0 [batch #825, batch_size 16, seq length 981]\tLoss: 0.008223\n","850it [00:43, 14.72it/s]Train epoch: 0 [batch #850, batch_size 16, seq length 994]\tLoss: 0.008089\n","874it [00:45, 14.68it/s]Train epoch: 0 [batch #875, batch_size 16, seq length 1006]\tLoss: 0.007581\n","900it [00:47, 14.40it/s]Train epoch: 0 [batch #900, batch_size 16, seq length 1020]\tLoss: 0.008398\n","924it [00:48, 14.19it/s]Train epoch: 0 [batch #925, batch_size 16, seq length 1033]\tLoss: 0.007900\n","950it [00:50, 13.98it/s]Train epoch: 0 [batch #950, batch_size 16, seq length 1046]\tLoss: 0.008515\n","974it [00:52, 13.68it/s]Train epoch: 0 [batch #975, batch_size 16, seq length 1059]\tLoss: 0.008045\n","1000it [00:54, 13.63it/s]Train epoch: 0 [batch #1000, batch_size 16, seq length 1073]\tLoss: 0.008812\n","1024it [00:56, 13.56it/s]Train epoch: 0 [batch #1025, batch_size 16, seq length 1085]\tLoss: 0.008358\n","1050it [00:58, 13.49it/s]Train epoch: 0 [batch #1050, batch_size 16, seq length 1099]\tLoss: 0.008556\n","1074it [01:00, 13.11it/s]Train epoch: 0 [batch #1075, batch_size 16, seq length 1113]\tLoss: 0.008531\n","1100it [01:02, 13.14it/s]Train epoch: 0 [batch #1100, batch_size 16, seq length 1126]\tLoss: 0.008319\n","1124it [01:03, 12.93it/s]Train epoch: 0 [batch #1125, batch_size 16, seq length 1140]\tLoss: 0.008680\n","1150it [01:05, 12.54it/s]Train epoch: 0 [batch #1150, batch_size 16, seq length 1154]\tLoss: 0.008594\n","1174it [01:07, 12.37it/s]Train epoch: 0 [batch #1175, batch_size 16, seq length 1167]\tLoss: 0.008517\n","1200it [01:09, 12.34it/s]Train epoch: 0 [batch #1200, batch_size 16, seq length 1180]\tLoss: 0.008610\n","1224it [01:11, 12.36it/s]Train epoch: 0 [batch #1225, batch_size 16, seq length 1195]\tLoss: 0.008321\n","1250it [01:14, 12.08it/s]Train epoch: 0 [batch #1250, batch_size 16, seq length 1208]\tLoss: 0.008349\n","1274it [01:16, 11.90it/s]Train epoch: 0 [batch #1275, batch_size 16, seq length 1220]\tLoss: 0.008140\n","1300it [01:18, 12.01it/s]Train epoch: 0 [batch #1300, batch_size 16, seq length 1234]\tLoss: 0.008619\n","1324it [01:20, 11.77it/s]Train epoch: 0 [batch #1325, batch_size 16, seq length 1248]\tLoss: 0.007967\n","1350it [01:22, 11.74it/s]Train epoch: 0 [batch #1350, batch_size 16, seq length 1262]\tLoss: 0.008907\n","1374it [01:24, 11.07it/s]Train epoch: 0 [batch #1375, batch_size 16, seq length 1276]\tLoss: 0.009686\n","1400it [01:27, 11.19it/s]Train epoch: 0 [batch #1400, batch_size 16, seq length 1290]\tLoss: 0.009143\n","1424it [01:29, 11.27it/s]Train epoch: 0 [batch #1425, batch_size 16, seq length 1304]\tLoss: 0.008845\n","1450it [01:31, 10.99it/s]Train epoch: 0 [batch #1450, batch_size 16, seq length 1319]\tLoss: 0.008902\n","1474it [01:33, 10.95it/s]Train epoch: 0 [batch #1475, batch_size 16, seq length 1332]\tLoss: 0.009586\n","1500it [01:36, 10.64it/s]Train epoch: 0 [batch #1500, batch_size 16, seq length 1346]\tLoss: 0.008991\n","1524it [01:38, 10.62it/s]Train epoch: 0 [batch #1525, batch_size 16, seq length 1362]\tLoss: 0.008331\n","1550it [01:40, 10.58it/s]Train epoch: 0 [batch #1550, batch_size 16, seq length 1377]\tLoss: 0.009372\n","1574it [01:43, 10.17it/s]Train epoch: 0 [batch #1575, batch_size 16, seq length 1392]\tLoss: 0.008522\n","1600it [01:45, 10.38it/s]Train epoch: 0 [batch #1600, batch_size 16, seq length 1406]\tLoss: 0.008711\n","1624it [01:48, 10.19it/s]Train epoch: 0 [batch #1625, batch_size 16, seq length 1422]\tLoss: 0.009247\n","1650it [01:50,  9.99it/s]Train epoch: 0 [batch #1650, batch_size 16, seq length 1437]\tLoss: 0.008952\n","1675it [01:53,  9.83it/s]Train epoch: 0 [batch #1675, batch_size 16, seq length 1454]\tLoss: 0.009437\n","1700it [01:55,  9.75it/s]Train epoch: 0 [batch #1700, batch_size 16, seq length 1469]\tLoss: 0.009377\n","1724it [01:58,  9.87it/s]Train epoch: 0 [batch #1725, batch_size 16, seq length 1484]\tLoss: 0.008452\n","1749it [02:00,  9.36it/s]Train epoch: 0 [batch #1750, batch_size 16, seq length 1501]\tLoss: 0.008843\n","1775it [02:03,  9.17it/s]Train epoch: 0 [batch #1775, batch_size 16, seq length 1518]\tLoss: 0.009373\n","1800it [02:06,  9.38it/s]Train epoch: 0 [batch #1800, batch_size 16, seq length 1533]\tLoss: 0.009618\n","1825it [02:09,  9.18it/s]Train epoch: 0 [batch #1825, batch_size 16, seq length 1548]\tLoss: 0.009721\n","1850it [02:11,  8.61it/s]Train epoch: 0 [batch #1850, batch_size 16, seq length 1566]\tLoss: 0.009058\n","1875it [02:14,  8.88it/s]Train epoch: 0 [batch #1875, batch_size 16, seq length 1582]\tLoss: 0.009266\n","1900it [02:17,  8.60it/s]Train epoch: 0 [batch #1900, batch_size 16, seq length 1599]\tLoss: 0.010111\n","1925it [02:20,  8.86it/s]Train epoch: 0 [batch #1925, batch_size 16, seq length 1616]\tLoss: 0.009445\n","1950it [02:23,  8.38it/s]Train epoch: 0 [batch #1950, batch_size 16, seq length 1633]\tLoss: 0.009408\n","1975it [02:26,  8.54it/s]Train epoch: 0 [batch #1975, batch_size 16, seq length 1651]\tLoss: 0.009166\n","2000it [02:29,  8.26it/s]Train epoch: 0 [batch #2000, batch_size 16, seq length 1670]\tLoss: 0.009061\n","2025it [02:32,  8.19it/s]Train epoch: 0 [batch #2025, batch_size 16, seq length 1688]\tLoss: 0.009061\n","2050it [02:35,  8.27it/s]Train epoch: 0 [batch #2050, batch_size 16, seq length 1707]\tLoss: 0.009555\n","2075it [02:38,  8.22it/s]Train epoch: 0 [batch #2075, batch_size 16, seq length 1728]\tLoss: 0.009590\n","2100it [02:41,  8.25it/s]Train epoch: 0 [batch #2100, batch_size 16, seq length 1748]\tLoss: 0.008961\n","2125it [02:44,  8.24it/s]Train epoch: 0 [batch #2125, batch_size 16, seq length 1768]\tLoss: 0.010012\n","2150it [02:47,  8.06it/s]Train epoch: 0 [batch #2150, batch_size 16, seq length 1787]\tLoss: 0.009205\n","2175it [02:50,  7.91it/s]Train epoch: 0 [batch #2175, batch_size 16, seq length 1809]\tLoss: 0.010082\n","2200it [02:54,  7.83it/s]Train epoch: 0 [batch #2200, batch_size 16, seq length 1833]\tLoss: 0.009239\n","2225it [02:57,  7.73it/s]Train epoch: 0 [batch #2225, batch_size 16, seq length 1855]\tLoss: 0.009432\n","2250it [03:00,  7.56it/s]Train epoch: 0 [batch #2250, batch_size 16, seq length 1878]\tLoss: 0.010069\n","2275it [03:03,  7.53it/s]Train epoch: 0 [batch #2275, batch_size 16, seq length 1901]\tLoss: 0.009188\n","2300it [03:07,  7.47it/s]Train epoch: 0 [batch #2300, batch_size 16, seq length 1927]\tLoss: 0.009321\n","2325it [03:10,  7.25it/s]Train epoch: 0 [batch #2325, batch_size 16, seq length 1953]\tLoss: 0.010018\n","2350it [03:14,  7.23it/s]Train epoch: 0 [batch #2350, batch_size 16, seq length 1980]\tLoss: 0.009377\n","2375it [03:17,  7.13it/s]Train epoch: 0 [batch #2375, batch_size 16, seq length 2006]\tLoss: 0.009977\n","2400it [03:21,  6.96it/s]Train epoch: 0 [batch #2400, batch_size 16, seq length 2034]\tLoss: 0.009639\n","2425it [03:24,  6.94it/s]Train epoch: 0 [batch #2425, batch_size 16, seq length 2064]\tLoss: 0.009522\n","2450it [03:28,  6.89it/s]Train epoch: 0 [batch #2450, batch_size 16, seq length 2094]\tLoss: 0.009807\n","2475it [03:32,  6.63it/s]Train epoch: 0 [batch #2475, batch_size 16, seq length 2126]\tLoss: 0.010508\n","2500it [03:35,  6.60it/s]Train epoch: 0 [batch #2500, batch_size 16, seq length 2160]\tLoss: 0.010270\n","2525it [03:39,  6.58it/s]Train epoch: 0 [batch #2525, batch_size 16, seq length 2192]\tLoss: 0.010381\n","2550it [03:43,  6.41it/s]Train epoch: 0 [batch #2550, batch_size 16, seq length 2228]\tLoss: 0.010065\n","2575it [03:47,  6.34it/s]Train epoch: 0 [batch #2575, batch_size 16, seq length 2266]\tLoss: 0.010552\n","2600it [03:51,  6.12it/s]Train epoch: 0 [batch #2600, batch_size 16, seq length 2307]\tLoss: 0.011123\n","2625it [03:55,  6.08it/s]Train epoch: 0 [batch #2625, batch_size 16, seq length 2350]\tLoss: 0.010250\n","2650it [03:59,  5.97it/s]Train epoch: 0 [batch #2650, batch_size 16, seq length 2392]\tLoss: 0.010179\n","2675it [04:04,  5.51it/s]Train epoch: 0 [batch #2675, batch_size 16, seq length 2439]\tLoss: 0.010452\n","2700it [04:08,  5.77it/s]Train epoch: 0 [batch #2700, batch_size 16, seq length 2491]\tLoss: 0.010466\n","2725it [04:12,  5.74it/s]Train epoch: 0 [batch #2725, batch_size 16, seq length 2500]\tLoss: 0.010713\n","2750it [04:17,  5.71it/s]Train epoch: 0 [batch #2750, batch_size 16, seq length 2500]\tLoss: 0.010520\n","2775it [04:21,  5.68it/s]Train epoch: 0 [batch #2775, batch_size 16, seq length 2500]\tLoss: 0.011002\n","2800it [04:25,  5.69it/s]Train epoch: 0 [batch #2800, batch_size 16, seq length 2500]\tLoss: 0.011314\n","2825it [04:30,  5.68it/s]Train epoch: 0 [batch #2825, batch_size 16, seq length 2500]\tLoss: 0.010828\n","2850it [04:34,  5.68it/s]Train epoch: 0 [batch #2850, batch_size 16, seq length 2500]\tLoss: 0.011362\n","2875it [04:39,  5.71it/s]Train epoch: 0 [batch #2875, batch_size 16, seq length 2500]\tLoss: 0.012197\n","2900it [04:43,  5.58it/s]Train epoch: 0 [batch #2900, batch_size 16, seq length 2500]\tLoss: 0.011804\n","2925it [04:48,  5.61it/s]Train epoch: 0 [batch #2925, batch_size 16, seq length 2500]\tLoss: 0.012039\n","2950it [04:52,  5.48it/s]Train epoch: 0 [batch #2950, batch_size 16, seq length 2500]\tLoss: 0.011549\n","2975it [04:57,  5.35it/s]Train epoch: 0 [batch #2975, batch_size 16, seq length 2500]\tLoss: 0.013723\n","2983it [04:58,  9.99it/s]\n","epoch loss: 0.009785111479029095\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:45, 36.19it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0013, 0.0029, 0.0018, 0.0022, 0.7190\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.0805, 0.5212, 0.0869, 0.1490, 0.9513\n","rec_at_8: 0.1639\n","prec_at_8: 0.3382\n","rec_at_15: 0.2324\n","prec_at_15: 0.2589\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/rnn_May_04_023326\n","\n","EPOCH 1\n","0it [00:00, ?it/s]Train epoch: 1 [batch #0, batch_size 16, seq length 117]\tLoss: 0.007932\n","24it [00:00, 37.72it/s]Train epoch: 1 [batch #25, batch_size 16, seq length 337]\tLoss: 0.005334\n","48it [00:01, 32.96it/s]Train epoch: 1 [batch #50, batch_size 16, seq length 402]\tLoss: 0.004849\n","72it [00:02, 30.48it/s]Train epoch: 1 [batch #75, batch_size 16, seq length 452]\tLoss: 0.004523\n","100it [00:03, 27.94it/s]Train epoch: 1 [batch #100, batch_size 16, seq length 490]\tLoss: 0.004311\n","124it [00:04, 25.62it/s]Train epoch: 1 [batch #125, batch_size 16, seq length 520]\tLoss: 0.004805\n","148it [00:05, 24.68it/s]Train epoch: 1 [batch #150, batch_size 16, seq length 548]\tLoss: 0.005024\n","175it [00:06, 20.42it/s]Train epoch: 1 [batch #175, batch_size 16, seq length 574]\tLoss: 0.004622\n","199it [00:07, 22.80it/s]Train epoch: 1 [batch #200, batch_size 16, seq length 596]\tLoss: 0.004966\n","223it [00:08, 21.95it/s]Train epoch: 1 [batch #225, batch_size 16, seq length 618]\tLoss: 0.005030\n","250it [00:09, 21.24it/s]Train epoch: 1 [batch #250, batch_size 16, seq length 638]\tLoss: 0.004694\n","274it [00:10, 20.98it/s]Train epoch: 1 [batch #275, batch_size 16, seq length 656]\tLoss: 0.005176\n","298it [00:11, 20.22it/s]Train epoch: 1 [batch #300, batch_size 16, seq length 674]\tLoss: 0.005157\n","325it [00:13, 19.71it/s]Train epoch: 1 [batch #325, batch_size 16, seq length 693]\tLoss: 0.004807\n","349it [00:14, 19.23it/s]Train epoch: 1 [batch #350, batch_size 16, seq length 710]\tLoss: 0.004927\n","375it [00:15, 18.89it/s]Train epoch: 1 [batch #375, batch_size 16, seq length 726]\tLoss: 0.005632\n","399it [00:17, 18.27it/s]Train epoch: 1 [batch #400, batch_size 16, seq length 743]\tLoss: 0.004886\n","425it [00:18, 18.13it/s]Train epoch: 1 [batch #425, batch_size 16, seq length 758]\tLoss: 0.005006\n","449it [00:20, 17.63it/s]Train epoch: 1 [batch #450, batch_size 16, seq length 773]\tLoss: 0.005221\n","475it [00:21, 17.14it/s]Train epoch: 1 [batch #475, batch_size 16, seq length 789]\tLoss: 0.004770\n","499it [00:22, 16.67it/s]Train epoch: 1 [batch #500, batch_size 16, seq length 803]\tLoss: 0.005173\n","525it [00:24, 16.43it/s]Train epoch: 1 [batch #525, batch_size 16, seq length 818]\tLoss: 0.005132\n","549it [00:26, 16.41it/s]Train epoch: 1 [batch #550, batch_size 16, seq length 833]\tLoss: 0.005093\n","575it [00:27, 16.38it/s]Train epoch: 1 [batch #575, batch_size 16, seq length 846]\tLoss: 0.004985\n","599it [00:29, 16.44it/s]Train epoch: 1 [batch #600, batch_size 16, seq length 860]\tLoss: 0.005794\n","625it [00:30, 15.76it/s]Train epoch: 1 [batch #625, batch_size 16, seq length 874]\tLoss: 0.005401\n","649it [00:32, 16.04it/s]Train epoch: 1 [batch #650, batch_size 16, seq length 887]\tLoss: 0.004934\n","675it [00:33, 15.16it/s]Train epoch: 1 [batch #675, batch_size 16, seq length 901]\tLoss: 0.005234\n","699it [00:35, 15.24it/s]Train epoch: 1 [batch #700, batch_size 16, seq length 915]\tLoss: 0.005100\n","725it [00:37, 14.83it/s]Train epoch: 1 [batch #725, batch_size 16, seq length 929]\tLoss: 0.005470\n","749it [00:38, 14.75it/s]Train epoch: 1 [batch #750, batch_size 16, seq length 942]\tLoss: 0.005550\n","775it [00:40, 14.53it/s]Train epoch: 1 [batch #775, batch_size 16, seq length 954]\tLoss: 0.005629\n","799it [00:42, 14.36it/s]Train epoch: 1 [batch #800, batch_size 16, seq length 968]\tLoss: 0.005513\n","825it [00:44, 13.88it/s]Train epoch: 1 [batch #825, batch_size 16, seq length 981]\tLoss: 0.005665\n","849it [00:45, 14.08it/s]Train epoch: 1 [batch #850, batch_size 16, seq length 994]\tLoss: 0.005668\n","875it [00:47, 13.85it/s]Train epoch: 1 [batch #875, batch_size 16, seq length 1006]\tLoss: 0.005286\n","899it [00:49, 13.64it/s]Train epoch: 1 [batch #900, batch_size 16, seq length 1020]\tLoss: 0.005818\n","925it [00:51, 13.29it/s]Train epoch: 1 [batch #925, batch_size 16, seq length 1033]\tLoss: 0.005463\n","949it [00:53, 13.05it/s]Train epoch: 1 [batch #950, batch_size 16, seq length 1046]\tLoss: 0.005966\n","975it [00:55, 13.01it/s]Train epoch: 1 [batch #975, batch_size 16, seq length 1059]\tLoss: 0.005657\n","999it [00:57, 12.54it/s]Train epoch: 1 [batch #1000, batch_size 16, seq length 1073]\tLoss: 0.006096\n","1025it [00:59, 12.63it/s]Train epoch: 1 [batch #1025, batch_size 16, seq length 1085]\tLoss: 0.005931\n","1049it [01:01, 12.32it/s]Train epoch: 1 [batch #1050, batch_size 16, seq length 1099]\tLoss: 0.005991\n","1075it [01:03, 12.34it/s]Train epoch: 1 [batch #1075, batch_size 16, seq length 1113]\tLoss: 0.006025\n","1099it [01:05, 12.34it/s]Train epoch: 1 [batch #1100, batch_size 16, seq length 1126]\tLoss: 0.005815\n","1125it [01:07, 12.27it/s]Train epoch: 1 [batch #1125, batch_size 16, seq length 1140]\tLoss: 0.006134\n","1149it [01:09, 12.33it/s]Train epoch: 1 [batch #1150, batch_size 16, seq length 1154]\tLoss: 0.006145\n","1175it [01:11, 12.37it/s]Train epoch: 1 [batch #1175, batch_size 16, seq length 1167]\tLoss: 0.006136\n","1199it [01:13, 12.15it/s]Train epoch: 1 [batch #1200, batch_size 16, seq length 1180]\tLoss: 0.006154\n","1225it [01:15, 11.83it/s]Train epoch: 1 [batch #1225, batch_size 16, seq length 1195]\tLoss: 0.005975\n","1249it [01:17, 11.67it/s]Train epoch: 1 [batch #1250, batch_size 16, seq length 1208]\tLoss: 0.005871\n","1275it [01:19, 11.67it/s]Train epoch: 1 [batch #1275, batch_size 16, seq length 1220]\tLoss: 0.005870\n","1299it [01:21, 11.39it/s]Train epoch: 1 [batch #1300, batch_size 16, seq length 1234]\tLoss: 0.006247\n","1325it [01:24, 11.38it/s]Train epoch: 1 [batch #1325, batch_size 16, seq length 1248]\tLoss: 0.005689\n","1349it [01:26, 11.14it/s]Train epoch: 1 [batch #1350, batch_size 16, seq length 1262]\tLoss: 0.006255\n","1375it [01:28, 10.90it/s]Train epoch: 1 [batch #1375, batch_size 16, seq length 1276]\tLoss: 0.006839\n","1399it [01:30, 10.82it/s]Train epoch: 1 [batch #1400, batch_size 16, seq length 1290]\tLoss: 0.006452\n","1425it [01:33, 10.78it/s]Train epoch: 1 [batch #1425, batch_size 16, seq length 1304]\tLoss: 0.006446\n","1449it [01:35, 10.49it/s]Train epoch: 1 [batch #1450, batch_size 16, seq length 1319]\tLoss: 0.006338\n","1475it [01:38, 10.44it/s]Train epoch: 1 [batch #1475, batch_size 16, seq length 1332]\tLoss: 0.006816\n","1499it [01:40, 10.25it/s]Train epoch: 1 [batch #1500, batch_size 16, seq length 1346]\tLoss: 0.006489\n","1525it [01:42, 10.22it/s]Train epoch: 1 [batch #1525, batch_size 16, seq length 1362]\tLoss: 0.005878\n","1550it [01:45,  9.92it/s]Train epoch: 1 [batch #1550, batch_size 16, seq length 1377]\tLoss: 0.006735\n","1574it [01:47,  9.98it/s]Train epoch: 1 [batch #1575, batch_size 16, seq length 1392]\tLoss: 0.006126\n","1600it [01:50,  9.93it/s]Train epoch: 1 [batch #1600, batch_size 16, seq length 1406]\tLoss: 0.006219\n","1625it [01:53,  9.86it/s]Train epoch: 1 [batch #1625, batch_size 16, seq length 1422]\tLoss: 0.006641\n","1649it [01:55,  9.87it/s]Train epoch: 1 [batch #1650, batch_size 16, seq length 1437]\tLoss: 0.006432\n","1675it [01:58,  9.91it/s]Train epoch: 1 [batch #1675, batch_size 16, seq length 1454]\tLoss: 0.006721\n","1700it [02:00,  9.74it/s]Train epoch: 1 [batch #1700, batch_size 16, seq length 1469]\tLoss: 0.006842\n","1725it [02:03,  9.75it/s]Train epoch: 1 [batch #1725, batch_size 16, seq length 1484]\tLoss: 0.006155\n","1750it [02:05,  9.65it/s]Train epoch: 1 [batch #1750, batch_size 16, seq length 1501]\tLoss: 0.006339\n","1775it [02:08,  9.24it/s]Train epoch: 1 [batch #1775, batch_size 16, seq length 1518]\tLoss: 0.006717\n","1800it [02:11,  9.28it/s]Train epoch: 1 [batch #1800, batch_size 16, seq length 1533]\tLoss: 0.006839\n","1825it [02:13,  9.11it/s]Train epoch: 1 [batch #1825, batch_size 16, seq length 1548]\tLoss: 0.006975\n","1850it [02:16,  9.09it/s]Train epoch: 1 [batch #1850, batch_size 16, seq length 1566]\tLoss: 0.006450\n","1875it [02:19,  8.94it/s]Train epoch: 1 [batch #1875, batch_size 16, seq length 1582]\tLoss: 0.006725\n","1900it [02:22,  8.91it/s]Train epoch: 1 [batch #1900, batch_size 16, seq length 1599]\tLoss: 0.007333\n","1925it [02:25,  8.74it/s]Train epoch: 1 [batch #1925, batch_size 16, seq length 1616]\tLoss: 0.006931\n","1950it [02:27,  8.56it/s]Train epoch: 1 [batch #1950, batch_size 16, seq length 1633]\tLoss: 0.006742\n","1975it [02:30,  8.49it/s]Train epoch: 1 [batch #1975, batch_size 16, seq length 1651]\tLoss: 0.006813\n","2000it [02:33,  8.52it/s]Train epoch: 1 [batch #2000, batch_size 16, seq length 1670]\tLoss: 0.006492\n","2025it [02:36,  8.32it/s]Train epoch: 1 [batch #2025, batch_size 16, seq length 1688]\tLoss: 0.006557\n","2050it [02:39,  8.18it/s]Train epoch: 1 [batch #2050, batch_size 16, seq length 1707]\tLoss: 0.006866\n","2075it [02:42,  8.23it/s]Train epoch: 1 [batch #2075, batch_size 16, seq length 1728]\tLoss: 0.006991\n","2100it [02:45,  8.21it/s]Train epoch: 1 [batch #2100, batch_size 16, seq length 1748]\tLoss: 0.006398\n","2125it [02:48,  8.24it/s]Train epoch: 1 [batch #2125, batch_size 16, seq length 1768]\tLoss: 0.007301\n","2150it [02:52,  8.11it/s]Train epoch: 1 [batch #2150, batch_size 16, seq length 1787]\tLoss: 0.006732\n","2175it [02:55,  7.92it/s]Train epoch: 1 [batch #2175, batch_size 16, seq length 1809]\tLoss: 0.007559\n","2200it [02:58,  7.75it/s]Train epoch: 1 [batch #2200, batch_size 16, seq length 1833]\tLoss: 0.006939\n","2225it [03:01,  7.66it/s]Train epoch: 1 [batch #2225, batch_size 16, seq length 1855]\tLoss: 0.006984\n","2250it [03:04,  7.67it/s]Train epoch: 1 [batch #2250, batch_size 16, seq length 1878]\tLoss: 0.007654\n","2275it [03:08,  7.43it/s]Train epoch: 1 [batch #2275, batch_size 16, seq length 1901]\tLoss: 0.006951\n","2300it [03:11,  7.39it/s]Train epoch: 1 [batch #2300, batch_size 16, seq length 1927]\tLoss: 0.007143\n","2325it [03:15,  7.20it/s]Train epoch: 1 [batch #2325, batch_size 16, seq length 1953]\tLoss: 0.007693\n","2350it [03:18,  7.11it/s]Train epoch: 1 [batch #2350, batch_size 16, seq length 1980]\tLoss: 0.007186\n","2375it [03:22,  7.04it/s]Train epoch: 1 [batch #2375, batch_size 16, seq length 2006]\tLoss: 0.007624\n","2400it [03:25,  7.04it/s]Train epoch: 1 [batch #2400, batch_size 16, seq length 2034]\tLoss: 0.007438\n","2425it [03:29,  6.98it/s]Train epoch: 1 [batch #2425, batch_size 16, seq length 2064]\tLoss: 0.007236\n","2450it [03:32,  6.84it/s]Train epoch: 1 [batch #2450, batch_size 16, seq length 2094]\tLoss: 0.007523\n","2475it [03:36,  6.62it/s]Train epoch: 1 [batch #2475, batch_size 16, seq length 2126]\tLoss: 0.007976\n","2500it [03:40,  6.63it/s]Train epoch: 1 [batch #2500, batch_size 16, seq length 2160]\tLoss: 0.007685\n","2525it [03:44,  6.53it/s]Train epoch: 1 [batch #2525, batch_size 16, seq length 2192]\tLoss: 0.007856\n","2550it [03:48,  6.43it/s]Train epoch: 1 [batch #2550, batch_size 16, seq length 2228]\tLoss: 0.007604\n","2575it [03:52,  6.32it/s]Train epoch: 1 [batch #2575, batch_size 16, seq length 2266]\tLoss: 0.008102\n","2600it [03:56,  6.14it/s]Train epoch: 1 [batch #2600, batch_size 16, seq length 2307]\tLoss: 0.008432\n","2625it [04:00,  6.09it/s]Train epoch: 1 [batch #2625, batch_size 16, seq length 2350]\tLoss: 0.007755\n","2650it [04:04,  6.08it/s]Train epoch: 1 [batch #2650, batch_size 16, seq length 2392]\tLoss: 0.007775\n","2675it [04:08,  5.91it/s]Train epoch: 1 [batch #2675, batch_size 16, seq length 2439]\tLoss: 0.008131\n","2700it [04:12,  5.81it/s]Train epoch: 1 [batch #2700, batch_size 16, seq length 2491]\tLoss: 0.008221\n","2725it [04:17,  5.72it/s]Train epoch: 1 [batch #2725, batch_size 16, seq length 2500]\tLoss: 0.008322\n","2750it [04:21,  5.75it/s]Train epoch: 1 [batch #2750, batch_size 16, seq length 2500]\tLoss: 0.008254\n","2775it [04:25,  5.74it/s]Train epoch: 1 [batch #2775, batch_size 16, seq length 2500]\tLoss: 0.008742\n","2800it [04:30,  5.66it/s]Train epoch: 1 [batch #2800, batch_size 16, seq length 2500]\tLoss: 0.009001\n","2825it [04:34,  5.67it/s]Train epoch: 1 [batch #2825, batch_size 16, seq length 2500]\tLoss: 0.008513\n","2850it [04:39,  5.71it/s]Train epoch: 1 [batch #2850, batch_size 16, seq length 2500]\tLoss: 0.008873\n","2875it [04:43,  5.63it/s]Train epoch: 1 [batch #2875, batch_size 16, seq length 2500]\tLoss: 0.009665\n","2900it [04:47,  5.66it/s]Train epoch: 1 [batch #2900, batch_size 16, seq length 2500]\tLoss: 0.009506\n","2925it [04:52,  5.57it/s]Train epoch: 1 [batch #2925, batch_size 16, seq length 2500]\tLoss: 0.009847\n","2950it [04:56,  5.55it/s]Train epoch: 1 [batch #2950, batch_size 16, seq length 2500]\tLoss: 0.009579\n","2975it [05:01,  5.37it/s]Train epoch: 1 [batch #2975, batch_size 16, seq length 2500]\tLoss: 0.011578\n","2983it [05:02,  9.85it/s]\n","epoch loss: 0.006520477590234491\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:44, 36.59it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0104, 0.0234, 0.0138, 0.0174, 0.8418\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2091, 0.6140, 0.2408, 0.3459, 0.9736\n","rec_at_8: 0.2813\n","prec_at_8: 0.5392\n","rec_at_15: 0.3862\n","prec_at_15: 0.4087\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/rnn_May_04_023326\n","\n","EPOCH 2\n","0it [00:00, ?it/s]Train epoch: 2 [batch #0, batch_size 16, seq length 117]\tLoss: 0.006355\n","24it [00:00, 37.18it/s]Train epoch: 2 [batch #25, batch_size 16, seq length 337]\tLoss: 0.004297\n","48it [00:01, 33.67it/s]Train epoch: 2 [batch #50, batch_size 16, seq length 402]\tLoss: 0.003889\n","72it [00:02, 30.17it/s]Train epoch: 2 [batch #75, batch_size 16, seq length 452]\tLoss: 0.003557\n","100it [00:03, 27.08it/s]Train epoch: 2 [batch #100, batch_size 16, seq length 490]\tLoss: 0.003423\n","124it [00:04, 25.95it/s]Train epoch: 2 [batch #125, batch_size 16, seq length 520]\tLoss: 0.003846\n","148it [00:05, 24.69it/s]Train epoch: 2 [batch #150, batch_size 16, seq length 548]\tLoss: 0.003979\n","175it [00:06, 22.69it/s]Train epoch: 2 [batch #175, batch_size 16, seq length 574]\tLoss: 0.003612\n","199it [00:07, 22.63it/s]Train epoch: 2 [batch #200, batch_size 16, seq length 596]\tLoss: 0.003886\n","223it [00:08, 22.16it/s]Train epoch: 2 [batch #225, batch_size 16, seq length 618]\tLoss: 0.003973\n","250it [00:09, 21.43it/s]Train epoch: 2 [batch #250, batch_size 16, seq length 638]\tLoss: 0.003646\n","274it [00:10, 20.88it/s]Train epoch: 2 [batch #275, batch_size 16, seq length 656]\tLoss: 0.004082\n","298it [00:12, 20.07it/s]Train epoch: 2 [batch #300, batch_size 16, seq length 674]\tLoss: 0.004162\n","325it [00:13, 19.78it/s]Train epoch: 2 [batch #325, batch_size 16, seq length 693]\tLoss: 0.003836\n","349it [00:14, 19.34it/s]Train epoch: 2 [batch #350, batch_size 16, seq length 710]\tLoss: 0.003961\n","375it [00:16, 18.66it/s]Train epoch: 2 [batch #375, batch_size 16, seq length 726]\tLoss: 0.004523\n","399it [00:17, 18.24it/s]Train epoch: 2 [batch #400, batch_size 16, seq length 743]\tLoss: 0.003949\n","425it [00:18, 18.05it/s]Train epoch: 2 [batch #425, batch_size 16, seq length 758]\tLoss: 0.003977\n","449it [00:20, 17.67it/s]Train epoch: 2 [batch #450, batch_size 16, seq length 773]\tLoss: 0.004276\n","475it [00:21, 16.72it/s]Train epoch: 2 [batch #475, batch_size 16, seq length 789]\tLoss: 0.003778\n","499it [00:23, 16.55it/s]Train epoch: 2 [batch #500, batch_size 16, seq length 803]\tLoss: 0.004202\n","525it [00:24, 16.50it/s]Train epoch: 2 [batch #525, batch_size 16, seq length 818]\tLoss: 0.004142\n","549it [00:26, 16.44it/s]Train epoch: 2 [batch #550, batch_size 16, seq length 833]\tLoss: 0.004060\n","575it [00:27, 16.38it/s]Train epoch: 2 [batch #575, batch_size 16, seq length 846]\tLoss: 0.003999\n","599it [00:29, 16.37it/s]Train epoch: 2 [batch #600, batch_size 16, seq length 860]\tLoss: 0.004712\n","625it [00:30, 16.24it/s]Train epoch: 2 [batch #625, batch_size 16, seq length 874]\tLoss: 0.004404\n","649it [00:32, 15.84it/s]Train epoch: 2 [batch #650, batch_size 16, seq length 887]\tLoss: 0.004038\n","675it [00:34, 15.51it/s]Train epoch: 2 [batch #675, batch_size 16, seq length 901]\tLoss: 0.004170\n","699it [00:35, 15.11it/s]Train epoch: 2 [batch #700, batch_size 16, seq length 915]\tLoss: 0.004089\n","725it [00:37, 14.81it/s]Train epoch: 2 [batch #725, batch_size 16, seq length 929]\tLoss: 0.004386\n","749it [00:38, 14.69it/s]Train epoch: 2 [batch #750, batch_size 16, seq length 942]\tLoss: 0.004560\n","775it [00:40, 14.57it/s]Train epoch: 2 [batch #775, batch_size 16, seq length 954]\tLoss: 0.004531\n","799it [00:42, 14.26it/s]Train epoch: 2 [batch #800, batch_size 16, seq length 968]\tLoss: 0.004521\n","825it [00:44, 14.03it/s]Train epoch: 2 [batch #825, batch_size 16, seq length 981]\tLoss: 0.004575\n","849it [00:45, 13.97it/s]Train epoch: 2 [batch #850, batch_size 16, seq length 994]\tLoss: 0.004567\n","875it [00:47, 13.85it/s]Train epoch: 2 [batch #875, batch_size 16, seq length 1006]\tLoss: 0.004295\n","899it [00:49, 13.61it/s]Train epoch: 2 [batch #900, batch_size 16, seq length 1020]\tLoss: 0.004754\n","925it [00:51, 13.53it/s]Train epoch: 2 [batch #925, batch_size 16, seq length 1033]\tLoss: 0.004437\n","949it [00:53, 13.03it/s]Train epoch: 2 [batch #950, batch_size 16, seq length 1046]\tLoss: 0.004890\n","975it [00:55, 13.03it/s]Train epoch: 2 [batch #975, batch_size 16, seq length 1059]\tLoss: 0.004604\n","999it [00:57, 12.93it/s]Train epoch: 2 [batch #1000, batch_size 16, seq length 1073]\tLoss: 0.004780\n","1025it [00:59, 12.47it/s]Train epoch: 2 [batch #1025, batch_size 16, seq length 1085]\tLoss: 0.004865\n","1049it [01:01, 12.32it/s]Train epoch: 2 [batch #1050, batch_size 16, seq length 1099]\tLoss: 0.004873\n","1075it [01:03, 12.33it/s]Train epoch: 2 [batch #1075, batch_size 16, seq length 1113]\tLoss: 0.004890\n","1099it [01:05, 12.29it/s]Train epoch: 2 [batch #1100, batch_size 16, seq length 1126]\tLoss: 0.004691\n","1125it [01:07, 12.34it/s]Train epoch: 2 [batch #1125, batch_size 16, seq length 1140]\tLoss: 0.005031\n","1149it [01:09, 12.32it/s]Train epoch: 2 [batch #1150, batch_size 16, seq length 1154]\tLoss: 0.004999\n","1175it [01:11, 12.39it/s]Train epoch: 2 [batch #1175, batch_size 16, seq length 1167]\tLoss: 0.004990\n","1199it [01:13, 12.04it/s]Train epoch: 2 [batch #1200, batch_size 16, seq length 1180]\tLoss: 0.004946\n","1225it [01:15, 11.83it/s]Train epoch: 2 [batch #1225, batch_size 16, seq length 1195]\tLoss: 0.004895\n","1249it [01:17, 11.79it/s]Train epoch: 2 [batch #1250, batch_size 16, seq length 1208]\tLoss: 0.004818\n","1275it [01:19, 11.63it/s]Train epoch: 2 [batch #1275, batch_size 16, seq length 1220]\tLoss: 0.004827\n","1299it [01:22, 11.38it/s]Train epoch: 2 [batch #1300, batch_size 16, seq length 1234]\tLoss: 0.005142\n","1325it [01:24, 11.19it/s]Train epoch: 2 [batch #1325, batch_size 16, seq length 1248]\tLoss: 0.004705\n","1349it [01:26, 11.18it/s]Train epoch: 2 [batch #1350, batch_size 16, seq length 1262]\tLoss: 0.005154\n","1375it [01:28, 11.02it/s]Train epoch: 2 [batch #1375, batch_size 16, seq length 1276]\tLoss: 0.005682\n","1399it [01:31, 10.89it/s]Train epoch: 2 [batch #1400, batch_size 16, seq length 1290]\tLoss: 0.005332\n","1425it [01:33, 10.65it/s]Train epoch: 2 [batch #1425, batch_size 16, seq length 1304]\tLoss: 0.005344\n","1449it [01:35, 10.59it/s]Train epoch: 2 [batch #1450, batch_size 16, seq length 1319]\tLoss: 0.005277\n","1475it [01:38, 10.50it/s]Train epoch: 2 [batch #1475, batch_size 16, seq length 1332]\tLoss: 0.005572\n","1499it [01:40, 10.34it/s]Train epoch: 2 [batch #1500, batch_size 16, seq length 1346]\tLoss: 0.005391\n","1525it [01:43, 10.24it/s]Train epoch: 2 [batch #1525, batch_size 16, seq length 1362]\tLoss: 0.004799\n","1550it [01:45,  9.90it/s]Train epoch: 2 [batch #1550, batch_size 16, seq length 1377]\tLoss: 0.005625\n","1575it [01:48,  9.86it/s]Train epoch: 2 [batch #1575, batch_size 16, seq length 1392]\tLoss: 0.005037\n","1600it [01:50,  9.80it/s]Train epoch: 2 [batch #1600, batch_size 16, seq length 1406]\tLoss: 0.005078\n","1625it [01:53,  9.88it/s]Train epoch: 2 [batch #1625, batch_size 16, seq length 1422]\tLoss: 0.005580\n","1650it [01:55,  9.72it/s]Train epoch: 2 [batch #1650, batch_size 16, seq length 1437]\tLoss: 0.005374\n","1675it [01:58,  9.88it/s]Train epoch: 2 [batch #1675, batch_size 16, seq length 1454]\tLoss: 0.005623\n","1700it [02:00,  9.83it/s]Train epoch: 2 [batch #1700, batch_size 16, seq length 1469]\tLoss: 0.005724\n","1725it [02:03,  9.43it/s]Train epoch: 2 [batch #1725, batch_size 16, seq length 1484]\tLoss: 0.005157\n","1750it [02:05,  9.26it/s]Train epoch: 2 [batch #1750, batch_size 16, seq length 1501]\tLoss: 0.005204\n","1775it [02:08,  9.31it/s]Train epoch: 2 [batch #1775, batch_size 16, seq length 1518]\tLoss: 0.005652\n","1800it [02:11,  9.11it/s]Train epoch: 2 [batch #1800, batch_size 16, seq length 1533]\tLoss: 0.005765\n","1825it [02:14,  9.00it/s]Train epoch: 2 [batch #1825, batch_size 16, seq length 1548]\tLoss: 0.005814\n","1850it [02:16,  8.90it/s]Train epoch: 2 [batch #1850, batch_size 16, seq length 1566]\tLoss: 0.005403\n","1875it [02:19,  8.85it/s]Train epoch: 2 [batch #1875, batch_size 16, seq length 1582]\tLoss: 0.005651\n","1900it [02:22,  8.80it/s]Train epoch: 2 [batch #1900, batch_size 16, seq length 1599]\tLoss: 0.006099\n","1925it [02:25,  8.65it/s]Train epoch: 2 [batch #1925, batch_size 16, seq length 1616]\tLoss: 0.005790\n","1950it [02:28,  8.47it/s]Train epoch: 2 [batch #1950, batch_size 16, seq length 1633]\tLoss: 0.005605\n","1975it [02:31,  8.33it/s]Train epoch: 2 [batch #1975, batch_size 16, seq length 1651]\tLoss: 0.005674\n","2000it [02:34,  8.25it/s]Train epoch: 2 [batch #2000, batch_size 16, seq length 1670]\tLoss: 0.005485\n","2025it [02:37,  8.24it/s]Train epoch: 2 [batch #2025, batch_size 16, seq length 1688]\tLoss: 0.005486\n","2050it [02:40,  8.23it/s]Train epoch: 2 [batch #2050, batch_size 16, seq length 1707]\tLoss: 0.005733\n","2075it [02:43,  8.21it/s]Train epoch: 2 [batch #2075, batch_size 16, seq length 1728]\tLoss: 0.005929\n","2100it [02:46,  8.09it/s]Train epoch: 2 [batch #2100, batch_size 16, seq length 1748]\tLoss: 0.005378\n","2125it [02:49,  8.02it/s]Train epoch: 2 [batch #2125, batch_size 16, seq length 1768]\tLoss: 0.006255\n","2150it [02:52,  7.77it/s]Train epoch: 2 [batch #2150, batch_size 16, seq length 1787]\tLoss: 0.005574\n","2175it [02:55,  7.74it/s]Train epoch: 2 [batch #2175, batch_size 16, seq length 1809]\tLoss: 0.006333\n","2200it [02:59,  7.74it/s]Train epoch: 2 [batch #2200, batch_size 16, seq length 1833]\tLoss: 0.005899\n","2225it [03:02,  7.57it/s]Train epoch: 2 [batch #2225, batch_size 16, seq length 1855]\tLoss: 0.005923\n","2250it [03:05,  7.48it/s]Train epoch: 2 [batch #2250, batch_size 16, seq length 1878]\tLoss: 0.006502\n","2275it [03:09,  7.44it/s]Train epoch: 2 [batch #2275, batch_size 16, seq length 1901]\tLoss: 0.005923\n","2300it [03:12,  7.27it/s]Train epoch: 2 [batch #2300, batch_size 16, seq length 1927]\tLoss: 0.006085\n","2325it [03:16,  7.24it/s]Train epoch: 2 [batch #2325, batch_size 16, seq length 1953]\tLoss: 0.006511\n","2350it [03:19,  7.09it/s]Train epoch: 2 [batch #2350, batch_size 16, seq length 1980]\tLoss: 0.006084\n","2375it [03:23,  7.04it/s]Train epoch: 2 [batch #2375, batch_size 16, seq length 2006]\tLoss: 0.006482\n","2400it [03:26,  7.02it/s]Train epoch: 2 [batch #2400, batch_size 16, seq length 2034]\tLoss: 0.006381\n","2425it [03:30,  6.96it/s]Train epoch: 2 [batch #2425, batch_size 16, seq length 2064]\tLoss: 0.006160\n","2450it [03:33,  6.86it/s]Train epoch: 2 [batch #2450, batch_size 16, seq length 2094]\tLoss: 0.006491\n","2475it [03:37,  6.68it/s]Train epoch: 2 [batch #2475, batch_size 16, seq length 2126]\tLoss: 0.006719\n","2500it [03:41,  6.60it/s]Train epoch: 2 [batch #2500, batch_size 16, seq length 2160]\tLoss: 0.006575\n","2525it [03:45,  6.42it/s]Train epoch: 2 [batch #2525, batch_size 16, seq length 2192]\tLoss: 0.006762\n","2550it [03:49,  6.42it/s]Train epoch: 2 [batch #2550, batch_size 16, seq length 2228]\tLoss: 0.006531\n","2575it [03:53,  6.32it/s]Train epoch: 2 [batch #2575, batch_size 16, seq length 2266]\tLoss: 0.007064\n","2600it [03:57,  6.17it/s]Train epoch: 2 [batch #2600, batch_size 16, seq length 2307]\tLoss: 0.007209\n","2625it [04:01,  6.08it/s]Train epoch: 2 [batch #2625, batch_size 16, seq length 2350]\tLoss: 0.006700\n","2650it [04:05,  5.87it/s]Train epoch: 2 [batch #2650, batch_size 16, seq length 2392]\tLoss: 0.006647\n","2675it [04:09,  5.85it/s]Train epoch: 2 [batch #2675, batch_size 16, seq length 2439]\tLoss: 0.007040\n","2700it [04:13,  5.72it/s]Train epoch: 2 [batch #2700, batch_size 16, seq length 2491]\tLoss: 0.007173\n","2725it [04:18,  5.74it/s]Train epoch: 2 [batch #2725, batch_size 16, seq length 2500]\tLoss: 0.007182\n","2750it [04:22,  5.66it/s]Train epoch: 2 [batch #2750, batch_size 16, seq length 2500]\tLoss: 0.007089\n","2775it [04:27,  5.67it/s]Train epoch: 2 [batch #2775, batch_size 16, seq length 2500]\tLoss: 0.007541\n","2800it [04:31,  5.59it/s]Train epoch: 2 [batch #2800, batch_size 16, seq length 2500]\tLoss: 0.007714\n","2825it [04:36,  5.61it/s]Train epoch: 2 [batch #2825, batch_size 16, seq length 2500]\tLoss: 0.007342\n","2850it [04:40,  5.66it/s]Train epoch: 2 [batch #2850, batch_size 16, seq length 2500]\tLoss: 0.007652\n","2875it [04:44,  5.56it/s]Train epoch: 2 [batch #2875, batch_size 16, seq length 2500]\tLoss: 0.008337\n","2900it [04:49,  5.59it/s]Train epoch: 2 [batch #2900, batch_size 16, seq length 2500]\tLoss: 0.008338\n","2925it [04:53,  5.56it/s]Train epoch: 2 [batch #2925, batch_size 16, seq length 2500]\tLoss: 0.008621\n","2950it [04:58,  5.51it/s]Train epoch: 2 [batch #2950, batch_size 16, seq length 2500]\tLoss: 0.008473\n","2975it [05:02,  5.40it/s]Train epoch: 2 [batch #2975, batch_size 16, seq length 2500]\tLoss: 0.010231\n","2983it [05:04,  9.80it/s]\n","epoch loss: 0.005434122492917984\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:44, 36.71it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0215, 0.0399, 0.0289, 0.0335, 0.8603\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2770, 0.6039, 0.3386, 0.4339, 0.9770\n","rec_at_8: 0.3276\n","prec_at_8: 0.6140\n","rec_at_15: 0.4446\n","prec_at_15: 0.4626\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/rnn_May_04_023326\n","\n","EPOCH 3\n","0it [00:00, ?it/s]Train epoch: 3 [batch #0, batch_size 16, seq length 117]\tLoss: 0.005910\n","24it [00:00, 36.51it/s]Train epoch: 3 [batch #25, batch_size 16, seq length 337]\tLoss: 0.003737\n","48it [00:01, 33.39it/s]Train epoch: 3 [batch #50, batch_size 16, seq length 402]\tLoss: 0.003333\n","72it [00:02, 30.05it/s]Train epoch: 3 [batch #75, batch_size 16, seq length 452]\tLoss: 0.002974\n","100it [00:03, 27.29it/s]Train epoch: 3 [batch #100, batch_size 16, seq length 490]\tLoss: 0.002921\n","124it [00:04, 26.12it/s]Train epoch: 3 [batch #125, batch_size 16, seq length 520]\tLoss: 0.003251\n","148it [00:05, 24.83it/s]Train epoch: 3 [batch #150, batch_size 16, seq length 548]\tLoss: 0.003398\n","175it [00:06, 22.38it/s]Train epoch: 3 [batch #175, batch_size 16, seq length 574]\tLoss: 0.003140\n","199it [00:07, 22.86it/s]Train epoch: 3 [batch #200, batch_size 16, seq length 596]\tLoss: 0.003309\n","223it [00:08, 22.05it/s]Train epoch: 3 [batch #225, batch_size 16, seq length 618]\tLoss: 0.003367\n","250it [00:09, 21.62it/s]Train epoch: 3 [batch #250, batch_size 16, seq length 638]\tLoss: 0.003148\n","274it [00:10, 20.77it/s]Train epoch: 3 [batch #275, batch_size 16, seq length 656]\tLoss: 0.003518\n","300it [00:12, 19.83it/s]Train epoch: 3 [batch #300, batch_size 16, seq length 674]\tLoss: 0.003525\n","325it [00:13, 19.69it/s]Train epoch: 3 [batch #325, batch_size 16, seq length 693]\tLoss: 0.003280\n","349it [00:14, 18.95it/s]Train epoch: 3 [batch #350, batch_size 16, seq length 710]\tLoss: 0.003433\n","375it [00:16, 18.87it/s]Train epoch: 3 [batch #375, batch_size 16, seq length 726]\tLoss: 0.003913\n","399it [00:17, 18.18it/s]Train epoch: 3 [batch #400, batch_size 16, seq length 743]\tLoss: 0.003415\n","425it [00:18, 17.91it/s]Train epoch: 3 [batch #425, batch_size 16, seq length 758]\tLoss: 0.003412\n","449it [00:20, 17.61it/s]Train epoch: 3 [batch #450, batch_size 16, seq length 773]\tLoss: 0.003710\n","475it [00:21, 17.03it/s]Train epoch: 3 [batch #475, batch_size 16, seq length 789]\tLoss: 0.003272\n","499it [00:23, 16.59it/s]Train epoch: 3 [batch #500, batch_size 16, seq length 803]\tLoss: 0.003632\n","525it [00:24, 16.49it/s]Train epoch: 3 [batch #525, batch_size 16, seq length 818]\tLoss: 0.003572\n","549it [00:26, 16.47it/s]Train epoch: 3 [batch #550, batch_size 16, seq length 833]\tLoss: 0.003469\n","575it [00:27, 16.09it/s]Train epoch: 3 [batch #575, batch_size 16, seq length 846]\tLoss: 0.003416\n","599it [00:29, 16.36it/s]Train epoch: 3 [batch #600, batch_size 16, seq length 860]\tLoss: 0.004001\n","625it [00:30, 15.75it/s]Train epoch: 3 [batch #625, batch_size 16, seq length 874]\tLoss: 0.003733\n","649it [00:32, 15.57it/s]Train epoch: 3 [batch #650, batch_size 16, seq length 887]\tLoss: 0.003533\n","675it [00:34, 15.03it/s]Train epoch: 3 [batch #675, batch_size 16, seq length 901]\tLoss: 0.003600\n","699it [00:35, 14.93it/s]Train epoch: 3 [batch #700, batch_size 16, seq length 915]\tLoss: 0.003490\n","725it [00:37, 14.64it/s]Train epoch: 3 [batch #725, batch_size 16, seq length 929]\tLoss: 0.003779\n","749it [00:39, 14.69it/s]Train epoch: 3 [batch #750, batch_size 16, seq length 942]\tLoss: 0.003926\n","775it [00:40, 14.43it/s]Train epoch: 3 [batch #775, batch_size 16, seq length 954]\tLoss: 0.003883\n","799it [00:42, 13.84it/s]Train epoch: 3 [batch #800, batch_size 16, seq length 968]\tLoss: 0.003939\n","825it [00:44, 14.15it/s]Train epoch: 3 [batch #825, batch_size 16, seq length 981]\tLoss: 0.003953\n","849it [00:46, 13.75it/s]Train epoch: 3 [batch #850, batch_size 16, seq length 994]\tLoss: 0.003950\n","875it [00:48, 13.61it/s]Train epoch: 3 [batch #875, batch_size 16, seq length 1006]\tLoss: 0.003698\n","899it [00:49, 13.60it/s]Train epoch: 3 [batch #900, batch_size 16, seq length 1020]\tLoss: 0.004094\n","925it [00:51, 13.29it/s]Train epoch: 3 [batch #925, batch_size 16, seq length 1033]\tLoss: 0.003818\n","949it [00:53, 13.25it/s]Train epoch: 3 [batch #950, batch_size 16, seq length 1046]\tLoss: 0.004202\n","975it [00:55, 12.98it/s]Train epoch: 3 [batch #975, batch_size 16, seq length 1059]\tLoss: 0.003984\n","999it [00:57, 12.99it/s]Train epoch: 3 [batch #1000, batch_size 16, seq length 1073]\tLoss: 0.004063\n","1025it [00:59, 12.40it/s]Train epoch: 3 [batch #1025, batch_size 16, seq length 1085]\tLoss: 0.004245\n","1049it [01:01, 12.31it/s]Train epoch: 3 [batch #1050, batch_size 16, seq length 1099]\tLoss: 0.004209\n","1075it [01:03, 12.32it/s]Train epoch: 3 [batch #1075, batch_size 16, seq length 1113]\tLoss: 0.004225\n","1099it [01:05, 12.28it/s]Train epoch: 3 [batch #1100, batch_size 16, seq length 1126]\tLoss: 0.004006\n","1125it [01:07, 12.34it/s]Train epoch: 3 [batch #1125, batch_size 16, seq length 1140]\tLoss: 0.004344\n","1149it [01:09, 12.32it/s]Train epoch: 3 [batch #1150, batch_size 16, seq length 1154]\tLoss: 0.004317\n","1175it [01:11, 12.41it/s]Train epoch: 3 [batch #1175, batch_size 16, seq length 1167]\tLoss: 0.004340\n","1199it [01:13, 12.08it/s]Train epoch: 3 [batch #1200, batch_size 16, seq length 1180]\tLoss: 0.004249\n","1225it [01:15, 11.90it/s]Train epoch: 3 [batch #1225, batch_size 16, seq length 1195]\tLoss: 0.004253\n","1249it [01:17, 11.74it/s]Train epoch: 3 [batch #1250, batch_size 16, seq length 1208]\tLoss: 0.004205\n","1275it [01:20, 11.46it/s]Train epoch: 3 [batch #1275, batch_size 16, seq length 1220]\tLoss: 0.004218\n","1299it [01:22, 11.24it/s]Train epoch: 3 [batch #1300, batch_size 16, seq length 1234]\tLoss: 0.004463\n","1325it [01:24, 11.36it/s]Train epoch: 3 [batch #1325, batch_size 16, seq length 1248]\tLoss: 0.004122\n","1349it [01:26, 10.95it/s]Train epoch: 3 [batch #1350, batch_size 16, seq length 1262]\tLoss: 0.004508\n","1375it [01:28, 10.98it/s]Train epoch: 3 [batch #1375, batch_size 16, seq length 1276]\tLoss: 0.004845\n","1399it [01:31, 11.01it/s]Train epoch: 3 [batch #1400, batch_size 16, seq length 1290]\tLoss: 0.004703\n","1425it [01:33, 10.74it/s]Train epoch: 3 [batch #1425, batch_size 16, seq length 1304]\tLoss: 0.004709\n","1449it [01:35, 10.42it/s]Train epoch: 3 [batch #1450, batch_size 16, seq length 1319]\tLoss: 0.004590\n","1475it [01:38, 10.48it/s]Train epoch: 3 [batch #1475, batch_size 16, seq length 1332]\tLoss: 0.004865\n","1499it [01:40, 10.37it/s]Train epoch: 3 [batch #1500, batch_size 16, seq length 1346]\tLoss: 0.004662\n","1525it [01:43, 10.26it/s]Train epoch: 3 [batch #1525, batch_size 16, seq length 1362]\tLoss: 0.004149\n","1549it [01:45, 10.02it/s]Train epoch: 3 [batch #1550, batch_size 16, seq length 1377]\tLoss: 0.004895\n","1574it [01:48,  9.88it/s]Train epoch: 3 [batch #1575, batch_size 16, seq length 1392]\tLoss: 0.004347\n","1600it [01:50,  9.81it/s]Train epoch: 3 [batch #1600, batch_size 16, seq length 1406]\tLoss: 0.004454\n","1624it [01:53,  9.83it/s]Train epoch: 3 [batch #1625, batch_size 16, seq length 1422]\tLoss: 0.004885\n","1650it [01:55,  9.85it/s]Train epoch: 3 [batch #1650, batch_size 16, seq length 1437]\tLoss: 0.004703\n","1675it [01:58,  9.90it/s]Train epoch: 3 [batch #1675, batch_size 16, seq length 1454]\tLoss: 0.004846\n","1700it [02:00,  9.64it/s]Train epoch: 3 [batch #1700, batch_size 16, seq length 1469]\tLoss: 0.004993\n","1725it [02:03,  9.59it/s]Train epoch: 3 [batch #1725, batch_size 16, seq length 1484]\tLoss: 0.004547\n","1750it [02:06,  9.41it/s]Train epoch: 3 [batch #1750, batch_size 16, seq length 1501]\tLoss: 0.004541\n","1775it [02:08,  9.36it/s]Train epoch: 3 [batch #1775, batch_size 16, seq length 1518]\tLoss: 0.004960\n","1800it [02:11,  9.21it/s]Train epoch: 3 [batch #1800, batch_size 16, seq length 1533]\tLoss: 0.005053\n","1825it [02:14,  8.98it/s]Train epoch: 3 [batch #1825, batch_size 16, seq length 1548]\tLoss: 0.005089\n","1850it [02:17,  9.04it/s]Train epoch: 3 [batch #1850, batch_size 16, seq length 1566]\tLoss: 0.004715\n","1875it [02:19,  8.83it/s]Train epoch: 3 [batch #1875, batch_size 16, seq length 1582]\tLoss: 0.004945\n","1900it [02:22,  8.67it/s]Train epoch: 3 [batch #1900, batch_size 16, seq length 1599]\tLoss: 0.005272\n","1925it [02:25,  8.64it/s]Train epoch: 3 [batch #1925, batch_size 16, seq length 1616]\tLoss: 0.005038\n","1950it [02:28,  8.58it/s]Train epoch: 3 [batch #1950, batch_size 16, seq length 1633]\tLoss: 0.004958\n","1975it [02:31,  8.41it/s]Train epoch: 3 [batch #1975, batch_size 16, seq length 1651]\tLoss: 0.004966\n","2000it [02:34,  8.26it/s]Train epoch: 3 [batch #2000, batch_size 16, seq length 1670]\tLoss: 0.004813\n","2025it [02:37,  8.17it/s]Train epoch: 3 [batch #2025, batch_size 16, seq length 1688]\tLoss: 0.004818\n","2050it [02:40,  8.24it/s]Train epoch: 3 [batch #2050, batch_size 16, seq length 1707]\tLoss: 0.005028\n","2075it [02:43,  8.24it/s]Train epoch: 3 [batch #2075, batch_size 16, seq length 1728]\tLoss: 0.005212\n","2100it [02:46,  8.22it/s]Train epoch: 3 [batch #2100, batch_size 16, seq length 1748]\tLoss: 0.004848\n","2125it [02:49,  7.77it/s]Train epoch: 3 [batch #2125, batch_size 16, seq length 1768]\tLoss: 0.005468\n","2150it [02:52,  7.87it/s]Train epoch: 3 [batch #2150, batch_size 16, seq length 1787]\tLoss: 0.004849\n","2175it [02:56,  7.77it/s]Train epoch: 3 [batch #2175, batch_size 16, seq length 1809]\tLoss: 0.005592\n","2200it [02:59,  7.67it/s]Train epoch: 3 [batch #2200, batch_size 16, seq length 1833]\tLoss: 0.005164\n","2225it [03:02,  7.67it/s]Train epoch: 3 [batch #2225, batch_size 16, seq length 1855]\tLoss: 0.005203\n","2250it [03:05,  7.51it/s]Train epoch: 3 [batch #2250, batch_size 16, seq length 1878]\tLoss: 0.005702\n","2275it [03:09,  7.38it/s]Train epoch: 3 [batch #2275, batch_size 16, seq length 1901]\tLoss: 0.005230\n","2300it [03:12,  7.28it/s]Train epoch: 3 [batch #2300, batch_size 16, seq length 1927]\tLoss: 0.005340\n","2325it [03:16,  7.13it/s]Train epoch: 3 [batch #2325, batch_size 16, seq length 1953]\tLoss: 0.005741\n","2350it [03:19,  7.11it/s]Train epoch: 3 [batch #2350, batch_size 16, seq length 1980]\tLoss: 0.005329\n","2375it [03:23,  7.03it/s]Train epoch: 3 [batch #2375, batch_size 16, seq length 2006]\tLoss: 0.005730\n","2400it [03:26,  6.91it/s]Train epoch: 3 [batch #2400, batch_size 16, seq length 2034]\tLoss: 0.005594\n","2425it [03:30,  6.90it/s]Train epoch: 3 [batch #2425, batch_size 16, seq length 2064]\tLoss: 0.005443\n","2450it [03:34,  6.76it/s]Train epoch: 3 [batch #2450, batch_size 16, seq length 2094]\tLoss: 0.005748\n","2475it [03:37,  6.64it/s]Train epoch: 3 [batch #2475, batch_size 16, seq length 2126]\tLoss: 0.005930\n","2500it [03:41,  6.58it/s]Train epoch: 3 [batch #2500, batch_size 16, seq length 2160]\tLoss: 0.005796\n","2525it [03:45,  6.42it/s]Train epoch: 3 [batch #2525, batch_size 16, seq length 2192]\tLoss: 0.005950\n","2550it [03:49,  6.41it/s]Train epoch: 3 [batch #2550, batch_size 16, seq length 2228]\tLoss: 0.005825\n","2575it [03:53,  6.21it/s]Train epoch: 3 [batch #2575, batch_size 16, seq length 2266]\tLoss: 0.006244\n","2600it [03:57,  6.18it/s]Train epoch: 3 [batch #2600, batch_size 16, seq length 2307]\tLoss: 0.006289\n","2625it [04:01,  6.07it/s]Train epoch: 3 [batch #2625, batch_size 16, seq length 2350]\tLoss: 0.005981\n","2650it [04:05,  5.94it/s]Train epoch: 3 [batch #2650, batch_size 16, seq length 2392]\tLoss: 0.005904\n","2675it [04:09,  5.83it/s]Train epoch: 3 [batch #2675, batch_size 16, seq length 2439]\tLoss: 0.006258\n","2700it [04:14,  5.62it/s]Train epoch: 3 [batch #2700, batch_size 16, seq length 2491]\tLoss: 0.006376\n","2725it [04:18,  5.67it/s]Train epoch: 3 [batch #2725, batch_size 16, seq length 2500]\tLoss: 0.006361\n","2750it [04:23,  5.68it/s]Train epoch: 3 [batch #2750, batch_size 16, seq length 2500]\tLoss: 0.006119\n","2775it [04:27,  5.59it/s]Train epoch: 3 [batch #2775, batch_size 16, seq length 2500]\tLoss: 0.006650\n","2800it [04:32,  5.66it/s]Train epoch: 3 [batch #2800, batch_size 16, seq length 2500]\tLoss: 0.006706\n","2825it [04:36,  5.68it/s]Train epoch: 3 [batch #2825, batch_size 16, seq length 2500]\tLoss: 0.006474\n","2850it [04:40,  5.61it/s]Train epoch: 3 [batch #2850, batch_size 16, seq length 2500]\tLoss: 0.006718\n","2875it [04:45,  5.59it/s]Train epoch: 3 [batch #2875, batch_size 16, seq length 2500]\tLoss: 0.007390\n","2900it [04:49,  5.56it/s]Train epoch: 3 [batch #2900, batch_size 16, seq length 2500]\tLoss: 0.007399\n","2925it [04:54,  5.58it/s]Train epoch: 3 [batch #2925, batch_size 16, seq length 2500]\tLoss: 0.007686\n","2950it [04:58,  5.52it/s]Train epoch: 3 [batch #2950, batch_size 16, seq length 2500]\tLoss: 0.007589\n","2975it [05:03,  5.36it/s]Train epoch: 3 [batch #2975, batch_size 16, seq length 2500]\tLoss: 0.009073\n","2983it [05:04,  9.78it/s]\n","epoch loss: 0.004745645905562941\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:44, 36.61it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0277, 0.0470, 0.0393, 0.0428, 0.8598\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2892, 0.5470, 0.3803, 0.4486, 0.9765\n","rec_at_8: 0.3341\n","prec_at_8: 0.6222\n","rec_at_15: 0.4488\n","prec_at_15: 0.4654\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/rnn_May_04_023326\n","\n","EPOCH 4\n","0it [00:00, ?it/s]Train epoch: 4 [batch #0, batch_size 16, seq length 117]\tLoss: 0.005431\n","24it [00:00, 37.63it/s]Train epoch: 4 [batch #25, batch_size 16, seq length 337]\tLoss: 0.003340\n","48it [00:01, 32.99it/s]Train epoch: 4 [batch #50, batch_size 16, seq length 402]\tLoss: 0.002984\n","72it [00:02, 30.33it/s]Train epoch: 4 [batch #75, batch_size 16, seq length 452]\tLoss: 0.002636\n","100it [00:03, 27.43it/s]Train epoch: 4 [batch #100, batch_size 16, seq length 490]\tLoss: 0.002571\n","124it [00:04, 25.54it/s]Train epoch: 4 [batch #125, batch_size 16, seq length 520]\tLoss: 0.002877\n","148it [00:05, 24.94it/s]Train epoch: 4 [batch #150, batch_size 16, seq length 548]\tLoss: 0.002969\n","175it [00:06, 22.69it/s]Train epoch: 4 [batch #175, batch_size 16, seq length 574]\tLoss: 0.002822\n","199it [00:07, 22.78it/s]Train epoch: 4 [batch #200, batch_size 16, seq length 596]\tLoss: 0.002965\n","223it [00:08, 22.14it/s]Train epoch: 4 [batch #225, batch_size 16, seq length 618]\tLoss: 0.002983\n","250it [00:09, 21.27it/s]Train epoch: 4 [batch #250, batch_size 16, seq length 638]\tLoss: 0.002836\n","274it [00:10, 20.70it/s]Train epoch: 4 [batch #275, batch_size 16, seq length 656]\tLoss: 0.003128\n","298it [00:12, 20.42it/s]Train epoch: 4 [batch #300, batch_size 16, seq length 674]\tLoss: 0.003128\n","325it [00:13, 19.74it/s]Train epoch: 4 [batch #325, batch_size 16, seq length 693]\tLoss: 0.002913\n","349it [00:14, 19.01it/s]Train epoch: 4 [batch #350, batch_size 16, seq length 710]\tLoss: 0.003090\n","375it [00:16, 18.47it/s]Train epoch: 4 [batch #375, batch_size 16, seq length 726]\tLoss: 0.003491\n","399it [00:17, 18.44it/s]Train epoch: 4 [batch #400, batch_size 16, seq length 743]\tLoss: 0.003050\n","425it [00:18, 17.95it/s]Train epoch: 4 [batch #425, batch_size 16, seq length 758]\tLoss: 0.003023\n","449it [00:20, 17.37it/s]Train epoch: 4 [batch #450, batch_size 16, seq length 773]\tLoss: 0.003321\n","475it [00:21, 17.04it/s]Train epoch: 4 [batch #475, batch_size 16, seq length 789]\tLoss: 0.002939\n","499it [00:23, 16.66it/s]Train epoch: 4 [batch #500, batch_size 16, seq length 803]\tLoss: 0.003315\n","525it [00:24, 16.28it/s]Train epoch: 4 [batch #525, batch_size 16, seq length 818]\tLoss: 0.003202\n","549it [00:26, 16.35it/s]Train epoch: 4 [batch #550, batch_size 16, seq length 833]\tLoss: 0.003076\n","575it [00:27, 16.40it/s]Train epoch: 4 [batch #575, batch_size 16, seq length 846]\tLoss: 0.003046\n","599it [00:29, 16.34it/s]Train epoch: 4 [batch #600, batch_size 16, seq length 860]\tLoss: 0.003476\n","625it [00:30, 15.63it/s]Train epoch: 4 [batch #625, batch_size 16, seq length 874]\tLoss: 0.003304\n","649it [00:32, 15.93it/s]Train epoch: 4 [batch #650, batch_size 16, seq length 887]\tLoss: 0.003148\n","675it [00:34, 15.02it/s]Train epoch: 4 [batch #675, batch_size 16, seq length 901]\tLoss: 0.003207\n","699it [00:35, 15.03it/s]Train epoch: 4 [batch #700, batch_size 16, seq length 915]\tLoss: 0.003077\n","725it [00:37, 15.02it/s]Train epoch: 4 [batch #725, batch_size 16, seq length 929]\tLoss: 0.003319\n","749it [00:38, 14.81it/s]Train epoch: 4 [batch #750, batch_size 16, seq length 942]\tLoss: 0.003467\n","775it [00:40, 14.62it/s]Train epoch: 4 [batch #775, batch_size 16, seq length 954]\tLoss: 0.003373\n","799it [00:42, 14.30it/s]Train epoch: 4 [batch #800, batch_size 16, seq length 968]\tLoss: 0.003471\n","825it [00:44, 13.98it/s]Train epoch: 4 [batch #825, batch_size 16, seq length 981]\tLoss: 0.003505\n","849it [00:45, 13.86it/s]Train epoch: 4 [batch #850, batch_size 16, seq length 994]\tLoss: 0.003484\n","875it [00:47, 13.87it/s]Train epoch: 4 [batch #875, batch_size 16, seq length 1006]\tLoss: 0.003302\n","899it [00:49, 13.68it/s]Train epoch: 4 [batch #900, batch_size 16, seq length 1020]\tLoss: 0.003658\n","925it [00:51, 13.51it/s]Train epoch: 4 [batch #925, batch_size 16, seq length 1033]\tLoss: 0.003369\n","949it [00:53, 13.20it/s]Train epoch: 4 [batch #950, batch_size 16, seq length 1046]\tLoss: 0.003705\n","975it [00:55, 12.97it/s]Train epoch: 4 [batch #975, batch_size 16, seq length 1059]\tLoss: 0.003545\n","999it [00:57, 12.89it/s]Train epoch: 4 [batch #1000, batch_size 16, seq length 1073]\tLoss: 0.003540\n","1025it [00:59, 12.38it/s]Train epoch: 4 [batch #1025, batch_size 16, seq length 1085]\tLoss: 0.003794\n","1049it [01:01, 12.40it/s]Train epoch: 4 [batch #1050, batch_size 16, seq length 1099]\tLoss: 0.003754\n","1075it [01:03, 12.37it/s]Train epoch: 4 [batch #1075, batch_size 16, seq length 1113]\tLoss: 0.003770\n","1099it [01:05, 12.21it/s]Train epoch: 4 [batch #1100, batch_size 16, seq length 1126]\tLoss: 0.003564\n","1125it [01:07, 12.35it/s]Train epoch: 4 [batch #1125, batch_size 16, seq length 1140]\tLoss: 0.003846\n","1149it [01:09, 12.39it/s]Train epoch: 4 [batch #1150, batch_size 16, seq length 1154]\tLoss: 0.003795\n","1175it [01:11, 12.36it/s]Train epoch: 4 [batch #1175, batch_size 16, seq length 1167]\tLoss: 0.003877\n","1199it [01:13, 12.18it/s]Train epoch: 4 [batch #1200, batch_size 16, seq length 1180]\tLoss: 0.003811\n","1225it [01:15, 11.95it/s]Train epoch: 4 [batch #1225, batch_size 16, seq length 1195]\tLoss: 0.003760\n","1249it [01:17, 11.76it/s]Train epoch: 4 [batch #1250, batch_size 16, seq length 1208]\tLoss: 0.003739\n","1275it [01:19, 11.70it/s]Train epoch: 4 [batch #1275, batch_size 16, seq length 1220]\tLoss: 0.003743\n","1299it [01:21, 11.57it/s]Train epoch: 4 [batch #1300, batch_size 16, seq length 1234]\tLoss: 0.003993\n","1325it [01:24, 11.23it/s]Train epoch: 4 [batch #1325, batch_size 16, seq length 1248]\tLoss: 0.003657\n","1349it [01:26, 11.12it/s]Train epoch: 4 [batch #1350, batch_size 16, seq length 1262]\tLoss: 0.004022\n","1375it [01:28, 11.04it/s]Train epoch: 4 [batch #1375, batch_size 16, seq length 1276]\tLoss: 0.004252\n","1399it [01:30, 10.94it/s]Train epoch: 4 [batch #1400, batch_size 16, seq length 1290]\tLoss: 0.004223\n","1425it [01:33, 10.82it/s]Train epoch: 4 [batch #1425, batch_size 16, seq length 1304]\tLoss: 0.004206\n","1449it [01:35, 10.59it/s]Train epoch: 4 [batch #1450, batch_size 16, seq length 1319]\tLoss: 0.004033\n","1475it [01:37, 10.61it/s]Train epoch: 4 [batch #1475, batch_size 16, seq length 1332]\tLoss: 0.004266\n","1499it [01:40, 10.42it/s]Train epoch: 4 [batch #1500, batch_size 16, seq length 1346]\tLoss: 0.004154\n","1525it [01:42, 10.21it/s]Train epoch: 4 [batch #1525, batch_size 16, seq length 1362]\tLoss: 0.003678\n","1549it [01:45, 10.12it/s]Train epoch: 4 [batch #1550, batch_size 16, seq length 1377]\tLoss: 0.004348\n","1575it [01:47,  9.88it/s]Train epoch: 4 [batch #1575, batch_size 16, seq length 1392]\tLoss: 0.003802\n","1600it [01:50,  9.83it/s]Train epoch: 4 [batch #1600, batch_size 16, seq length 1406]\tLoss: 0.003960\n","1625it [01:52,  9.85it/s]Train epoch: 4 [batch #1625, batch_size 16, seq length 1422]\tLoss: 0.004381\n","1649it [01:55,  9.84it/s]Train epoch: 4 [batch #1650, batch_size 16, seq length 1437]\tLoss: 0.004213\n","1675it [01:57,  9.89it/s]Train epoch: 4 [batch #1675, batch_size 16, seq length 1454]\tLoss: 0.004271\n","1700it [02:00,  9.85it/s]Train epoch: 4 [batch #1700, batch_size 16, seq length 1469]\tLoss: 0.004485\n","1725it [02:02,  9.78it/s]Train epoch: 4 [batch #1725, batch_size 16, seq length 1484]\tLoss: 0.004161\n","1750it [02:05,  9.22it/s]Train epoch: 4 [batch #1750, batch_size 16, seq length 1501]\tLoss: 0.004060\n","1775it [02:08,  9.36it/s]Train epoch: 4 [batch #1775, batch_size 16, seq length 1518]\tLoss: 0.004420\n","1800it [02:10,  9.18it/s]Train epoch: 4 [batch #1800, batch_size 16, seq length 1533]\tLoss: 0.004599\n","1825it [02:13,  9.12it/s]Train epoch: 4 [batch #1825, batch_size 16, seq length 1548]\tLoss: 0.004574\n","1850it [02:16,  8.88it/s]Train epoch: 4 [batch #1850, batch_size 16, seq length 1566]\tLoss: 0.004202\n","1875it [02:19,  8.86it/s]Train epoch: 4 [batch #1875, batch_size 16, seq length 1582]\tLoss: 0.004435\n","1900it [02:22,  8.72it/s]Train epoch: 4 [batch #1900, batch_size 16, seq length 1599]\tLoss: 0.004605\n","1925it [02:25,  8.78it/s]Train epoch: 4 [batch #1925, batch_size 16, seq length 1616]\tLoss: 0.004498\n","1950it [02:27,  8.57it/s]Train epoch: 4 [batch #1950, batch_size 16, seq length 1633]\tLoss: 0.004406\n","1975it [02:30,  8.51it/s]Train epoch: 4 [batch #1975, batch_size 16, seq length 1651]\tLoss: 0.004442\n","2000it [02:33,  8.21it/s]Train epoch: 4 [batch #2000, batch_size 16, seq length 1670]\tLoss: 0.004315\n","2025it [02:36,  8.21it/s]Train epoch: 4 [batch #2025, batch_size 16, seq length 1688]\tLoss: 0.004364\n","2050it [02:40,  8.19it/s]Train epoch: 4 [batch #2050, batch_size 16, seq length 1707]\tLoss: 0.004513\n","2075it [02:43,  8.14it/s]Train epoch: 4 [batch #2075, batch_size 16, seq length 1728]\tLoss: 0.004650\n","2100it [02:46,  8.22it/s]Train epoch: 4 [batch #2100, batch_size 16, seq length 1748]\tLoss: 0.004387\n","2125it [02:49,  7.86it/s]Train epoch: 4 [batch #2125, batch_size 16, seq length 1768]\tLoss: 0.004892\n","2150it [02:52,  7.99it/s]Train epoch: 4 [batch #2150, batch_size 16, seq length 1787]\tLoss: 0.004353\n","2175it [02:55,  7.91it/s]Train epoch: 4 [batch #2175, batch_size 16, seq length 1809]\tLoss: 0.004989\n","2200it [02:58,  7.77it/s]Train epoch: 4 [batch #2200, batch_size 16, seq length 1833]\tLoss: 0.004639\n","2225it [03:02,  7.51it/s]Train epoch: 4 [batch #2225, batch_size 16, seq length 1855]\tLoss: 0.004723\n","2250it [03:05,  7.42it/s]Train epoch: 4 [batch #2250, batch_size 16, seq length 1878]\tLoss: 0.005167\n","2275it [03:08,  7.32it/s]Train epoch: 4 [batch #2275, batch_size 16, seq length 1901]\tLoss: 0.004727\n","2300it [03:12,  7.28it/s]Train epoch: 4 [batch #2300, batch_size 16, seq length 1927]\tLoss: 0.004845\n","2325it [03:15,  7.05it/s]Train epoch: 4 [batch #2325, batch_size 16, seq length 1953]\tLoss: 0.005154\n","2350it [03:19,  7.09it/s]Train epoch: 4 [batch #2350, batch_size 16, seq length 1980]\tLoss: 0.004829\n","2375it [03:22,  7.07it/s]Train epoch: 4 [batch #2375, batch_size 16, seq length 2006]\tLoss: 0.005126\n","2400it [03:26,  7.00it/s]Train epoch: 4 [batch #2400, batch_size 16, seq length 2034]\tLoss: 0.005028\n","2425it [03:29,  6.81it/s]Train epoch: 4 [batch #2425, batch_size 16, seq length 2064]\tLoss: 0.004932\n","2450it [03:33,  6.70it/s]Train epoch: 4 [batch #2450, batch_size 16, seq length 2094]\tLoss: 0.005228\n","2475it [03:37,  6.51it/s]Train epoch: 4 [batch #2475, batch_size 16, seq length 2126]\tLoss: 0.005356\n","2500it [03:41,  6.54it/s]Train epoch: 4 [batch #2500, batch_size 16, seq length 2160]\tLoss: 0.005251\n","2525it [03:45,  6.48it/s]Train epoch: 4 [batch #2525, batch_size 16, seq length 2192]\tLoss: 0.005463\n","2550it [03:48,  6.41it/s]Train epoch: 4 [batch #2550, batch_size 16, seq length 2228]\tLoss: 0.005278\n","2575it [03:52,  6.29it/s]Train epoch: 4 [batch #2575, batch_size 16, seq length 2266]\tLoss: 0.005654\n","2600it [03:56,  6.22it/s]Train epoch: 4 [batch #2600, batch_size 16, seq length 2307]\tLoss: 0.005628\n","2625it [04:00,  6.11it/s]Train epoch: 4 [batch #2625, batch_size 16, seq length 2350]\tLoss: 0.005473\n","2650it [04:05,  5.93it/s]Train epoch: 4 [batch #2650, batch_size 16, seq length 2392]\tLoss: 0.005444\n","2675it [04:09,  5.85it/s]Train epoch: 4 [batch #2675, batch_size 16, seq length 2439]\tLoss: 0.005731\n","2700it [04:13,  5.78it/s]Train epoch: 4 [batch #2700, batch_size 16, seq length 2491]\tLoss: 0.005812\n","2725it [04:18,  5.62it/s]Train epoch: 4 [batch #2725, batch_size 16, seq length 2500]\tLoss: 0.005694\n","2750it [04:22,  5.61it/s]Train epoch: 4 [batch #2750, batch_size 16, seq length 2500]\tLoss: 0.005520\n","2775it [04:27,  5.67it/s]Train epoch: 4 [batch #2775, batch_size 16, seq length 2500]\tLoss: 0.005893\n","2800it [04:31,  5.54it/s]Train epoch: 4 [batch #2800, batch_size 16, seq length 2500]\tLoss: 0.005974\n","2825it [04:35,  5.67it/s]Train epoch: 4 [batch #2825, batch_size 16, seq length 2500]\tLoss: 0.005908\n","2850it [04:40,  5.64it/s]Train epoch: 4 [batch #2850, batch_size 16, seq length 2500]\tLoss: 0.006082\n","2875it [04:44,  5.63it/s]Train epoch: 4 [batch #2875, batch_size 16, seq length 2500]\tLoss: 0.006639\n","2900it [04:49,  5.54it/s]Train epoch: 4 [batch #2900, batch_size 16, seq length 2500]\tLoss: 0.006701\n","2925it [04:53,  5.57it/s]Train epoch: 4 [batch #2925, batch_size 16, seq length 2500]\tLoss: 0.007013\n","2950it [04:58,  5.48it/s]Train epoch: 4 [batch #2950, batch_size 16, seq length 2500]\tLoss: 0.006868\n","2975it [05:02,  5.41it/s]Train epoch: 4 [batch #2975, batch_size 16, seq length 2500]\tLoss: 0.008238\n","2983it [05:04,  9.80it/s]\n","epoch loss: 0.004251134674980527\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:44, 36.70it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0298, 0.0502, 0.0422, 0.0459, 0.8536\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2912, 0.5290, 0.3932, 0.4511, 0.9761\n","rec_at_8: 0.3346\n","prec_at_8: 0.6213\n","rec_at_15: 0.4502\n","prec_at_15: 0.4650\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/rnn_May_04_023326\n","\n","EPOCH 5\n","0it [00:00, ?it/s]Train epoch: 5 [batch #0, batch_size 16, seq length 117]\tLoss: 0.005127\n","25it [00:00, 37.60it/s]Train epoch: 5 [batch #25, batch_size 16, seq length 337]\tLoss: 0.003143\n","49it [00:01, 33.34it/s]Train epoch: 5 [batch #50, batch_size 16, seq length 402]\tLoss: 0.002709\n","73it [00:02, 30.37it/s]Train epoch: 5 [batch #75, batch_size 16, seq length 452]\tLoss: 0.002451\n","98it [00:03, 27.69it/s]Train epoch: 5 [batch #100, batch_size 16, seq length 490]\tLoss: 0.002328\n","125it [00:04, 26.13it/s]Train epoch: 5 [batch #125, batch_size 16, seq length 520]\tLoss: 0.002622\n","149it [00:05, 24.59it/s]Train epoch: 5 [batch #150, batch_size 16, seq length 548]\tLoss: 0.002664\n","173it [00:06, 23.98it/s]Train epoch: 5 [batch #175, batch_size 16, seq length 574]\tLoss: 0.002589\n","200it [00:07, 23.26it/s]Train epoch: 5 [batch #200, batch_size 16, seq length 596]\tLoss: 0.002693\n","224it [00:08, 22.51it/s]Train epoch: 5 [batch #225, batch_size 16, seq length 618]\tLoss: 0.002712\n","248it [00:09, 21.60it/s]Train epoch: 5 [batch #250, batch_size 16, seq length 638]\tLoss: 0.002559\n","275it [00:10, 20.76it/s]Train epoch: 5 [batch #275, batch_size 16, seq length 656]\tLoss: 0.002839\n","299it [00:11, 20.25it/s]Train epoch: 5 [batch #300, batch_size 16, seq length 674]\tLoss: 0.002796\n","325it [00:13, 19.80it/s]Train epoch: 5 [batch #325, batch_size 16, seq length 693]\tLoss: 0.002628\n","349it [00:14, 19.47it/s]Train epoch: 5 [batch #350, batch_size 16, seq length 710]\tLoss: 0.002812\n","375it [00:15, 18.81it/s]Train epoch: 5 [batch #375, batch_size 16, seq length 726]\tLoss: 0.003136\n","399it [00:17, 18.42it/s]Train epoch: 5 [batch #400, batch_size 16, seq length 743]\tLoss: 0.002736\n","425it [00:18, 18.24it/s]Train epoch: 5 [batch #425, batch_size 16, seq length 758]\tLoss: 0.002744\n","449it [00:19, 17.65it/s]Train epoch: 5 [batch #450, batch_size 16, seq length 773]\tLoss: 0.003021\n","475it [00:21, 17.24it/s]Train epoch: 5 [batch #475, batch_size 16, seq length 789]\tLoss: 0.002708\n","499it [00:22, 17.11it/s]Train epoch: 5 [batch #500, batch_size 16, seq length 803]\tLoss: 0.003006\n","525it [00:24, 16.46it/s]Train epoch: 5 [batch #525, batch_size 16, seq length 818]\tLoss: 0.002931\n","549it [00:25, 16.35it/s]Train epoch: 5 [batch #550, batch_size 16, seq length 833]\tLoss: 0.002784\n","575it [00:27, 16.52it/s]Train epoch: 5 [batch #575, batch_size 16, seq length 846]\tLoss: 0.002752\n","599it [00:28, 16.27it/s]Train epoch: 5 [batch #600, batch_size 16, seq length 860]\tLoss: 0.003099\n","625it [00:30, 15.90it/s]Train epoch: 5 [batch #625, batch_size 16, seq length 874]\tLoss: 0.002989\n","649it [00:32, 15.84it/s]Train epoch: 5 [batch #650, batch_size 16, seq length 887]\tLoss: 0.002830\n","675it [00:33, 15.46it/s]Train epoch: 5 [batch #675, batch_size 16, seq length 901]\tLoss: 0.002910\n","699it [00:35, 15.03it/s]Train epoch: 5 [batch #700, batch_size 16, seq length 915]\tLoss: 0.002790\n","725it [00:37, 14.99it/s]Train epoch: 5 [batch #725, batch_size 16, seq length 929]\tLoss: 0.002988\n","749it [00:38, 14.60it/s]Train epoch: 5 [batch #750, batch_size 16, seq length 942]\tLoss: 0.003120\n","775it [00:40, 14.53it/s]Train epoch: 5 [batch #775, batch_size 16, seq length 954]\tLoss: 0.003093\n","799it [00:42, 14.23it/s]Train epoch: 5 [batch #800, batch_size 16, seq length 968]\tLoss: 0.003118\n","825it [00:44, 14.06it/s]Train epoch: 5 [batch #825, batch_size 16, seq length 981]\tLoss: 0.003123\n","849it [00:45, 13.94it/s]Train epoch: 5 [batch #850, batch_size 16, seq length 994]\tLoss: 0.003178\n","875it [00:47, 13.77it/s]Train epoch: 5 [batch #875, batch_size 16, seq length 1006]\tLoss: 0.002969\n","899it [00:49, 13.61it/s]Train epoch: 5 [batch #900, batch_size 16, seq length 1020]\tLoss: 0.003326\n","925it [00:51, 13.43it/s]Train epoch: 5 [batch #925, batch_size 16, seq length 1033]\tLoss: 0.003059\n","949it [00:53, 13.18it/s]Train epoch: 5 [batch #950, batch_size 16, seq length 1046]\tLoss: 0.003317\n","975it [00:55, 13.12it/s]Train epoch: 5 [batch #975, batch_size 16, seq length 1059]\tLoss: 0.003229\n","999it [00:56, 12.85it/s]Train epoch: 5 [batch #1000, batch_size 16, seq length 1073]\tLoss: 0.003201\n","1025it [00:59, 12.69it/s]Train epoch: 5 [batch #1025, batch_size 16, seq length 1085]\tLoss: 0.003422\n","1049it [01:00, 12.48it/s]Train epoch: 5 [batch #1050, batch_size 16, seq length 1099]\tLoss: 0.003360\n","1075it [01:03, 12.31it/s]Train epoch: 5 [batch #1075, batch_size 16, seq length 1113]\tLoss: 0.003405\n","1099it [01:05, 12.24it/s]Train epoch: 5 [batch #1100, batch_size 16, seq length 1126]\tLoss: 0.003210\n","1125it [01:07, 12.34it/s]Train epoch: 5 [batch #1125, batch_size 16, seq length 1140]\tLoss: 0.003492\n","1149it [01:09, 12.34it/s]Train epoch: 5 [batch #1150, batch_size 16, seq length 1154]\tLoss: 0.003355\n","1175it [01:11, 12.04it/s]Train epoch: 5 [batch #1175, batch_size 16, seq length 1167]\tLoss: 0.003483\n","1199it [01:13, 12.08it/s]Train epoch: 5 [batch #1200, batch_size 16, seq length 1180]\tLoss: 0.003429\n","1225it [01:15, 11.86it/s]Train epoch: 5 [batch #1225, batch_size 16, seq length 1195]\tLoss: 0.003415\n","1249it [01:17, 11.77it/s]Train epoch: 5 [batch #1250, batch_size 16, seq length 1208]\tLoss: 0.003375\n","1275it [01:19, 11.48it/s]Train epoch: 5 [batch #1275, batch_size 16, seq length 1220]\tLoss: 0.003451\n","1299it [01:21, 11.59it/s]Train epoch: 5 [batch #1300, batch_size 16, seq length 1234]\tLoss: 0.003667\n","1325it [01:23, 11.41it/s]Train epoch: 5 [batch #1325, batch_size 16, seq length 1248]\tLoss: 0.003388\n","1349it [01:26, 11.04it/s]Train epoch: 5 [batch #1350, batch_size 16, seq length 1262]\tLoss: 0.003630\n","1375it [01:28, 10.98it/s]Train epoch: 5 [batch #1375, batch_size 16, seq length 1276]\tLoss: 0.003850\n","1399it [01:30, 11.01it/s]Train epoch: 5 [batch #1400, batch_size 16, seq length 1290]\tLoss: 0.003789\n","1425it [01:33, 10.81it/s]Train epoch: 5 [batch #1425, batch_size 16, seq length 1304]\tLoss: 0.003874\n","1449it [01:35, 10.54it/s]Train epoch: 5 [batch #1450, batch_size 16, seq length 1319]\tLoss: 0.003672\n","1475it [01:37, 10.42it/s]Train epoch: 5 [batch #1475, batch_size 16, seq length 1332]\tLoss: 0.003871\n","1499it [01:40, 10.40it/s]Train epoch: 5 [batch #1500, batch_size 16, seq length 1346]\tLoss: 0.003725\n","1525it [01:42, 10.36it/s]Train epoch: 5 [batch #1525, batch_size 16, seq length 1362]\tLoss: 0.003320\n","1550it [01:45,  9.92it/s]Train epoch: 5 [batch #1550, batch_size 16, seq length 1377]\tLoss: 0.003942\n","1575it [01:47,  9.86it/s]Train epoch: 5 [batch #1575, batch_size 16, seq length 1392]\tLoss: 0.003487\n","1600it [01:50,  9.88it/s]Train epoch: 5 [batch #1600, batch_size 16, seq length 1406]\tLoss: 0.003624\n","1625it [01:52,  9.88it/s]Train epoch: 5 [batch #1625, batch_size 16, seq length 1422]\tLoss: 0.003971\n","1650it [01:55,  9.94it/s]Train epoch: 5 [batch #1650, batch_size 16, seq length 1437]\tLoss: 0.003787\n","1675it [01:57,  9.80it/s]Train epoch: 5 [batch #1675, batch_size 16, seq length 1454]\tLoss: 0.003866\n","1700it [02:00,  9.74it/s]Train epoch: 5 [batch #1700, batch_size 16, seq length 1469]\tLoss: 0.004085\n","1725it [02:02,  9.77it/s]Train epoch: 5 [batch #1725, batch_size 16, seq length 1484]\tLoss: 0.003770\n","1750it [02:05,  9.63it/s]Train epoch: 5 [batch #1750, batch_size 16, seq length 1501]\tLoss: 0.003711\n","1775it [02:08,  9.41it/s]Train epoch: 5 [batch #1775, batch_size 16, seq length 1518]\tLoss: 0.004009\n","1800it [02:10,  9.30it/s]Train epoch: 5 [batch #1800, batch_size 16, seq length 1533]\tLoss: 0.004180\n","1825it [02:13,  9.24it/s]Train epoch: 5 [batch #1825, batch_size 16, seq length 1548]\tLoss: 0.004178\n","1850it [02:16,  8.90it/s]Train epoch: 5 [batch #1850, batch_size 16, seq length 1566]\tLoss: 0.003830\n","1875it [02:19,  9.04it/s]Train epoch: 5 [batch #1875, batch_size 16, seq length 1582]\tLoss: 0.004072\n","1900it [02:22,  8.75it/s]Train epoch: 5 [batch #1900, batch_size 16, seq length 1599]\tLoss: 0.004217\n","1925it [02:24,  8.64it/s]Train epoch: 5 [batch #1925, batch_size 16, seq length 1616]\tLoss: 0.004119\n","1950it [02:27,  8.66it/s]Train epoch: 5 [batch #1950, batch_size 16, seq length 1633]\tLoss: 0.004062\n","1975it [02:30,  8.41it/s]Train epoch: 5 [batch #1975, batch_size 16, seq length 1651]\tLoss: 0.004089\n","2000it [02:33,  8.35it/s]Train epoch: 5 [batch #2000, batch_size 16, seq length 1670]\tLoss: 0.003926\n","2025it [02:36,  8.21it/s]Train epoch: 5 [batch #2025, batch_size 16, seq length 1688]\tLoss: 0.004005\n","2050it [02:39,  8.26it/s]Train epoch: 5 [batch #2050, batch_size 16, seq length 1707]\tLoss: 0.004128\n","2075it [02:42,  8.27it/s]Train epoch: 5 [batch #2075, batch_size 16, seq length 1728]\tLoss: 0.004296\n","2100it [02:45,  8.11it/s]Train epoch: 5 [batch #2100, batch_size 16, seq length 1748]\tLoss: 0.004016\n","2125it [02:48,  8.21it/s]Train epoch: 5 [batch #2125, batch_size 16, seq length 1768]\tLoss: 0.004479\n","2150it [02:52,  7.99it/s]Train epoch: 5 [batch #2150, batch_size 16, seq length 1787]\tLoss: 0.003971\n","2175it [02:55,  7.83it/s]Train epoch: 5 [batch #2175, batch_size 16, seq length 1809]\tLoss: 0.004559\n","2200it [02:58,  7.58it/s]Train epoch: 5 [batch #2200, batch_size 16, seq length 1833]\tLoss: 0.004235\n","2225it [03:01,  7.57it/s]Train epoch: 5 [batch #2225, batch_size 16, seq length 1855]\tLoss: 0.004366\n","2250it [03:05,  7.52it/s]Train epoch: 5 [batch #2250, batch_size 16, seq length 1878]\tLoss: 0.004749\n","2275it [03:08,  7.35it/s]Train epoch: 5 [batch #2275, batch_size 16, seq length 1901]\tLoss: 0.004346\n","2300it [03:11,  7.40it/s]Train epoch: 5 [batch #2300, batch_size 16, seq length 1927]\tLoss: 0.004456\n","2325it [03:15,  7.23it/s]Train epoch: 5 [batch #2325, batch_size 16, seq length 1953]\tLoss: 0.004693\n","2350it [03:18,  7.25it/s]Train epoch: 5 [batch #2350, batch_size 16, seq length 1980]\tLoss: 0.004472\n","2375it [03:22,  6.99it/s]Train epoch: 5 [batch #2375, batch_size 16, seq length 2006]\tLoss: 0.004728\n","2400it [03:25,  6.95it/s]Train epoch: 5 [batch #2400, batch_size 16, seq length 2034]\tLoss: 0.004639\n","2425it [03:29,  6.87it/s]Train epoch: 5 [batch #2425, batch_size 16, seq length 2064]\tLoss: 0.004547\n","2450it [03:33,  6.75it/s]Train epoch: 5 [batch #2450, batch_size 16, seq length 2094]\tLoss: 0.004852\n","2475it [03:36,  6.66it/s]Train epoch: 5 [batch #2475, batch_size 16, seq length 2126]\tLoss: 0.004892\n","2500it [03:40,  6.60it/s]Train epoch: 5 [batch #2500, batch_size 16, seq length 2160]\tLoss: 0.004842\n","2525it [03:44,  6.53it/s]Train epoch: 5 [batch #2525, batch_size 16, seq length 2192]\tLoss: 0.005093\n","2550it [03:48,  6.47it/s]Train epoch: 5 [batch #2550, batch_size 16, seq length 2228]\tLoss: 0.004807\n","2575it [03:52,  6.33it/s]Train epoch: 5 [batch #2575, batch_size 16, seq length 2266]\tLoss: 0.005200\n","2600it [03:56,  6.22it/s]Train epoch: 5 [batch #2600, batch_size 16, seq length 2307]\tLoss: 0.005137\n","2625it [04:00,  6.12it/s]Train epoch: 5 [batch #2625, batch_size 16, seq length 2350]\tLoss: 0.005084\n","2650it [04:04,  5.89it/s]Train epoch: 5 [batch #2650, batch_size 16, seq length 2392]\tLoss: 0.005028\n","2675it [04:08,  5.85it/s]Train epoch: 5 [batch #2675, batch_size 16, seq length 2439]\tLoss: 0.005343\n","2700it [04:13,  5.76it/s]Train epoch: 5 [batch #2700, batch_size 16, seq length 2491]\tLoss: 0.005300\n","2725it [04:17,  5.70it/s]Train epoch: 5 [batch #2725, batch_size 16, seq length 2500]\tLoss: 0.005218\n","2750it [04:21,  5.67it/s]Train epoch: 5 [batch #2750, batch_size 16, seq length 2500]\tLoss: 0.005106\n","2775it [04:26,  5.61it/s]Train epoch: 5 [batch #2775, batch_size 16, seq length 2500]\tLoss: 0.005450\n","2800it [04:30,  5.56it/s]Train epoch: 5 [batch #2800, batch_size 16, seq length 2500]\tLoss: 0.005425\n","2825it [04:34,  5.70it/s]Train epoch: 5 [batch #2825, batch_size 16, seq length 2500]\tLoss: 0.005430\n","2850it [04:39,  5.69it/s]Train epoch: 5 [batch #2850, batch_size 16, seq length 2500]\tLoss: 0.005690\n","2875it [04:43,  5.63it/s]Train epoch: 5 [batch #2875, batch_size 16, seq length 2500]\tLoss: 0.006137\n","2900it [04:48,  5.62it/s]Train epoch: 5 [batch #2900, batch_size 16, seq length 2500]\tLoss: 0.006252\n","2925it [04:52,  5.65it/s]Train epoch: 5 [batch #2925, batch_size 16, seq length 2500]\tLoss: 0.006540\n","2950it [04:57,  5.49it/s]Train epoch: 5 [batch #2950, batch_size 16, seq length 2500]\tLoss: 0.006368\n","2975it [05:01,  5.38it/s]Train epoch: 5 [batch #2975, batch_size 16, seq length 2500]\tLoss: 0.007644\n","2983it [05:03,  9.83it/s]\n","epoch loss: 0.003885863885413662\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:44, 36.82it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0292, 0.0495, 0.0400, 0.0442, 0.8474\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2938, 0.5485, 0.3875, 0.4541, 0.9754\n","rec_at_8: 0.3363\n","prec_at_8: 0.6227\n","rec_at_15: 0.4543\n","prec_at_15: 0.4697\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/rnn_May_04_023326\n","\n","EPOCH 6\n","0it [00:00, ?it/s]Train epoch: 6 [batch #0, batch_size 16, seq length 117]\tLoss: 0.005005\n","24it [00:00, 37.50it/s]Train epoch: 6 [batch #25, batch_size 16, seq length 337]\tLoss: 0.002885\n","48it [00:01, 33.51it/s]Train epoch: 6 [batch #50, batch_size 16, seq length 402]\tLoss: 0.002556\n","72it [00:02, 30.37it/s]Train epoch: 6 [batch #75, batch_size 16, seq length 452]\tLoss: 0.002248\n","98it [00:03, 27.34it/s]Train epoch: 6 [batch #100, batch_size 16, seq length 490]\tLoss: 0.002152\n","125it [00:04, 25.77it/s]Train epoch: 6 [batch #125, batch_size 16, seq length 520]\tLoss: 0.002384\n","149it [00:05, 24.78it/s]Train epoch: 6 [batch #150, batch_size 16, seq length 548]\tLoss: 0.002479\n","173it [00:06, 22.06it/s]Train epoch: 6 [batch #175, batch_size 16, seq length 574]\tLoss: 0.002409\n","200it [00:07, 22.55it/s]Train epoch: 6 [batch #200, batch_size 16, seq length 596]\tLoss: 0.002478\n","224it [00:08, 22.03it/s]Train epoch: 6 [batch #225, batch_size 16, seq length 618]\tLoss: 0.002490\n","248it [00:09, 21.48it/s]Train epoch: 6 [batch #250, batch_size 16, seq length 638]\tLoss: 0.002395\n","275it [00:10, 20.94it/s]Train epoch: 6 [batch #275, batch_size 16, seq length 656]\tLoss: 0.002633\n","299it [00:12, 20.14it/s]Train epoch: 6 [batch #300, batch_size 16, seq length 674]\tLoss: 0.002533\n","324it [00:13, 19.71it/s]Train epoch: 6 [batch #325, batch_size 16, seq length 693]\tLoss: 0.002419\n","349it [00:14, 19.46it/s]Train epoch: 6 [batch #350, batch_size 16, seq length 710]\tLoss: 0.002596\n","375it [00:15, 19.02it/s]Train epoch: 6 [batch #375, batch_size 16, seq length 726]\tLoss: 0.002888\n","399it [00:17, 18.37it/s]Train epoch: 6 [batch #400, batch_size 16, seq length 743]\tLoss: 0.002550\n","425it [00:18, 17.90it/s]Train epoch: 6 [batch #425, batch_size 16, seq length 758]\tLoss: 0.002556\n","449it [00:20, 17.87it/s]Train epoch: 6 [batch #450, batch_size 16, seq length 773]\tLoss: 0.002766\n","475it [00:21, 17.18it/s]Train epoch: 6 [batch #475, batch_size 16, seq length 789]\tLoss: 0.002510\n","499it [00:22, 16.77it/s]Train epoch: 6 [batch #500, batch_size 16, seq length 803]\tLoss: 0.002774\n","525it [00:24, 16.50it/s]Train epoch: 6 [batch #525, batch_size 16, seq length 818]\tLoss: 0.002704\n","549it [00:25, 16.56it/s]Train epoch: 6 [batch #550, batch_size 16, seq length 833]\tLoss: 0.002524\n","575it [00:27, 16.48it/s]Train epoch: 6 [batch #575, batch_size 16, seq length 846]\tLoss: 0.002541\n","599it [00:29, 15.97it/s]Train epoch: 6 [batch #600, batch_size 16, seq length 860]\tLoss: 0.002856\n","625it [00:30, 15.62it/s]Train epoch: 6 [batch #625, batch_size 16, seq length 874]\tLoss: 0.002717\n","649it [00:32, 15.75it/s]Train epoch: 6 [batch #650, batch_size 16, seq length 887]\tLoss: 0.002596\n","675it [00:33, 15.40it/s]Train epoch: 6 [batch #675, batch_size 16, seq length 901]\tLoss: 0.002664\n","699it [00:35, 15.18it/s]Train epoch: 6 [batch #700, batch_size 16, seq length 915]\tLoss: 0.002562\n","725it [00:37, 15.17it/s]Train epoch: 6 [batch #725, batch_size 16, seq length 929]\tLoss: 0.002716\n","749it [00:38, 14.80it/s]Train epoch: 6 [batch #750, batch_size 16, seq length 942]\tLoss: 0.002878\n","775it [00:40, 14.46it/s]Train epoch: 6 [batch #775, batch_size 16, seq length 954]\tLoss: 0.002815\n","799it [00:42, 14.30it/s]Train epoch: 6 [batch #800, batch_size 16, seq length 968]\tLoss: 0.002890\n","825it [00:44, 14.14it/s]Train epoch: 6 [batch #825, batch_size 16, seq length 981]\tLoss: 0.002861\n","849it [00:45, 14.07it/s]Train epoch: 6 [batch #850, batch_size 16, seq length 994]\tLoss: 0.002935\n","875it [00:47, 13.71it/s]Train epoch: 6 [batch #875, batch_size 16, seq length 1006]\tLoss: 0.002720\n","899it [00:49, 13.66it/s]Train epoch: 6 [batch #900, batch_size 16, seq length 1020]\tLoss: 0.003034\n","925it [00:51, 13.24it/s]Train epoch: 6 [batch #925, batch_size 16, seq length 1033]\tLoss: 0.002799\n","949it [00:53, 13.38it/s]Train epoch: 6 [batch #950, batch_size 16, seq length 1046]\tLoss: 0.003009\n","975it [00:55, 12.98it/s]Train epoch: 6 [batch #975, batch_size 16, seq length 1059]\tLoss: 0.002949\n","999it [00:57, 12.82it/s]Train epoch: 6 [batch #1000, batch_size 16, seq length 1073]\tLoss: 0.002996\n","1025it [00:59, 12.83it/s]Train epoch: 6 [batch #1025, batch_size 16, seq length 1085]\tLoss: 0.003202\n","1049it [01:01, 12.41it/s]Train epoch: 6 [batch #1050, batch_size 16, seq length 1099]\tLoss: 0.003137\n","1075it [01:03, 12.35it/s]Train epoch: 6 [batch #1075, batch_size 16, seq length 1113]\tLoss: 0.003144\n","1099it [01:05, 12.33it/s]Train epoch: 6 [batch #1100, batch_size 16, seq length 1126]\tLoss: 0.002932\n","1125it [01:07, 12.29it/s]Train epoch: 6 [batch #1125, batch_size 16, seq length 1140]\tLoss: 0.003232\n","1149it [01:09, 12.33it/s]Train epoch: 6 [batch #1150, batch_size 16, seq length 1154]\tLoss: 0.003100\n","1175it [01:11, 12.13it/s]Train epoch: 6 [batch #1175, batch_size 16, seq length 1167]\tLoss: 0.003182\n","1199it [01:13, 12.06it/s]Train epoch: 6 [batch #1200, batch_size 16, seq length 1180]\tLoss: 0.003158\n","1225it [01:15, 11.70it/s]Train epoch: 6 [batch #1225, batch_size 16, seq length 1195]\tLoss: 0.003201\n","1249it [01:17, 11.71it/s]Train epoch: 6 [batch #1250, batch_size 16, seq length 1208]\tLoss: 0.003107\n","1275it [01:19, 11.77it/s]Train epoch: 6 [batch #1275, batch_size 16, seq length 1220]\tLoss: 0.003179\n","1299it [01:21, 11.52it/s]Train epoch: 6 [batch #1300, batch_size 16, seq length 1234]\tLoss: 0.003360\n","1325it [01:24, 11.28it/s]Train epoch: 6 [batch #1325, batch_size 16, seq length 1248]\tLoss: 0.003129\n","1349it [01:26, 11.10it/s]Train epoch: 6 [batch #1350, batch_size 16, seq length 1262]\tLoss: 0.003416\n","1375it [01:28, 11.01it/s]Train epoch: 6 [batch #1375, batch_size 16, seq length 1276]\tLoss: 0.003523\n","1399it [01:30, 10.93it/s]Train epoch: 6 [batch #1400, batch_size 16, seq length 1290]\tLoss: 0.003523\n","1425it [01:33, 10.89it/s]Train epoch: 6 [batch #1425, batch_size 16, seq length 1304]\tLoss: 0.003624\n","1449it [01:35, 10.75it/s]Train epoch: 6 [batch #1450, batch_size 16, seq length 1319]\tLoss: 0.003428\n","1475it [01:37, 10.55it/s]Train epoch: 6 [batch #1475, batch_size 16, seq length 1332]\tLoss: 0.003539\n","1499it [01:40, 10.59it/s]Train epoch: 6 [batch #1500, batch_size 16, seq length 1346]\tLoss: 0.003477\n","1525it [01:42, 10.25it/s]Train epoch: 6 [batch #1525, batch_size 16, seq length 1362]\tLoss: 0.003092\n","1549it [01:45, 10.29it/s]Train epoch: 6 [batch #1550, batch_size 16, seq length 1377]\tLoss: 0.003658\n","1575it [01:47,  9.89it/s]Train epoch: 6 [batch #1575, batch_size 16, seq length 1392]\tLoss: 0.003203\n","1600it [01:50,  9.81it/s]Train epoch: 6 [batch #1600, batch_size 16, seq length 1406]\tLoss: 0.003350\n","1625it [01:52,  9.88it/s]Train epoch: 6 [batch #1625, batch_size 16, seq length 1422]\tLoss: 0.003664\n","1650it [01:55,  9.91it/s]Train epoch: 6 [batch #1650, batch_size 16, seq length 1437]\tLoss: 0.003491\n","1675it [01:57,  9.67it/s]Train epoch: 6 [batch #1675, batch_size 16, seq length 1454]\tLoss: 0.003538\n","1700it [02:00,  9.59it/s]Train epoch: 6 [batch #1700, batch_size 16, seq length 1469]\tLoss: 0.003843\n","1725it [02:02,  9.48it/s]Train epoch: 6 [batch #1725, batch_size 16, seq length 1484]\tLoss: 0.003471\n","1750it [02:05,  9.43it/s]Train epoch: 6 [batch #1750, batch_size 16, seq length 1501]\tLoss: 0.003376\n","1775it [02:08,  9.32it/s]Train epoch: 6 [batch #1775, batch_size 16, seq length 1518]\tLoss: 0.003725\n","1800it [02:10,  9.29it/s]Train epoch: 6 [batch #1800, batch_size 16, seq length 1533]\tLoss: 0.003878\n","1825it [02:13,  8.95it/s]Train epoch: 6 [batch #1825, batch_size 16, seq length 1548]\tLoss: 0.003847\n","1850it [02:16,  9.00it/s]Train epoch: 6 [batch #1850, batch_size 16, seq length 1566]\tLoss: 0.003528\n","1875it [02:19,  8.87it/s]Train epoch: 6 [batch #1875, batch_size 16, seq length 1582]\tLoss: 0.003792\n","1900it [02:22,  8.73it/s]Train epoch: 6 [batch #1900, batch_size 16, seq length 1599]\tLoss: 0.003901\n","1925it [02:24,  8.75it/s]Train epoch: 6 [batch #1925, batch_size 16, seq length 1616]\tLoss: 0.003799\n","1950it [02:27,  8.67it/s]Train epoch: 6 [batch #1950, batch_size 16, seq length 1633]\tLoss: 0.003793\n","1975it [02:30,  8.53it/s]Train epoch: 6 [batch #1975, batch_size 16, seq length 1651]\tLoss: 0.003729\n","2000it [02:33,  8.38it/s]Train epoch: 6 [batch #2000, batch_size 16, seq length 1670]\tLoss: 0.003631\n","2025it [02:36,  8.30it/s]Train epoch: 6 [batch #2025, batch_size 16, seq length 1688]\tLoss: 0.003712\n","2050it [02:39,  8.18it/s]Train epoch: 6 [batch #2050, batch_size 16, seq length 1707]\tLoss: 0.003879\n","2075it [02:42,  8.12it/s]Train epoch: 6 [batch #2075, batch_size 16, seq length 1728]\tLoss: 0.004027\n","2100it [02:45,  8.08it/s]Train epoch: 6 [batch #2100, batch_size 16, seq length 1748]\tLoss: 0.003769\n","2125it [02:49,  8.08it/s]Train epoch: 6 [batch #2125, batch_size 16, seq length 1768]\tLoss: 0.004192\n","2150it [02:52,  7.95it/s]Train epoch: 6 [batch #2150, batch_size 16, seq length 1787]\tLoss: 0.003704\n","2175it [02:55,  7.83it/s]Train epoch: 6 [batch #2175, batch_size 16, seq length 1809]\tLoss: 0.004219\n","2200it [02:58,  7.87it/s]Train epoch: 6 [batch #2200, batch_size 16, seq length 1833]\tLoss: 0.003918\n","2225it [03:01,  7.65it/s]Train epoch: 6 [batch #2225, batch_size 16, seq length 1855]\tLoss: 0.004024\n","2250it [03:05,  7.64it/s]Train epoch: 6 [batch #2250, batch_size 16, seq length 1878]\tLoss: 0.004452\n","2275it [03:08,  7.47it/s]Train epoch: 6 [batch #2275, batch_size 16, seq length 1901]\tLoss: 0.004077\n","2300it [03:11,  7.39it/s]Train epoch: 6 [batch #2300, batch_size 16, seq length 1927]\tLoss: 0.004159\n","2325it [03:15,  7.20it/s]Train epoch: 6 [batch #2325, batch_size 16, seq length 1953]\tLoss: 0.004394\n","2350it [03:18,  7.16it/s]Train epoch: 6 [batch #2350, batch_size 16, seq length 1980]\tLoss: 0.004127\n","2375it [03:22,  7.08it/s]Train epoch: 6 [batch #2375, batch_size 16, seq length 2006]\tLoss: 0.004431\n","2400it [03:25,  6.82it/s]Train epoch: 6 [batch #2400, batch_size 16, seq length 2034]\tLoss: 0.004368\n","2425it [03:29,  6.96it/s]Train epoch: 6 [batch #2425, batch_size 16, seq length 2064]\tLoss: 0.004222\n","2450it [03:33,  6.70it/s]Train epoch: 6 [batch #2450, batch_size 16, seq length 2094]\tLoss: 0.004524\n","2475it [03:36,  6.67it/s]Train epoch: 6 [batch #2475, batch_size 16, seq length 2126]\tLoss: 0.004575\n","2500it [03:40,  6.59it/s]Train epoch: 6 [batch #2500, batch_size 16, seq length 2160]\tLoss: 0.004523\n","2525it [03:44,  6.50it/s]Train epoch: 6 [batch #2525, batch_size 16, seq length 2192]\tLoss: 0.004745\n","2550it [03:48,  6.32it/s]Train epoch: 6 [batch #2550, batch_size 16, seq length 2228]\tLoss: 0.004556\n","2575it [03:52,  6.26it/s]Train epoch: 6 [batch #2575, batch_size 16, seq length 2266]\tLoss: 0.004848\n","2600it [03:56,  6.20it/s]Train epoch: 6 [batch #2600, batch_size 16, seq length 2307]\tLoss: 0.004759\n","2625it [04:00,  6.08it/s]Train epoch: 6 [batch #2625, batch_size 16, seq length 2350]\tLoss: 0.004702\n","2650it [04:04,  5.90it/s]Train epoch: 6 [batch #2650, batch_size 16, seq length 2392]\tLoss: 0.004702\n","2675it [04:08,  5.85it/s]Train epoch: 6 [batch #2675, batch_size 16, seq length 2439]\tLoss: 0.005039\n","2700it [04:13,  5.76it/s]Train epoch: 6 [batch #2700, batch_size 16, seq length 2491]\tLoss: 0.004980\n","2725it [04:17,  5.69it/s]Train epoch: 6 [batch #2725, batch_size 16, seq length 2500]\tLoss: 0.004908\n","2750it [04:22,  5.63it/s]Train epoch: 6 [batch #2750, batch_size 16, seq length 2500]\tLoss: 0.004760\n","2775it [04:26,  5.63it/s]Train epoch: 6 [batch #2775, batch_size 16, seq length 2500]\tLoss: 0.005093\n","2800it [04:30,  5.71it/s]Train epoch: 6 [batch #2800, batch_size 16, seq length 2500]\tLoss: 0.005119\n","2825it [04:35,  5.68it/s]Train epoch: 6 [batch #2825, batch_size 16, seq length 2500]\tLoss: 0.005015\n","2850it [04:39,  5.65it/s]Train epoch: 6 [batch #2850, batch_size 16, seq length 2500]\tLoss: 0.005377\n","2875it [04:44,  5.64it/s]Train epoch: 6 [batch #2875, batch_size 16, seq length 2500]\tLoss: 0.005701\n","2900it [04:48,  5.55it/s]Train epoch: 6 [batch #2900, batch_size 16, seq length 2500]\tLoss: 0.005833\n","2925it [04:53,  5.56it/s]Train epoch: 6 [batch #2925, batch_size 16, seq length 2500]\tLoss: 0.006080\n","2950it [04:57,  5.47it/s]Train epoch: 6 [batch #2950, batch_size 16, seq length 2500]\tLoss: 0.005977\n","2975it [05:02,  5.40it/s]Train epoch: 6 [batch #2975, batch_size 16, seq length 2500]\tLoss: 0.007105\n","2983it [05:03,  9.82it/s]\n","epoch loss: 0.003604846442687446\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:44, 36.60it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0309, 0.0500, 0.0458, 0.0478, 0.8422\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2884, 0.4896, 0.4124, 0.4477, 0.9744\n","rec_at_8: 0.3289\n","prec_at_8: 0.6075\n","rec_at_15: 0.4469\n","prec_at_15: 0.4603\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/rnn_May_04_023326\n","\n","EPOCH 7\n","0it [00:00, ?it/s]Train epoch: 7 [batch #0, batch_size 16, seq length 117]\tLoss: 0.004865\n","23it [00:00, 37.44it/s]Train epoch: 7 [batch #25, batch_size 16, seq length 337]\tLoss: 0.002783\n","47it [00:01, 33.21it/s]Train epoch: 7 [batch #50, batch_size 16, seq length 402]\tLoss: 0.002356\n","75it [00:02, 29.97it/s]Train epoch: 7 [batch #75, batch_size 16, seq length 452]\tLoss: 0.002087\n","100it [00:03, 27.25it/s]Train epoch: 7 [batch #100, batch_size 16, seq length 490]\tLoss: 0.002013\n","124it [00:04, 25.62it/s]Train epoch: 7 [batch #125, batch_size 16, seq length 520]\tLoss: 0.002224\n","148it [00:05, 24.80it/s]Train epoch: 7 [batch #150, batch_size 16, seq length 548]\tLoss: 0.002276\n","175it [00:06, 23.32it/s]Train epoch: 7 [batch #175, batch_size 16, seq length 574]\tLoss: 0.002245\n","199it [00:07, 22.42it/s]Train epoch: 7 [batch #200, batch_size 16, seq length 596]\tLoss: 0.002328\n","223it [00:08, 22.26it/s]Train epoch: 7 [batch #225, batch_size 16, seq length 618]\tLoss: 0.002349\n","250it [00:09, 21.24it/s]Train epoch: 7 [batch #250, batch_size 16, seq length 638]\tLoss: 0.002233\n","274it [00:10, 20.88it/s]Train epoch: 7 [batch #275, batch_size 16, seq length 656]\tLoss: 0.002473\n","298it [00:11, 20.27it/s]Train epoch: 7 [batch #300, batch_size 16, seq length 674]\tLoss: 0.002373\n","325it [00:13, 19.67it/s]Train epoch: 7 [batch #325, batch_size 16, seq length 693]\tLoss: 0.002255\n","350it [00:14, 19.13it/s]Train epoch: 7 [batch #350, batch_size 16, seq length 710]\tLoss: 0.002474\n","374it [00:15, 18.81it/s]Train epoch: 7 [batch #375, batch_size 16, seq length 726]\tLoss: 0.002673\n","400it [00:17, 18.10it/s]Train epoch: 7 [batch #400, batch_size 16, seq length 743]\tLoss: 0.002332\n","424it [00:18, 17.75it/s]Train epoch: 7 [batch #425, batch_size 16, seq length 758]\tLoss: 0.002377\n","450it [00:20, 17.73it/s]Train epoch: 7 [batch #450, batch_size 16, seq length 773]\tLoss: 0.002619\n","474it [00:21, 16.89it/s]Train epoch: 7 [batch #475, batch_size 16, seq length 789]\tLoss: 0.002349\n","500it [00:23, 16.42it/s]Train epoch: 7 [batch #500, batch_size 16, seq length 803]\tLoss: 0.002593\n","524it [00:24, 16.38it/s]Train epoch: 7 [batch #525, batch_size 16, seq length 818]\tLoss: 0.002509\n","550it [00:26, 16.47it/s]Train epoch: 7 [batch #550, batch_size 16, seq length 833]\tLoss: 0.002385\n","574it [00:27, 16.46it/s]Train epoch: 7 [batch #575, batch_size 16, seq length 846]\tLoss: 0.002395\n","600it [00:29, 15.88it/s]Train epoch: 7 [batch #600, batch_size 16, seq length 860]\tLoss: 0.002650\n","624it [00:30, 15.49it/s]Train epoch: 7 [batch #625, batch_size 16, seq length 874]\tLoss: 0.002529\n","650it [00:32, 15.47it/s]Train epoch: 7 [batch #650, batch_size 16, seq length 887]\tLoss: 0.002349\n","674it [00:33, 15.36it/s]Train epoch: 7 [batch #675, batch_size 16, seq length 901]\tLoss: 0.002458\n","700it [00:35, 14.97it/s]Train epoch: 7 [batch #700, batch_size 16, seq length 915]\tLoss: 0.002363\n","724it [00:37, 14.76it/s]Train epoch: 7 [batch #725, batch_size 16, seq length 929]\tLoss: 0.002513\n","750it [00:39, 14.77it/s]Train epoch: 7 [batch #750, batch_size 16, seq length 942]\tLoss: 0.002677\n","774it [00:40, 14.42it/s]Train epoch: 7 [batch #775, batch_size 16, seq length 954]\tLoss: 0.002652\n","800it [00:42, 14.17it/s]Train epoch: 7 [batch #800, batch_size 16, seq length 968]\tLoss: 0.002673\n","824it [00:44, 14.24it/s]Train epoch: 7 [batch #825, batch_size 16, seq length 981]\tLoss: 0.002656\n","850it [00:46, 13.90it/s]Train epoch: 7 [batch #850, batch_size 16, seq length 994]\tLoss: 0.002719\n","874it [00:47, 13.90it/s]Train epoch: 7 [batch #875, batch_size 16, seq length 1006]\tLoss: 0.002525\n","900it [00:49, 13.58it/s]Train epoch: 7 [batch #900, batch_size 16, seq length 1020]\tLoss: 0.002858\n","924it [00:51, 13.48it/s]Train epoch: 7 [batch #925, batch_size 16, seq length 1033]\tLoss: 0.002633\n","950it [00:53, 13.29it/s]Train epoch: 7 [batch #950, batch_size 16, seq length 1046]\tLoss: 0.002770\n","974it [00:55, 13.42it/s]Train epoch: 7 [batch #975, batch_size 16, seq length 1059]\tLoss: 0.002742\n","1000it [00:57, 13.09it/s]Train epoch: 7 [batch #1000, batch_size 16, seq length 1073]\tLoss: 0.002723\n","1024it [00:59, 12.74it/s]Train epoch: 7 [batch #1025, batch_size 16, seq length 1085]\tLoss: 0.002969\n","1050it [01:01, 12.49it/s]Train epoch: 7 [batch #1050, batch_size 16, seq length 1099]\tLoss: 0.002857\n","1074it [01:03, 12.41it/s]Train epoch: 7 [batch #1075, batch_size 16, seq length 1113]\tLoss: 0.002896\n","1100it [01:05, 12.35it/s]Train epoch: 7 [batch #1100, batch_size 16, seq length 1126]\tLoss: 0.002761\n","1124it [01:07, 12.29it/s]Train epoch: 7 [batch #1125, batch_size 16, seq length 1140]\tLoss: 0.002992\n","1150it [01:09, 12.27it/s]Train epoch: 7 [batch #1150, batch_size 16, seq length 1154]\tLoss: 0.002875\n","1174it [01:11, 12.37it/s]Train epoch: 7 [batch #1175, batch_size 16, seq length 1167]\tLoss: 0.002931\n","1200it [01:13, 11.97it/s]Train epoch: 7 [batch #1200, batch_size 16, seq length 1180]\tLoss: 0.002873\n","1224it [01:15, 11.94it/s]Train epoch: 7 [batch #1225, batch_size 16, seq length 1195]\tLoss: 0.002931\n","1250it [01:17, 11.82it/s]Train epoch: 7 [batch #1250, batch_size 16, seq length 1208]\tLoss: 0.002866\n","1274it [01:19, 11.75it/s]Train epoch: 7 [batch #1275, batch_size 16, seq length 1220]\tLoss: 0.002979\n","1300it [01:21, 11.29it/s]Train epoch: 7 [batch #1300, batch_size 16, seq length 1234]\tLoss: 0.003132\n","1324it [01:24, 10.66it/s]Train epoch: 7 [batch #1325, batch_size 16, seq length 1248]\tLoss: 0.002968\n","1350it [01:26, 11.18it/s]Train epoch: 7 [batch #1350, batch_size 16, seq length 1262]\tLoss: 0.003178\n","1374it [01:28, 11.02it/s]Train epoch: 7 [batch #1375, batch_size 16, seq length 1276]\tLoss: 0.003286\n","1400it [01:31, 10.77it/s]Train epoch: 7 [batch #1400, batch_size 16, seq length 1290]\tLoss: 0.003320\n","1424it [01:33, 10.72it/s]Train epoch: 7 [batch #1425, batch_size 16, seq length 1304]\tLoss: 0.003375\n","1450it [01:35, 10.49it/s]Train epoch: 7 [batch #1450, batch_size 16, seq length 1319]\tLoss: 0.003177\n","1474it [01:38, 10.47it/s]Train epoch: 7 [batch #1475, batch_size 16, seq length 1332]\tLoss: 0.003328\n","1500it [01:40, 10.23it/s]Train epoch: 7 [batch #1500, batch_size 16, seq length 1346]\tLoss: 0.003160\n","1524it [01:42, 10.05it/s]Train epoch: 7 [batch #1525, batch_size 16, seq length 1362]\tLoss: 0.002865\n","1550it [01:45,  9.96it/s]Train epoch: 7 [batch #1550, batch_size 16, seq length 1377]\tLoss: 0.003365\n","1575it [01:48,  9.91it/s]Train epoch: 7 [batch #1575, batch_size 16, seq length 1392]\tLoss: 0.002993\n","1599it [01:50,  9.78it/s]Train epoch: 7 [batch #1600, batch_size 16, seq length 1406]\tLoss: 0.003162\n","1625it [01:53,  9.76it/s]Train epoch: 7 [batch #1625, batch_size 16, seq length 1422]\tLoss: 0.003445\n","1650it [01:55,  9.66it/s]Train epoch: 7 [batch #1650, batch_size 16, seq length 1437]\tLoss: 0.003324\n","1675it [01:58,  9.52it/s]Train epoch: 7 [batch #1675, batch_size 16, seq length 1454]\tLoss: 0.003343\n","1699it [02:00,  9.77it/s]Train epoch: 7 [batch #1700, batch_size 16, seq length 1469]\tLoss: 0.003588\n","1725it [02:03,  9.67it/s]Train epoch: 7 [batch #1725, batch_size 16, seq length 1484]\tLoss: 0.003261\n","1750it [02:06,  9.18it/s]Train epoch: 7 [batch #1750, batch_size 16, seq length 1501]\tLoss: 0.003095\n","1775it [02:09,  9.09it/s]Train epoch: 7 [batch #1775, batch_size 16, seq length 1518]\tLoss: 0.003506\n","1800it [02:11,  9.13it/s]Train epoch: 7 [batch #1800, batch_size 16, seq length 1533]\tLoss: 0.003619\n","1825it [02:14,  9.12it/s]Train epoch: 7 [batch #1825, batch_size 16, seq length 1548]\tLoss: 0.003605\n","1850it [02:17,  8.89it/s]Train epoch: 7 [batch #1850, batch_size 16, seq length 1566]\tLoss: 0.003251\n","1875it [02:20,  8.84it/s]Train epoch: 7 [batch #1875, batch_size 16, seq length 1582]\tLoss: 0.003582\n","1900it [02:23,  8.70it/s]Train epoch: 7 [batch #1900, batch_size 16, seq length 1599]\tLoss: 0.003619\n","1925it [02:25,  8.56it/s]Train epoch: 7 [batch #1925, batch_size 16, seq length 1616]\tLoss: 0.003543\n","1950it [02:28,  8.53it/s]Train epoch: 7 [batch #1950, batch_size 16, seq length 1633]\tLoss: 0.003484\n","1975it [02:31,  8.31it/s]Train epoch: 7 [batch #1975, batch_size 16, seq length 1651]\tLoss: 0.003477\n","2000it [02:34,  8.45it/s]Train epoch: 7 [batch #2000, batch_size 16, seq length 1670]\tLoss: 0.003481\n","2025it [02:37,  8.15it/s]Train epoch: 7 [batch #2025, batch_size 16, seq length 1688]\tLoss: 0.003506\n","2050it [02:41,  8.30it/s]Train epoch: 7 [batch #2050, batch_size 16, seq length 1707]\tLoss: 0.003637\n","2075it [02:44,  8.17it/s]Train epoch: 7 [batch #2075, batch_size 16, seq length 1728]\tLoss: 0.003738\n","2100it [02:47,  8.09it/s]Train epoch: 7 [batch #2100, batch_size 16, seq length 1748]\tLoss: 0.003490\n","2125it [02:50,  7.98it/s]Train epoch: 7 [batch #2125, batch_size 16, seq length 1768]\tLoss: 0.003956\n","2150it [02:53,  8.02it/s]Train epoch: 7 [batch #2150, batch_size 16, seq length 1787]\tLoss: 0.003432\n","2175it [02:56,  7.71it/s]Train epoch: 7 [batch #2175, batch_size 16, seq length 1809]\tLoss: 0.003968\n","2200it [02:59,  7.70it/s]Train epoch: 7 [batch #2200, batch_size 16, seq length 1833]\tLoss: 0.003658\n","2225it [03:03,  7.56it/s]Train epoch: 7 [batch #2225, batch_size 16, seq length 1855]\tLoss: 0.003769\n","2250it [03:06,  7.40it/s]Train epoch: 7 [batch #2250, batch_size 16, seq length 1878]\tLoss: 0.004180\n","2275it [03:09,  7.43it/s]Train epoch: 7 [batch #2275, batch_size 16, seq length 1901]\tLoss: 0.003779\n","2300it [03:13,  7.30it/s]Train epoch: 7 [batch #2300, batch_size 16, seq length 1927]\tLoss: 0.003911\n","2325it [03:16,  7.25it/s]Train epoch: 7 [batch #2325, batch_size 16, seq length 1953]\tLoss: 0.004176\n","2350it [03:20,  7.22it/s]Train epoch: 7 [batch #2350, batch_size 16, seq length 1980]\tLoss: 0.003891\n","2375it [03:23,  6.87it/s]Train epoch: 7 [batch #2375, batch_size 16, seq length 2006]\tLoss: 0.004078\n","2400it [03:27,  6.85it/s]Train epoch: 7 [batch #2400, batch_size 16, seq length 2034]\tLoss: 0.004079\n","2425it [03:31,  6.72it/s]Train epoch: 7 [batch #2425, batch_size 16, seq length 2064]\tLoss: 0.004006\n","2450it [03:34,  6.68it/s]Train epoch: 7 [batch #2450, batch_size 16, seq length 2094]\tLoss: 0.004226\n","2475it [03:38,  6.59it/s]Train epoch: 7 [batch #2475, batch_size 16, seq length 2126]\tLoss: 0.004322\n","2500it [03:42,  6.59it/s]Train epoch: 7 [batch #2500, batch_size 16, seq length 2160]\tLoss: 0.004262\n","2525it [03:46,  6.46it/s]Train epoch: 7 [batch #2525, batch_size 16, seq length 2192]\tLoss: 0.004484\n","2550it [03:50,  6.23it/s]Train epoch: 7 [batch #2550, batch_size 16, seq length 2228]\tLoss: 0.004342\n","2575it [03:54,  6.25it/s]Train epoch: 7 [batch #2575, batch_size 16, seq length 2266]\tLoss: 0.004528\n","2600it [03:58,  6.16it/s]Train epoch: 7 [batch #2600, batch_size 16, seq length 2307]\tLoss: 0.004502\n","2625it [04:02,  6.09it/s]Train epoch: 7 [batch #2625, batch_size 16, seq length 2350]\tLoss: 0.004453\n","2650it [04:06,  5.76it/s]Train epoch: 7 [batch #2650, batch_size 16, seq length 2392]\tLoss: 0.004441\n","2675it [04:11,  5.78it/s]Train epoch: 7 [batch #2675, batch_size 16, seq length 2439]\tLoss: 0.004801\n","2700it [04:15,  5.65it/s]Train epoch: 7 [batch #2700, batch_size 16, seq length 2491]\tLoss: 0.004718\n","2725it [04:19,  5.59it/s]Train epoch: 7 [batch #2725, batch_size 16, seq length 2500]\tLoss: 0.004682\n","2750it [04:24,  5.42it/s]Train epoch: 7 [batch #2750, batch_size 16, seq length 2500]\tLoss: 0.004539\n","2775it [04:29,  5.60it/s]Train epoch: 7 [batch #2775, batch_size 16, seq length 2500]\tLoss: 0.004784\n","2800it [04:33,  5.45it/s]Train epoch: 7 [batch #2800, batch_size 16, seq length 2500]\tLoss: 0.004812\n","2825it [04:38,  5.51it/s]Train epoch: 7 [batch #2825, batch_size 16, seq length 2500]\tLoss: 0.004770\n","2850it [04:42,  5.50it/s]Train epoch: 7 [batch #2850, batch_size 16, seq length 2500]\tLoss: 0.005033\n","2875it [04:47,  5.64it/s]Train epoch: 7 [batch #2875, batch_size 16, seq length 2500]\tLoss: 0.005356\n","2900it [04:51,  5.60it/s]Train epoch: 7 [batch #2900, batch_size 16, seq length 2500]\tLoss: 0.005518\n","2925it [04:56,  5.64it/s]Train epoch: 7 [batch #2925, batch_size 16, seq length 2500]\tLoss: 0.005748\n","2950it [05:00,  5.52it/s]Train epoch: 7 [batch #2950, batch_size 16, seq length 2500]\tLoss: 0.005693\n","2975it [05:05,  5.42it/s]Train epoch: 7 [batch #2975, batch_size 16, seq length 2500]\tLoss: 0.006659\n","2983it [05:06,  9.72it/s]\n","epoch loss: 0.0033742963522407276\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:44, 36.53it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0316, 0.0448, 0.0563, 0.0499, 0.8406\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2620, 0.3791, 0.4587, 0.4152, 0.9726\n","rec_at_8: 0.3149\n","prec_at_8: 0.5798\n","rec_at_15: 0.4316\n","prec_at_15: 0.4413\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/rnn_May_04_023326\n","\n","EPOCH 8\n","0it [00:00, ?it/s]Train epoch: 8 [batch #0, batch_size 16, seq length 117]\tLoss: 0.004676\n","24it [00:00, 37.38it/s]Train epoch: 8 [batch #25, batch_size 16, seq length 337]\tLoss: 0.002642\n","48it [00:01, 32.88it/s]Train epoch: 8 [batch #50, batch_size 16, seq length 402]\tLoss: 0.002237\n","72it [00:02, 30.27it/s]Train epoch: 8 [batch #75, batch_size 16, seq length 452]\tLoss: 0.001990\n","98it [00:03, 27.56it/s]Train epoch: 8 [batch #100, batch_size 16, seq length 490]\tLoss: 0.001901\n","125it [00:04, 25.88it/s]Train epoch: 8 [batch #125, batch_size 16, seq length 520]\tLoss: 0.002094\n","149it [00:05, 24.66it/s]Train epoch: 8 [batch #150, batch_size 16, seq length 548]\tLoss: 0.002176\n","173it [00:06, 23.54it/s]Train epoch: 8 [batch #175, batch_size 16, seq length 574]\tLoss: 0.002127\n","200it [00:07, 22.81it/s]Train epoch: 8 [batch #200, batch_size 16, seq length 596]\tLoss: 0.002201\n","224it [00:08, 22.18it/s]Train epoch: 8 [batch #225, batch_size 16, seq length 618]\tLoss: 0.002208\n","248it [00:09, 21.42it/s]Train epoch: 8 [batch #250, batch_size 16, seq length 638]\tLoss: 0.002116\n","275it [00:10, 20.66it/s]Train epoch: 8 [batch #275, batch_size 16, seq length 656]\tLoss: 0.002289\n","299it [00:12, 20.02it/s]Train epoch: 8 [batch #300, batch_size 16, seq length 674]\tLoss: 0.002217\n","325it [00:13, 19.08it/s]Train epoch: 8 [batch #325, batch_size 16, seq length 693]\tLoss: 0.002089\n","350it [00:14, 19.10it/s]Train epoch: 8 [batch #350, batch_size 16, seq length 710]\tLoss: 0.002355\n","374it [00:15, 18.71it/s]Train epoch: 8 [batch #375, batch_size 16, seq length 726]\tLoss: 0.002521\n","400it [00:17, 18.30it/s]Train epoch: 8 [batch #400, batch_size 16, seq length 743]\tLoss: 0.002185\n","424it [00:18, 18.00it/s]Train epoch: 8 [batch #425, batch_size 16, seq length 758]\tLoss: 0.002226\n","450it [00:20, 17.52it/s]Train epoch: 8 [batch #450, batch_size 16, seq length 773]\tLoss: 0.002471\n","474it [00:21, 16.90it/s]Train epoch: 8 [batch #475, batch_size 16, seq length 789]\tLoss: 0.002196\n","500it [00:23, 16.14it/s]Train epoch: 8 [batch #500, batch_size 16, seq length 803]\tLoss: 0.002456\n","524it [00:24, 16.41it/s]Train epoch: 8 [batch #525, batch_size 16, seq length 818]\tLoss: 0.002367\n","550it [00:26, 16.45it/s]Train epoch: 8 [batch #550, batch_size 16, seq length 833]\tLoss: 0.002209\n","574it [00:27, 16.50it/s]Train epoch: 8 [batch #575, batch_size 16, seq length 846]\tLoss: 0.002259\n","600it [00:29, 15.71it/s]Train epoch: 8 [batch #600, batch_size 16, seq length 860]\tLoss: 0.002525\n","624it [00:30, 15.76it/s]Train epoch: 8 [batch #625, batch_size 16, seq length 874]\tLoss: 0.002357\n","650it [00:32, 15.43it/s]Train epoch: 8 [batch #650, batch_size 16, seq length 887]\tLoss: 0.002245\n","674it [00:34, 15.33it/s]Train epoch: 8 [batch #675, batch_size 16, seq length 901]\tLoss: 0.002279\n","700it [00:35, 15.25it/s]Train epoch: 8 [batch #700, batch_size 16, seq length 915]\tLoss: 0.002216\n","724it [00:37, 14.75it/s]Train epoch: 8 [batch #725, batch_size 16, seq length 929]\tLoss: 0.002412\n","750it [00:39, 14.70it/s]Train epoch: 8 [batch #750, batch_size 16, seq length 942]\tLoss: 0.002508\n","774it [00:40, 14.22it/s]Train epoch: 8 [batch #775, batch_size 16, seq length 954]\tLoss: 0.002460\n","800it [00:42, 14.19it/s]Train epoch: 8 [batch #800, batch_size 16, seq length 968]\tLoss: 0.002552\n","824it [00:44, 14.06it/s]Train epoch: 8 [batch #825, batch_size 16, seq length 981]\tLoss: 0.002519\n","850it [00:46, 13.73it/s]Train epoch: 8 [batch #850, batch_size 16, seq length 994]\tLoss: 0.002586\n","874it [00:47, 13.71it/s]Train epoch: 8 [batch #875, batch_size 16, seq length 1006]\tLoss: 0.002353\n","900it [00:49, 13.53it/s]Train epoch: 8 [batch #900, batch_size 16, seq length 1020]\tLoss: 0.002667\n","924it [00:51, 13.35it/s]Train epoch: 8 [batch #925, batch_size 16, seq length 1033]\tLoss: 0.002424\n","950it [00:53, 13.31it/s]Train epoch: 8 [batch #950, batch_size 16, seq length 1046]\tLoss: 0.002663\n","974it [00:55, 13.16it/s]Train epoch: 8 [batch #975, batch_size 16, seq length 1059]\tLoss: 0.002577\n","1000it [00:57, 12.95it/s]Train epoch: 8 [batch #1000, batch_size 16, seq length 1073]\tLoss: 0.002566\n","1024it [00:59, 12.63it/s]Train epoch: 8 [batch #1025, batch_size 16, seq length 1085]\tLoss: 0.002841\n","1050it [01:01, 12.17it/s]Train epoch: 8 [batch #1050, batch_size 16, seq length 1099]\tLoss: 0.002692\n","1074it [01:03, 12.33it/s]Train epoch: 8 [batch #1075, batch_size 16, seq length 1113]\tLoss: 0.002753\n","1100it [01:05, 12.37it/s]Train epoch: 8 [batch #1100, batch_size 16, seq length 1126]\tLoss: 0.002588\n","1124it [01:07, 12.34it/s]Train epoch: 8 [batch #1125, batch_size 16, seq length 1140]\tLoss: 0.002794\n","1150it [01:09, 12.21it/s]Train epoch: 8 [batch #1150, batch_size 16, seq length 1154]\tLoss: 0.002631\n","1174it [01:11, 12.24it/s]Train epoch: 8 [batch #1175, batch_size 16, seq length 1167]\tLoss: 0.002756\n","1200it [01:13, 12.14it/s]Train epoch: 8 [batch #1200, batch_size 16, seq length 1180]\tLoss: 0.002685\n","1224it [01:15, 11.95it/s]Train epoch: 8 [batch #1225, batch_size 16, seq length 1195]\tLoss: 0.002749\n","1250it [01:17, 11.76it/s]Train epoch: 8 [batch #1250, batch_size 16, seq length 1208]\tLoss: 0.002711\n","1274it [01:20, 11.61it/s]Train epoch: 8 [batch #1275, batch_size 16, seq length 1220]\tLoss: 0.002825\n","1300it [01:22, 11.09it/s]Train epoch: 8 [batch #1300, batch_size 16, seq length 1234]\tLoss: 0.003003\n","1324it [01:24, 11.37it/s]Train epoch: 8 [batch #1325, batch_size 16, seq length 1248]\tLoss: 0.002770\n","1350it [01:26, 11.07it/s]Train epoch: 8 [batch #1350, batch_size 16, seq length 1262]\tLoss: 0.002989\n","1374it [01:28, 11.02it/s]Train epoch: 8 [batch #1375, batch_size 16, seq length 1276]\tLoss: 0.003048\n","1400it [01:31, 10.88it/s]Train epoch: 8 [batch #1400, batch_size 16, seq length 1290]\tLoss: 0.003068\n","1424it [01:33, 10.75it/s]Train epoch: 8 [batch #1425, batch_size 16, seq length 1304]\tLoss: 0.003153\n","1450it [01:36, 10.55it/s]Train epoch: 8 [batch #1450, batch_size 16, seq length 1319]\tLoss: 0.002958\n","1474it [01:38, 10.44it/s]Train epoch: 8 [batch #1475, batch_size 16, seq length 1332]\tLoss: 0.003139\n","1500it [01:40, 10.16it/s]Train epoch: 8 [batch #1500, batch_size 16, seq length 1346]\tLoss: 0.003033\n","1524it [01:43, 10.11it/s]Train epoch: 8 [batch #1525, batch_size 16, seq length 1362]\tLoss: 0.002683\n","1550it [01:45,  9.91it/s]Train epoch: 8 [batch #1550, batch_size 16, seq length 1377]\tLoss: 0.003198\n","1575it [01:48,  9.86it/s]Train epoch: 8 [batch #1575, batch_size 16, seq length 1392]\tLoss: 0.002749\n","1600it [01:50,  9.86it/s]Train epoch: 8 [batch #1600, batch_size 16, seq length 1406]\tLoss: 0.002968\n","1625it [01:53,  9.86it/s]Train epoch: 8 [batch #1625, batch_size 16, seq length 1422]\tLoss: 0.003245\n","1650it [01:55,  9.86it/s]Train epoch: 8 [batch #1650, batch_size 16, seq length 1437]\tLoss: 0.003104\n","1675it [01:58,  9.79it/s]Train epoch: 8 [batch #1675, batch_size 16, seq length 1454]\tLoss: 0.003114\n","1700it [02:00,  9.88it/s]Train epoch: 8 [batch #1700, batch_size 16, seq length 1469]\tLoss: 0.003412\n","1725it [02:03,  9.34it/s]Train epoch: 8 [batch #1725, batch_size 16, seq length 1484]\tLoss: 0.003086\n","1750it [02:06,  9.47it/s]Train epoch: 8 [batch #1750, batch_size 16, seq length 1501]\tLoss: 0.002943\n","1775it [02:08,  9.21it/s]Train epoch: 8 [batch #1775, batch_size 16, seq length 1518]\tLoss: 0.003304\n","1800it [02:11,  9.07it/s]Train epoch: 8 [batch #1800, batch_size 16, seq length 1533]\tLoss: 0.003453\n","1825it [02:14,  8.85it/s]Train epoch: 8 [batch #1825, batch_size 16, seq length 1548]\tLoss: 0.003398\n","1850it [02:17,  8.87it/s]Train epoch: 8 [batch #1850, batch_size 16, seq length 1566]\tLoss: 0.003128\n","1875it [02:20,  8.79it/s]Train epoch: 8 [batch #1875, batch_size 16, seq length 1582]\tLoss: 0.003391\n","1900it [02:22,  8.43it/s]Train epoch: 8 [batch #1900, batch_size 16, seq length 1599]\tLoss: 0.003453\n","1925it [02:25,  8.71it/s]Train epoch: 8 [batch #1925, batch_size 16, seq length 1616]\tLoss: 0.003345\n","1950it [02:28,  8.50it/s]Train epoch: 8 [batch #1950, batch_size 16, seq length 1633]\tLoss: 0.003290\n","1975it [02:31,  8.48it/s]Train epoch: 8 [batch #1975, batch_size 16, seq length 1651]\tLoss: 0.003282\n","2000it [02:34,  8.53it/s]Train epoch: 8 [batch #2000, batch_size 16, seq length 1670]\tLoss: 0.003231\n","2025it [02:37,  8.18it/s]Train epoch: 8 [batch #2025, batch_size 16, seq length 1688]\tLoss: 0.003329\n","2050it [02:40,  8.20it/s]Train epoch: 8 [batch #2050, batch_size 16, seq length 1707]\tLoss: 0.003393\n","2075it [02:43,  8.24it/s]Train epoch: 8 [batch #2075, batch_size 16, seq length 1728]\tLoss: 0.003477\n","2100it [02:46,  8.06it/s]Train epoch: 8 [batch #2100, batch_size 16, seq length 1748]\tLoss: 0.003329\n","2125it [02:49,  7.96it/s]Train epoch: 8 [batch #2125, batch_size 16, seq length 1768]\tLoss: 0.003733\n","2150it [02:53,  8.03it/s]Train epoch: 8 [batch #2150, batch_size 16, seq length 1787]\tLoss: 0.003245\n","2175it [02:56,  7.90it/s]Train epoch: 8 [batch #2175, batch_size 16, seq length 1809]\tLoss: 0.003737\n","2200it [02:59,  7.73it/s]Train epoch: 8 [batch #2200, batch_size 16, seq length 1833]\tLoss: 0.003518\n","2225it [03:02,  7.59it/s]Train epoch: 8 [batch #2225, batch_size 16, seq length 1855]\tLoss: 0.003552\n","2250it [03:06,  7.47it/s]Train epoch: 8 [batch #2250, batch_size 16, seq length 1878]\tLoss: 0.003880\n","2275it [03:09,  7.36it/s]Train epoch: 8 [batch #2275, batch_size 16, seq length 1901]\tLoss: 0.003636\n","2300it [03:12,  7.34it/s]Train epoch: 8 [batch #2300, batch_size 16, seq length 1927]\tLoss: 0.003662\n","2325it [03:16,  7.18it/s]Train epoch: 8 [batch #2325, batch_size 16, seq length 1953]\tLoss: 0.003896\n","2350it [03:19,  7.05it/s]Train epoch: 8 [batch #2350, batch_size 16, seq length 1980]\tLoss: 0.003651\n","2375it [03:23,  6.92it/s]Train epoch: 8 [batch #2375, batch_size 16, seq length 2006]\tLoss: 0.003801\n","2400it [03:27,  6.92it/s]Train epoch: 8 [batch #2400, batch_size 16, seq length 2034]\tLoss: 0.003916\n","2425it [03:30,  6.79it/s]Train epoch: 8 [batch #2425, batch_size 16, seq length 2064]\tLoss: 0.003824\n","2450it [03:34,  6.64it/s]Train epoch: 8 [batch #2450, batch_size 16, seq length 2094]\tLoss: 0.004084\n","2475it [03:38,  6.61it/s]Train epoch: 8 [batch #2475, batch_size 16, seq length 2126]\tLoss: 0.004139\n","2500it [03:41,  6.51it/s]Train epoch: 8 [batch #2500, batch_size 16, seq length 2160]\tLoss: 0.004009\n","2525it [03:45,  6.52it/s]Train epoch: 8 [batch #2525, batch_size 16, seq length 2192]\tLoss: 0.004284\n","2550it [03:49,  6.28it/s]Train epoch: 8 [batch #2550, batch_size 16, seq length 2228]\tLoss: 0.004171\n","2575it [03:53,  6.27it/s]Train epoch: 8 [batch #2575, batch_size 16, seq length 2266]\tLoss: 0.004357\n","2600it [03:57,  6.19it/s]Train epoch: 8 [batch #2600, batch_size 16, seq length 2307]\tLoss: 0.004291\n","2625it [04:01,  6.06it/s]Train epoch: 8 [batch #2625, batch_size 16, seq length 2350]\tLoss: 0.004144\n","2650it [04:06,  5.97it/s]Train epoch: 8 [batch #2650, batch_size 16, seq length 2392]\tLoss: 0.004163\n","2675it [04:10,  5.73it/s]Train epoch: 8 [batch #2675, batch_size 16, seq length 2439]\tLoss: 0.004531\n","2700it [04:14,  5.69it/s]Train epoch: 8 [batch #2700, batch_size 16, seq length 2491]\tLoss: 0.004433\n","2725it [04:19,  5.70it/s]Train epoch: 8 [batch #2725, batch_size 16, seq length 2500]\tLoss: 0.004428\n","2750it [04:23,  5.60it/s]Train epoch: 8 [batch #2750, batch_size 16, seq length 2500]\tLoss: 0.004312\n","2775it [04:27,  5.67it/s]Train epoch: 8 [batch #2775, batch_size 16, seq length 2500]\tLoss: 0.004549\n","2800it [04:32,  5.69it/s]Train epoch: 8 [batch #2800, batch_size 16, seq length 2500]\tLoss: 0.004561\n","2825it [04:36,  5.65it/s]Train epoch: 8 [batch #2825, batch_size 16, seq length 2500]\tLoss: 0.004538\n","2850it [04:41,  5.58it/s]Train epoch: 8 [batch #2850, batch_size 16, seq length 2500]\tLoss: 0.004825\n","2875it [04:45,  5.63it/s]Train epoch: 8 [batch #2875, batch_size 16, seq length 2500]\tLoss: 0.005058\n","2900it [04:50,  5.59it/s]Train epoch: 8 [batch #2900, batch_size 16, seq length 2500]\tLoss: 0.005268\n","2925it [04:54,  5.53it/s]Train epoch: 8 [batch #2925, batch_size 16, seq length 2500]\tLoss: 0.005488\n","2950it [04:59,  5.50it/s]Train epoch: 8 [batch #2950, batch_size 16, seq length 2500]\tLoss: 0.005394\n","2975it [05:03,  5.31it/s]Train epoch: 8 [batch #2975, batch_size 16, seq length 2500]\tLoss: 0.006269\n","2983it [05:05,  9.78it/s]\n","epoch loss: 0.0031855236355292113\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:45, 36.14it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0316, 0.0472, 0.0532, 0.0500, 0.8354\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2679, 0.4031, 0.4440, 0.4226, 0.9716\n","rec_at_8: 0.3161\n","prec_at_8: 0.5813\n","rec_at_15: 0.4336\n","prec_at_15: 0.4448\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/rnn_May_04_023326\n","\n","EPOCH 9\n","0it [00:00, ?it/s]Train epoch: 9 [batch #0, batch_size 16, seq length 117]\tLoss: 0.004386\n","23it [00:00, 37.16it/s]Train epoch: 9 [batch #25, batch_size 16, seq length 337]\tLoss: 0.002603\n","47it [00:01, 33.01it/s]Train epoch: 9 [batch #50, batch_size 16, seq length 402]\tLoss: 0.002146\n","75it [00:02, 29.76it/s]Train epoch: 9 [batch #75, batch_size 16, seq length 452]\tLoss: 0.001873\n","99it [00:03, 26.57it/s]Train epoch: 9 [batch #100, batch_size 16, seq length 490]\tLoss: 0.001766\n","123it [00:04, 25.35it/s]Train epoch: 9 [batch #125, batch_size 16, seq length 520]\tLoss: 0.001979\n","150it [00:05, 24.65it/s]Train epoch: 9 [batch #150, batch_size 16, seq length 548]\tLoss: 0.002044\n","174it [00:06, 23.49it/s]Train epoch: 9 [batch #175, batch_size 16, seq length 574]\tLoss: 0.002003\n","198it [00:07, 22.37it/s]Train epoch: 9 [batch #200, batch_size 16, seq length 596]\tLoss: 0.002061\n","225it [00:08, 21.93it/s]Train epoch: 9 [batch #225, batch_size 16, seq length 618]\tLoss: 0.002091\n","249it [00:09, 20.83it/s]Train epoch: 9 [batch #250, batch_size 16, seq length 638]\tLoss: 0.001990\n","273it [00:10, 20.46it/s]Train epoch: 9 [batch #275, batch_size 16, seq length 656]\tLoss: 0.002170\n","299it [00:12, 19.66it/s]Train epoch: 9 [batch #300, batch_size 16, seq length 674]\tLoss: 0.002088\n","324it [00:13, 19.50it/s]Train epoch: 9 [batch #325, batch_size 16, seq length 693]\tLoss: 0.001980\n","350it [00:14, 18.99it/s]Train epoch: 9 [batch #350, batch_size 16, seq length 710]\tLoss: 0.002232\n","374it [00:16, 18.53it/s]Train epoch: 9 [batch #375, batch_size 16, seq length 726]\tLoss: 0.002414\n","400it [00:17, 17.97it/s]Train epoch: 9 [batch #400, batch_size 16, seq length 743]\tLoss: 0.002091\n","424it [00:18, 17.00it/s]Train epoch: 9 [batch #425, batch_size 16, seq length 758]\tLoss: 0.002135\n","450it [00:20, 17.52it/s]Train epoch: 9 [batch #450, batch_size 16, seq length 773]\tLoss: 0.002344\n","474it [00:21, 16.44it/s]Train epoch: 9 [batch #475, batch_size 16, seq length 789]\tLoss: 0.002115\n","500it [00:23, 16.56it/s]Train epoch: 9 [batch #500, batch_size 16, seq length 803]\tLoss: 0.002321\n","524it [00:24, 16.23it/s]Train epoch: 9 [batch #525, batch_size 16, seq length 818]\tLoss: 0.002242\n","550it [00:26, 16.36it/s]Train epoch: 9 [batch #550, batch_size 16, seq length 833]\tLoss: 0.002086\n","574it [00:27, 16.24it/s]Train epoch: 9 [batch #575, batch_size 16, seq length 846]\tLoss: 0.002172\n","600it [00:29, 15.66it/s]Train epoch: 9 [batch #600, batch_size 16, seq length 860]\tLoss: 0.002370\n","624it [00:31, 15.30it/s]Train epoch: 9 [batch #625, batch_size 16, seq length 874]\tLoss: 0.002195\n","650it [00:32, 15.29it/s]Train epoch: 9 [batch #650, batch_size 16, seq length 887]\tLoss: 0.002123\n","674it [00:34, 15.08it/s]Train epoch: 9 [batch #675, batch_size 16, seq length 901]\tLoss: 0.002151\n","700it [00:36, 14.64it/s]Train epoch: 9 [batch #700, batch_size 16, seq length 915]\tLoss: 0.002053\n","724it [00:37, 14.53it/s]Train epoch: 9 [batch #725, batch_size 16, seq length 929]\tLoss: 0.002257\n","750it [00:39, 14.36it/s]Train epoch: 9 [batch #750, batch_size 16, seq length 942]\tLoss: 0.002382\n","774it [00:41, 14.03it/s]Train epoch: 9 [batch #775, batch_size 16, seq length 954]\tLoss: 0.002336\n","800it [00:43, 14.02it/s]Train epoch: 9 [batch #800, batch_size 16, seq length 968]\tLoss: 0.002408\n","824it [00:44, 13.91it/s]Train epoch: 9 [batch #825, batch_size 16, seq length 981]\tLoss: 0.002330\n","850it [00:46, 13.49it/s]Train epoch: 9 [batch #850, batch_size 16, seq length 994]\tLoss: 0.002436\n","874it [00:48, 13.53it/s]Train epoch: 9 [batch #875, batch_size 16, seq length 1006]\tLoss: 0.002228\n","900it [00:50, 13.37it/s]Train epoch: 9 [batch #900, batch_size 16, seq length 1020]\tLoss: 0.002513\n","924it [00:52, 13.27it/s]Train epoch: 9 [batch #925, batch_size 16, seq length 1033]\tLoss: 0.002309\n","950it [00:54, 13.22it/s]Train epoch: 9 [batch #950, batch_size 16, seq length 1046]\tLoss: 0.002512\n","974it [00:56, 13.10it/s]Train epoch: 9 [batch #975, batch_size 16, seq length 1059]\tLoss: 0.002438\n","1000it [00:58, 12.91it/s]Train epoch: 9 [batch #1000, batch_size 16, seq length 1073]\tLoss: 0.002443\n","1024it [01:00, 12.47it/s]Train epoch: 9 [batch #1025, batch_size 16, seq length 1085]\tLoss: 0.002669\n","1050it [01:02, 12.35it/s]Train epoch: 9 [batch #1050, batch_size 16, seq length 1099]\tLoss: 0.002551\n","1074it [01:04, 12.26it/s]Train epoch: 9 [batch #1075, batch_size 16, seq length 1113]\tLoss: 0.002593\n","1100it [01:06, 12.37it/s]Train epoch: 9 [batch #1100, batch_size 16, seq length 1126]\tLoss: 0.002425\n","1124it [01:08, 12.26it/s]Train epoch: 9 [batch #1125, batch_size 16, seq length 1140]\tLoss: 0.002653\n","1150it [01:10, 12.34it/s]Train epoch: 9 [batch #1150, batch_size 16, seq length 1154]\tLoss: 0.002541\n","1174it [01:12, 12.07it/s]Train epoch: 9 [batch #1175, batch_size 16, seq length 1167]\tLoss: 0.002591\n","1200it [01:14, 12.10it/s]Train epoch: 9 [batch #1200, batch_size 16, seq length 1180]\tLoss: 0.002521\n","1224it [01:16, 11.93it/s]Train epoch: 9 [batch #1225, batch_size 16, seq length 1195]\tLoss: 0.002620\n","1250it [01:18, 11.85it/s]Train epoch: 9 [batch #1250, batch_size 16, seq length 1208]\tLoss: 0.002563\n","1274it [01:20, 11.69it/s]Train epoch: 9 [batch #1275, batch_size 16, seq length 1220]\tLoss: 0.002663\n","1300it [01:22, 11.41it/s]Train epoch: 9 [batch #1300, batch_size 16, seq length 1234]\tLoss: 0.002823\n","1324it [01:25, 11.22it/s]Train epoch: 9 [batch #1325, batch_size 16, seq length 1248]\tLoss: 0.002638\n","1350it [01:27, 11.21it/s]Train epoch: 9 [batch #1350, batch_size 16, seq length 1262]\tLoss: 0.002839\n","1374it [01:29, 11.05it/s]Train epoch: 9 [batch #1375, batch_size 16, seq length 1276]\tLoss: 0.002931\n","1400it [01:31, 10.86it/s]Train epoch: 9 [batch #1400, batch_size 16, seq length 1290]\tLoss: 0.002956\n","1424it [01:34, 10.76it/s]Train epoch: 9 [batch #1425, batch_size 16, seq length 1304]\tLoss: 0.002965\n","1450it [01:36, 10.62it/s]Train epoch: 9 [batch #1450, batch_size 16, seq length 1319]\tLoss: 0.002798\n","1474it [01:38, 10.50it/s]Train epoch: 9 [batch #1475, batch_size 16, seq length 1332]\tLoss: 0.003005\n","1500it [01:41, 10.13it/s]Train epoch: 9 [batch #1500, batch_size 16, seq length 1346]\tLoss: 0.002862\n","1524it [01:43, 10.24it/s]Train epoch: 9 [batch #1525, batch_size 16, seq length 1362]\tLoss: 0.002545\n","1550it [01:46,  9.97it/s]Train epoch: 9 [batch #1550, batch_size 16, seq length 1377]\tLoss: 0.003067\n","1574it [01:48,  9.88it/s]Train epoch: 9 [batch #1575, batch_size 16, seq length 1392]\tLoss: 0.002608\n","1600it [01:51,  9.88it/s]Train epoch: 9 [batch #1600, batch_size 16, seq length 1406]\tLoss: 0.002806\n","1625it [01:53,  9.91it/s]Train epoch: 9 [batch #1625, batch_size 16, seq length 1422]\tLoss: 0.003055\n","1650it [01:56,  9.85it/s]Train epoch: 9 [batch #1650, batch_size 16, seq length 1437]\tLoss: 0.002900\n","1675it [01:58,  9.89it/s]Train epoch: 9 [batch #1675, batch_size 16, seq length 1454]\tLoss: 0.002903\n","1700it [02:01,  9.60it/s]Train epoch: 9 [batch #1700, batch_size 16, seq length 1469]\tLoss: 0.003161\n","1725it [02:04,  9.60it/s]Train epoch: 9 [batch #1725, batch_size 16, seq length 1484]\tLoss: 0.002940\n","1750it [02:06,  9.47it/s]Train epoch: 9 [batch #1750, batch_size 16, seq length 1501]\tLoss: 0.002753\n","1775it [02:09,  9.33it/s]Train epoch: 9 [batch #1775, batch_size 16, seq length 1518]\tLoss: 0.003119\n","1800it [02:12,  9.14it/s]Train epoch: 9 [batch #1800, batch_size 16, seq length 1533]\tLoss: 0.003263\n","1825it [02:14,  9.10it/s]Train epoch: 9 [batch #1825, batch_size 16, seq length 1548]\tLoss: 0.003206\n","1850it [02:17,  8.97it/s]Train epoch: 9 [batch #1850, batch_size 16, seq length 1566]\tLoss: 0.002958\n","1875it [02:20,  8.82it/s]Train epoch: 9 [batch #1875, batch_size 16, seq length 1582]\tLoss: 0.003171\n","1900it [02:23,  8.78it/s]Train epoch: 9 [batch #1900, batch_size 16, seq length 1599]\tLoss: 0.003230\n","1925it [02:26,  8.59it/s]Train epoch: 9 [batch #1925, batch_size 16, seq length 1616]\tLoss: 0.003161\n","1950it [02:29,  8.65it/s]Train epoch: 9 [batch #1950, batch_size 16, seq length 1633]\tLoss: 0.003124\n","1975it [02:32,  8.60it/s]Train epoch: 9 [batch #1975, batch_size 16, seq length 1651]\tLoss: 0.003077\n","2000it [02:35,  8.40it/s]Train epoch: 9 [batch #2000, batch_size 16, seq length 1670]\tLoss: 0.003073\n","2025it [02:38,  8.19it/s]Train epoch: 9 [batch #2025, batch_size 16, seq length 1688]\tLoss: 0.003128\n","2050it [02:41,  8.24it/s]Train epoch: 9 [batch #2050, batch_size 16, seq length 1707]\tLoss: 0.003213\n","2075it [02:44,  8.21it/s]Train epoch: 9 [batch #2075, batch_size 16, seq length 1728]\tLoss: 0.003334\n","2100it [02:47,  8.23it/s]Train epoch: 9 [batch #2100, batch_size 16, seq length 1748]\tLoss: 0.003147\n","2125it [02:50,  8.13it/s]Train epoch: 9 [batch #2125, batch_size 16, seq length 1768]\tLoss: 0.003554\n","2150it [02:53,  8.08it/s]Train epoch: 9 [batch #2150, batch_size 16, seq length 1787]\tLoss: 0.003093\n","2175it [02:56,  7.71it/s]Train epoch: 9 [batch #2175, batch_size 16, seq length 1809]\tLoss: 0.003542\n","2200it [02:59,  7.85it/s]Train epoch: 9 [batch #2200, batch_size 16, seq length 1833]\tLoss: 0.003270\n","2225it [03:03,  7.52it/s]Train epoch: 9 [batch #2225, batch_size 16, seq length 1855]\tLoss: 0.003292\n","2250it [03:06,  7.41it/s]Train epoch: 9 [batch #2250, batch_size 16, seq length 1878]\tLoss: 0.003671\n","2275it [03:09,  7.36it/s]Train epoch: 9 [batch #2275, batch_size 16, seq length 1901]\tLoss: 0.003380\n","2300it [03:13,  7.25it/s]Train epoch: 9 [batch #2300, batch_size 16, seq length 1927]\tLoss: 0.003455\n","2325it [03:16,  7.27it/s]Train epoch: 9 [batch #2325, batch_size 16, seq length 1953]\tLoss: 0.003673\n","2350it [03:20,  7.00it/s]Train epoch: 9 [batch #2350, batch_size 16, seq length 1980]\tLoss: 0.003398\n","2375it [03:23,  7.06it/s]Train epoch: 9 [batch #2375, batch_size 16, seq length 2006]\tLoss: 0.003573\n","2400it [03:27,  6.95it/s]Train epoch: 9 [batch #2400, batch_size 16, seq length 2034]\tLoss: 0.003675\n","2425it [03:30,  6.93it/s]Train epoch: 9 [batch #2425, batch_size 16, seq length 2064]\tLoss: 0.003619\n","2450it [03:34,  6.81it/s]Train epoch: 9 [batch #2450, batch_size 16, seq length 2094]\tLoss: 0.003858\n","2475it [03:38,  6.60it/s]Train epoch: 9 [batch #2475, batch_size 16, seq length 2126]\tLoss: 0.003855\n","2500it [03:42,  6.42it/s]Train epoch: 9 [batch #2500, batch_size 16, seq length 2160]\tLoss: 0.003822\n","2525it [03:46,  6.50it/s]Train epoch: 9 [batch #2525, batch_size 16, seq length 2192]\tLoss: 0.004077\n","2550it [03:50,  6.32it/s]Train epoch: 9 [batch #2550, batch_size 16, seq length 2228]\tLoss: 0.003955\n","2575it [03:53,  6.32it/s]Train epoch: 9 [batch #2575, batch_size 16, seq length 2266]\tLoss: 0.004136\n","2600it [03:58,  6.08it/s]Train epoch: 9 [batch #2600, batch_size 16, seq length 2307]\tLoss: 0.004019\n","2625it [04:02,  6.03it/s]Train epoch: 9 [batch #2625, batch_size 16, seq length 2350]\tLoss: 0.003927\n","2650it [04:06,  5.92it/s]Train epoch: 9 [batch #2650, batch_size 16, seq length 2392]\tLoss: 0.004064\n","2675it [04:10,  5.87it/s]Train epoch: 9 [batch #2675, batch_size 16, seq length 2439]\tLoss: 0.004288\n","2700it [04:14,  5.58it/s]Train epoch: 9 [batch #2700, batch_size 16, seq length 2491]\tLoss: 0.004160\n","2725it [04:19,  5.66it/s]Train epoch: 9 [batch #2725, batch_size 16, seq length 2500]\tLoss: 0.004249\n","2750it [04:23,  5.63it/s]Train epoch: 9 [batch #2750, batch_size 16, seq length 2500]\tLoss: 0.004103\n","2775it [04:28,  5.67it/s]Train epoch: 9 [batch #2775, batch_size 16, seq length 2500]\tLoss: 0.004345\n","2800it [04:32,  5.59it/s]Train epoch: 9 [batch #2800, batch_size 16, seq length 2500]\tLoss: 0.004342\n","2825it [04:37,  5.63it/s]Train epoch: 9 [batch #2825, batch_size 16, seq length 2500]\tLoss: 0.004316\n","2850it [04:41,  5.54it/s]Train epoch: 9 [batch #2850, batch_size 16, seq length 2500]\tLoss: 0.004472\n","2875it [04:46,  5.54it/s]Train epoch: 9 [batch #2875, batch_size 16, seq length 2500]\tLoss: 0.004774\n","2900it [04:50,  5.56it/s]Train epoch: 9 [batch #2900, batch_size 16, seq length 2500]\tLoss: 0.005020\n","2925it [04:55,  5.58it/s]Train epoch: 9 [batch #2925, batch_size 16, seq length 2500]\tLoss: 0.005174\n","2950it [04:59,  5.51it/s]Train epoch: 9 [batch #2950, batch_size 16, seq length 2500]\tLoss: 0.005233\n","2975it [05:04,  5.44it/s]Train epoch: 9 [batch #2975, batch_size 16, seq length 2500]\tLoss: 0.006007\n","2983it [05:05,  9.75it/s]\n","epoch loss: 0.0030175252894687047\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:44, 36.37it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0313, 0.0487, 0.0485, 0.0486, 0.8337\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2726, 0.4335, 0.4235, 0.4284, 0.9710\n","rec_at_8: 0.3189\n","prec_at_8: 0.5867\n","rec_at_15: 0.4341\n","prec_at_15: 0.4448\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/rnn_May_04_023326\n","\n","EPOCH 10\n","0it [00:00, ?it/s]Train epoch: 10 [batch #0, batch_size 16, seq length 117]\tLoss: 0.004361\n","24it [00:00, 37.63it/s]Train epoch: 10 [batch #25, batch_size 16, seq length 337]\tLoss: 0.002485\n","48it [00:01, 33.65it/s]Train epoch: 10 [batch #50, batch_size 16, seq length 402]\tLoss: 0.002025\n","72it [00:02, 30.34it/s]Train epoch: 10 [batch #75, batch_size 16, seq length 452]\tLoss: 0.001773\n","98it [00:03, 27.48it/s]Train epoch: 10 [batch #100, batch_size 16, seq length 490]\tLoss: 0.001685\n","125it [00:04, 25.38it/s]Train epoch: 10 [batch #125, batch_size 16, seq length 520]\tLoss: 0.001955\n","149it [00:05, 24.63it/s]Train epoch: 10 [batch #150, batch_size 16, seq length 548]\tLoss: 0.001973\n","173it [00:06, 23.32it/s]Train epoch: 10 [batch #175, batch_size 16, seq length 574]\tLoss: 0.001895\n","200it [00:07, 22.77it/s]Train epoch: 10 [batch #200, batch_size 16, seq length 596]\tLoss: 0.001977\n","224it [00:08, 21.44it/s]Train epoch: 10 [batch #225, batch_size 16, seq length 618]\tLoss: 0.001996\n","248it [00:09, 21.32it/s]Train epoch: 10 [batch #250, batch_size 16, seq length 638]\tLoss: 0.001917\n","275it [00:10, 20.21it/s]Train epoch: 10 [batch #275, batch_size 16, seq length 656]\tLoss: 0.002063\n","299it [00:12, 20.03it/s]Train epoch: 10 [batch #300, batch_size 16, seq length 674]\tLoss: 0.002017\n","325it [00:13, 19.63it/s]Train epoch: 10 [batch #325, batch_size 16, seq length 693]\tLoss: 0.001875\n","349it [00:14, 18.95it/s]Train epoch: 10 [batch #350, batch_size 16, seq length 710]\tLoss: 0.002102\n","375it [00:16, 18.33it/s]Train epoch: 10 [batch #375, batch_size 16, seq length 726]\tLoss: 0.002250\n","399it [00:17, 17.80it/s]Train epoch: 10 [batch #400, batch_size 16, seq length 743]\tLoss: 0.001949\n","425it [00:18, 17.81it/s]Train epoch: 10 [batch #425, batch_size 16, seq length 758]\tLoss: 0.002044\n","449it [00:20, 17.19it/s]Train epoch: 10 [batch #450, batch_size 16, seq length 773]\tLoss: 0.002235\n","475it [00:21, 16.48it/s]Train epoch: 10 [batch #475, batch_size 16, seq length 789]\tLoss: 0.001987\n","499it [00:23, 16.25it/s]Train epoch: 10 [batch #500, batch_size 16, seq length 803]\tLoss: 0.002197\n","525it [00:24, 16.49it/s]Train epoch: 10 [batch #525, batch_size 16, seq length 818]\tLoss: 0.002097\n","549it [00:26, 16.56it/s]Train epoch: 10 [batch #550, batch_size 16, seq length 833]\tLoss: 0.001977\n","575it [00:27, 16.47it/s]Train epoch: 10 [batch #575, batch_size 16, seq length 846]\tLoss: 0.002044\n","599it [00:29, 15.21it/s]Train epoch: 10 [batch #600, batch_size 16, seq length 860]\tLoss: 0.002299\n","625it [00:31, 15.84it/s]Train epoch: 10 [batch #625, batch_size 16, seq length 874]\tLoss: 0.002077\n","649it [00:32, 15.23it/s]Train epoch: 10 [batch #650, batch_size 16, seq length 887]\tLoss: 0.002002\n","675it [00:34, 14.87it/s]Train epoch: 10 [batch #675, batch_size 16, seq length 901]\tLoss: 0.002015\n","699it [00:36, 14.68it/s]Train epoch: 10 [batch #700, batch_size 16, seq length 915]\tLoss: 0.001963\n","725it [00:37, 14.63it/s]Train epoch: 10 [batch #725, batch_size 16, seq length 929]\tLoss: 0.002111\n","749it [00:39, 14.49it/s]Train epoch: 10 [batch #750, batch_size 16, seq length 942]\tLoss: 0.002284\n","775it [00:41, 14.21it/s]Train epoch: 10 [batch #775, batch_size 16, seq length 954]\tLoss: 0.002177\n","799it [00:42, 13.68it/s]Train epoch: 10 [batch #800, batch_size 16, seq length 968]\tLoss: 0.002263\n","825it [00:44, 13.81it/s]Train epoch: 10 [batch #825, batch_size 16, seq length 981]\tLoss: 0.002213\n","849it [00:46, 13.58it/s]Train epoch: 10 [batch #850, batch_size 16, seq length 994]\tLoss: 0.002304\n","875it [00:48, 13.29it/s]Train epoch: 10 [batch #875, batch_size 16, seq length 1006]\tLoss: 0.002136\n","899it [00:50, 13.35it/s]Train epoch: 10 [batch #900, batch_size 16, seq length 1020]\tLoss: 0.002348\n","925it [00:52, 13.17it/s]Train epoch: 10 [batch #925, batch_size 16, seq length 1033]\tLoss: 0.002158\n","949it [00:54, 13.15it/s]Train epoch: 10 [batch #950, batch_size 16, seq length 1046]\tLoss: 0.002307\n","975it [00:56, 12.94it/s]Train epoch: 10 [batch #975, batch_size 16, seq length 1059]\tLoss: 0.002295\n","999it [00:58, 12.75it/s]Train epoch: 10 [batch #1000, batch_size 16, seq length 1073]\tLoss: 0.002312\n","1025it [01:00, 12.64it/s]Train epoch: 10 [batch #1025, batch_size 16, seq length 1085]\tLoss: 0.002469\n","1049it [01:02, 12.34it/s]Train epoch: 10 [batch #1050, batch_size 16, seq length 1099]\tLoss: 0.002363\n","1075it [01:04, 12.05it/s]Train epoch: 10 [batch #1075, batch_size 16, seq length 1113]\tLoss: 0.002466\n","1099it [01:06, 12.24it/s]Train epoch: 10 [batch #1100, batch_size 16, seq length 1126]\tLoss: 0.002341\n","1125it [01:08, 12.42it/s]Train epoch: 10 [batch #1125, batch_size 16, seq length 1140]\tLoss: 0.002562\n","1149it [01:10, 11.94it/s]Train epoch: 10 [batch #1150, batch_size 16, seq length 1154]\tLoss: 0.002352\n","1175it [01:12, 12.01it/s]Train epoch: 10 [batch #1175, batch_size 16, seq length 1167]\tLoss: 0.002477\n","1199it [01:14, 11.85it/s]Train epoch: 10 [batch #1200, batch_size 16, seq length 1180]\tLoss: 0.002386\n","1225it [01:16, 11.78it/s]Train epoch: 10 [batch #1225, batch_size 16, seq length 1195]\tLoss: 0.002440\n","1249it [01:18, 11.45it/s]Train epoch: 10 [batch #1250, batch_size 16, seq length 1208]\tLoss: 0.002412\n","1275it [01:21, 11.38it/s]Train epoch: 10 [batch #1275, batch_size 16, seq length 1220]\tLoss: 0.002452\n","1299it [01:23, 11.43it/s]Train epoch: 10 [batch #1300, batch_size 16, seq length 1234]\tLoss: 0.002664\n","1325it [01:25, 11.17it/s]Train epoch: 10 [batch #1325, batch_size 16, seq length 1248]\tLoss: 0.002475\n","1349it [01:27, 10.94it/s]Train epoch: 10 [batch #1350, batch_size 16, seq length 1262]\tLoss: 0.002593\n","1375it [01:30, 10.97it/s]Train epoch: 10 [batch #1375, batch_size 16, seq length 1276]\tLoss: 0.002769\n","1399it [01:32, 10.79it/s]Train epoch: 10 [batch #1400, batch_size 16, seq length 1290]\tLoss: 0.002819\n","1425it [01:34, 10.51it/s]Train epoch: 10 [batch #1425, batch_size 16, seq length 1304]\tLoss: 0.002819\n","1449it [01:36, 10.55it/s]Train epoch: 10 [batch #1450, batch_size 16, seq length 1319]\tLoss: 0.002608\n","1475it [01:39, 10.53it/s]Train epoch: 10 [batch #1475, batch_size 16, seq length 1332]\tLoss: 0.002854\n","1499it [01:41, 10.38it/s]Train epoch: 10 [batch #1500, batch_size 16, seq length 1346]\tLoss: 0.002674\n","1525it [01:44,  9.98it/s]Train epoch: 10 [batch #1525, batch_size 16, seq length 1362]\tLoss: 0.002392\n","1549it [01:46,  9.84it/s]Train epoch: 10 [batch #1550, batch_size 16, seq length 1377]\tLoss: 0.002933\n","1575it [01:49,  9.88it/s]Train epoch: 10 [batch #1575, batch_size 16, seq length 1392]\tLoss: 0.002454\n","1600it [01:51,  9.86it/s]Train epoch: 10 [batch #1600, batch_size 16, seq length 1406]\tLoss: 0.002599\n","1625it [01:54,  9.82it/s]Train epoch: 10 [batch #1625, batch_size 16, seq length 1422]\tLoss: 0.002905\n","1650it [01:57,  9.53it/s]Train epoch: 10 [batch #1650, batch_size 16, seq length 1437]\tLoss: 0.002754\n","1675it [01:59,  9.59it/s]Train epoch: 10 [batch #1675, batch_size 16, seq length 1454]\tLoss: 0.002795\n","1700it [02:02,  9.40it/s]Train epoch: 10 [batch #1700, batch_size 16, seq length 1469]\tLoss: 0.002998\n","1725it [02:04,  9.45it/s]Train epoch: 10 [batch #1725, batch_size 16, seq length 1484]\tLoss: 0.002730\n","1750it [02:07,  9.24it/s]Train epoch: 10 [batch #1750, batch_size 16, seq length 1501]\tLoss: 0.002608\n","1775it [02:10,  9.38it/s]Train epoch: 10 [batch #1775, batch_size 16, seq length 1518]\tLoss: 0.002990\n","1800it [02:13,  9.14it/s]Train epoch: 10 [batch #1800, batch_size 16, seq length 1533]\tLoss: 0.003131\n","1825it [02:15,  9.18it/s]Train epoch: 10 [batch #1825, batch_size 16, seq length 1548]\tLoss: 0.003072\n","1850it [02:18,  8.82it/s]Train epoch: 10 [batch #1850, batch_size 16, seq length 1566]\tLoss: 0.002775\n","1875it [02:21,  8.62it/s]Train epoch: 10 [batch #1875, batch_size 16, seq length 1582]\tLoss: 0.003013\n","1900it [02:24,  8.81it/s]Train epoch: 10 [batch #1900, batch_size 16, seq length 1599]\tLoss: 0.003088\n","1925it [02:27,  8.28it/s]Train epoch: 10 [batch #1925, batch_size 16, seq length 1616]\tLoss: 0.003015\n","1950it [02:30,  8.60it/s]Train epoch: 10 [batch #1950, batch_size 16, seq length 1633]\tLoss: 0.002994\n","1975it [02:33,  8.42it/s]Train epoch: 10 [batch #1975, batch_size 16, seq length 1651]\tLoss: 0.002956\n","2000it [02:36,  8.29it/s]Train epoch: 10 [batch #2000, batch_size 16, seq length 1670]\tLoss: 0.002925\n","2025it [02:39,  8.21it/s]Train epoch: 10 [batch #2025, batch_size 16, seq length 1688]\tLoss: 0.002990\n","2050it [02:42,  8.17it/s]Train epoch: 10 [batch #2050, batch_size 16, seq length 1707]\tLoss: 0.003068\n","2075it [02:45,  8.23it/s]Train epoch: 10 [batch #2075, batch_size 16, seq length 1728]\tLoss: 0.003202\n","2100it [02:48,  8.12it/s]Train epoch: 10 [batch #2100, batch_size 16, seq length 1748]\tLoss: 0.003013\n","2125it [02:51,  8.01it/s]Train epoch: 10 [batch #2125, batch_size 16, seq length 1768]\tLoss: 0.003355\n","2150it [02:54,  8.07it/s]Train epoch: 10 [batch #2150, batch_size 16, seq length 1787]\tLoss: 0.002896\n","2175it [02:57,  7.91it/s]Train epoch: 10 [batch #2175, batch_size 16, seq length 1809]\tLoss: 0.003373\n","2200it [03:00,  7.81it/s]Train epoch: 10 [batch #2200, batch_size 16, seq length 1833]\tLoss: 0.003153\n","2225it [03:04,  7.70it/s]Train epoch: 10 [batch #2225, batch_size 16, seq length 1855]\tLoss: 0.003117\n","2250it [03:07,  7.61it/s]Train epoch: 10 [batch #2250, batch_size 16, seq length 1878]\tLoss: 0.003508\n","2275it [03:10,  7.49it/s]Train epoch: 10 [batch #2275, batch_size 16, seq length 1901]\tLoss: 0.003284\n","2300it [03:14,  7.28it/s]Train epoch: 10 [batch #2300, batch_size 16, seq length 1927]\tLoss: 0.003330\n","2325it [03:17,  7.09it/s]Train epoch: 10 [batch #2325, batch_size 16, seq length 1953]\tLoss: 0.003492\n","2350it [03:21,  7.07it/s]Train epoch: 10 [batch #2350, batch_size 16, seq length 1980]\tLoss: 0.003347\n","2375it [03:24,  6.98it/s]Train epoch: 10 [batch #2375, batch_size 16, seq length 2006]\tLoss: 0.003431\n","2400it [03:28,  6.91it/s]Train epoch: 10 [batch #2400, batch_size 16, seq length 2034]\tLoss: 0.003525\n","2425it [03:32,  6.83it/s]Train epoch: 10 [batch #2425, batch_size 16, seq length 2064]\tLoss: 0.003442\n","2450it [03:35,  6.71it/s]Train epoch: 10 [batch #2450, batch_size 16, seq length 2094]\tLoss: 0.003662\n","2475it [03:39,  6.66it/s]Train epoch: 10 [batch #2475, batch_size 16, seq length 2126]\tLoss: 0.003659\n","2500it [03:43,  6.46it/s]Train epoch: 10 [batch #2500, batch_size 16, seq length 2160]\tLoss: 0.003595\n","2525it [03:47,  6.45it/s]Train epoch: 10 [batch #2525, batch_size 16, seq length 2192]\tLoss: 0.003814\n","2550it [03:51,  6.38it/s]Train epoch: 10 [batch #2550, batch_size 16, seq length 2228]\tLoss: 0.003743\n","2575it [03:55,  6.29it/s]Train epoch: 10 [batch #2575, batch_size 16, seq length 2266]\tLoss: 0.003943\n","2600it [03:59,  6.17it/s]Train epoch: 10 [batch #2600, batch_size 16, seq length 2307]\tLoss: 0.003831\n","2625it [04:03,  6.02it/s]Train epoch: 10 [batch #2625, batch_size 16, seq length 2350]\tLoss: 0.003771\n","2650it [04:07,  5.99it/s]Train epoch: 10 [batch #2650, batch_size 16, seq length 2392]\tLoss: 0.003875\n","2675it [04:11,  5.86it/s]Train epoch: 10 [batch #2675, batch_size 16, seq length 2439]\tLoss: 0.004132\n","2700it [04:15,  5.75it/s]Train epoch: 10 [batch #2700, batch_size 16, seq length 2491]\tLoss: 0.004037\n","2725it [04:20,  5.67it/s]Train epoch: 10 [batch #2725, batch_size 16, seq length 2500]\tLoss: 0.004107\n","2750it [04:24,  5.64it/s]Train epoch: 10 [batch #2750, batch_size 16, seq length 2500]\tLoss: 0.003981\n","2775it [04:29,  5.57it/s]Train epoch: 10 [batch #2775, batch_size 16, seq length 2500]\tLoss: 0.004183\n","2800it [04:33,  5.68it/s]Train epoch: 10 [batch #2800, batch_size 16, seq length 2500]\tLoss: 0.004083\n","2825it [04:38,  5.62it/s]Train epoch: 10 [batch #2825, batch_size 16, seq length 2500]\tLoss: 0.004152\n","2850it [04:42,  5.64it/s]Train epoch: 10 [batch #2850, batch_size 16, seq length 2500]\tLoss: 0.004219\n","2875it [04:47,  5.59it/s]Train epoch: 10 [batch #2875, batch_size 16, seq length 2500]\tLoss: 0.004548\n","2900it [04:51,  5.56it/s]Train epoch: 10 [batch #2900, batch_size 16, seq length 2500]\tLoss: 0.004765\n","2925it [04:56,  5.56it/s]Train epoch: 10 [batch #2925, batch_size 16, seq length 2500]\tLoss: 0.004999\n","2950it [05:00,  5.50it/s]Train epoch: 10 [batch #2950, batch_size 16, seq length 2500]\tLoss: 0.005003\n","2975it [05:05,  5.32it/s]Train epoch: 10 [batch #2975, batch_size 16, seq length 2500]\tLoss: 0.005671\n","2983it [05:06,  9.73it/s]\n","epoch loss: 0.002866078229464011\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:45, 36.22it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0302, 0.0487, 0.0440, 0.0462, 0.8298\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2792, 0.4868, 0.3956, 0.4365, 0.9694\n","rec_at_8: 0.3240\n","prec_at_8: 0.5963\n","rec_at_15: 0.4382\n","prec_at_15: 0.4506\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/rnn_May_04_023326\n","\n","EPOCH 11\n","0it [00:00, ?it/s]Train epoch: 11 [batch #0, batch_size 16, seq length 117]\tLoss: 0.004407\n","24it [00:00, 36.40it/s]Train epoch: 11 [batch #25, batch_size 16, seq length 337]\tLoss: 0.002439\n","48it [00:01, 33.27it/s]Train epoch: 11 [batch #50, batch_size 16, seq length 402]\tLoss: 0.001914\n","72it [00:02, 30.15it/s]Train epoch: 11 [batch #75, batch_size 16, seq length 452]\tLoss: 0.001719\n","100it [00:03, 27.06it/s]Train epoch: 11 [batch #100, batch_size 16, seq length 490]\tLoss: 0.001594\n","124it [00:04, 25.84it/s]Train epoch: 11 [batch #125, batch_size 16, seq length 520]\tLoss: 0.001799\n","148it [00:05, 24.49it/s]Train epoch: 11 [batch #150, batch_size 16, seq length 548]\tLoss: 0.001872\n","175it [00:06, 23.83it/s]Train epoch: 11 [batch #175, batch_size 16, seq length 574]\tLoss: 0.001816\n","199it [00:07, 22.87it/s]Train epoch: 11 [batch #200, batch_size 16, seq length 596]\tLoss: 0.001880\n","223it [00:08, 22.16it/s]Train epoch: 11 [batch #225, batch_size 16, seq length 618]\tLoss: 0.001848\n","250it [00:09, 20.99it/s]Train epoch: 11 [batch #250, batch_size 16, seq length 638]\tLoss: 0.001822\n","274it [00:10, 20.49it/s]Train epoch: 11 [batch #275, batch_size 16, seq length 656]\tLoss: 0.001968\n","298it [00:12, 20.30it/s]Train epoch: 11 [batch #300, batch_size 16, seq length 674]\tLoss: 0.001932\n","324it [00:13, 19.53it/s]Train epoch: 11 [batch #325, batch_size 16, seq length 693]\tLoss: 0.001796\n","349it [00:14, 18.71it/s]Train epoch: 11 [batch #350, batch_size 16, seq length 710]\tLoss: 0.001997\n","375it [00:16, 18.45it/s]Train epoch: 11 [batch #375, batch_size 16, seq length 726]\tLoss: 0.002159\n","399it [00:17, 17.93it/s]Train epoch: 11 [batch #400, batch_size 16, seq length 743]\tLoss: 0.001834\n","425it [00:18, 17.41it/s]Train epoch: 11 [batch #425, batch_size 16, seq length 758]\tLoss: 0.001941\n","449it [00:20, 17.25it/s]Train epoch: 11 [batch #450, batch_size 16, seq length 773]\tLoss: 0.002124\n","475it [00:21, 16.66it/s]Train epoch: 11 [batch #475, batch_size 16, seq length 789]\tLoss: 0.001872\n","499it [00:23, 16.45it/s]Train epoch: 11 [batch #500, batch_size 16, seq length 803]\tLoss: 0.002104\n","525it [00:24, 16.30it/s]Train epoch: 11 [batch #525, batch_size 16, seq length 818]\tLoss: 0.002035\n","549it [00:26, 16.49it/s]Train epoch: 11 [batch #550, batch_size 16, seq length 833]\tLoss: 0.001908\n","575it [00:27, 15.73it/s]Train epoch: 11 [batch #575, batch_size 16, seq length 846]\tLoss: 0.001966\n","599it [00:29, 15.97it/s]Train epoch: 11 [batch #600, batch_size 16, seq length 860]\tLoss: 0.002211\n","625it [00:31, 15.46it/s]Train epoch: 11 [batch #625, batch_size 16, seq length 874]\tLoss: 0.001965\n","649it [00:32, 15.17it/s]Train epoch: 11 [batch #650, batch_size 16, seq length 887]\tLoss: 0.001922\n","675it [00:34, 14.86it/s]Train epoch: 11 [batch #675, batch_size 16, seq length 901]\tLoss: 0.001937\n","699it [00:36, 14.63it/s]Train epoch: 11 [batch #700, batch_size 16, seq length 915]\tLoss: 0.001805\n","725it [00:37, 14.77it/s]Train epoch: 11 [batch #725, batch_size 16, seq length 929]\tLoss: 0.002017\n","749it [00:39, 14.64it/s]Train epoch: 11 [batch #750, batch_size 16, seq length 942]\tLoss: 0.002107\n","775it [00:41, 14.39it/s]Train epoch: 11 [batch #775, batch_size 16, seq length 954]\tLoss: 0.002099\n","799it [00:42, 14.06it/s]Train epoch: 11 [batch #800, batch_size 16, seq length 968]\tLoss: 0.002176\n","825it [00:44, 14.02it/s]Train epoch: 11 [batch #825, batch_size 16, seq length 981]\tLoss: 0.002090\n","849it [00:46, 13.85it/s]Train epoch: 11 [batch #850, batch_size 16, seq length 994]\tLoss: 0.002207\n","875it [00:48, 13.52it/s]Train epoch: 11 [batch #875, batch_size 16, seq length 1006]\tLoss: 0.002000\n","899it [00:50, 13.36it/s]Train epoch: 11 [batch #900, batch_size 16, seq length 1020]\tLoss: 0.002290\n","925it [00:52, 13.27it/s]Train epoch: 11 [batch #925, batch_size 16, seq length 1033]\tLoss: 0.002030\n","949it [00:54, 13.14it/s]Train epoch: 11 [batch #950, batch_size 16, seq length 1046]\tLoss: 0.002240\n","975it [00:56, 13.11it/s]Train epoch: 11 [batch #975, batch_size 16, seq length 1059]\tLoss: 0.002216\n","999it [00:57, 12.82it/s]Train epoch: 11 [batch #1000, batch_size 16, seq length 1073]\tLoss: 0.002145\n","1025it [00:59, 12.65it/s]Train epoch: 11 [batch #1025, batch_size 16, seq length 1085]\tLoss: 0.002416\n","1049it [01:01, 12.39it/s]Train epoch: 11 [batch #1050, batch_size 16, seq length 1099]\tLoss: 0.002293\n","1075it [01:03, 12.35it/s]Train epoch: 11 [batch #1075, batch_size 16, seq length 1113]\tLoss: 0.002344\n","1099it [01:05, 12.39it/s]Train epoch: 11 [batch #1100, batch_size 16, seq length 1126]\tLoss: 0.002210\n","1125it [01:08, 12.37it/s]Train epoch: 11 [batch #1125, batch_size 16, seq length 1140]\tLoss: 0.002360\n","1149it [01:10, 12.38it/s]Train epoch: 11 [batch #1150, batch_size 16, seq length 1154]\tLoss: 0.002245\n","1175it [01:12, 12.22it/s]Train epoch: 11 [batch #1175, batch_size 16, seq length 1167]\tLoss: 0.002340\n","1199it [01:14, 11.87it/s]Train epoch: 11 [batch #1200, batch_size 16, seq length 1180]\tLoss: 0.002239\n","1225it [01:16, 11.78it/s]Train epoch: 11 [batch #1225, batch_size 16, seq length 1195]\tLoss: 0.002383\n","1249it [01:18, 11.69it/s]Train epoch: 11 [batch #1250, batch_size 16, seq length 1208]\tLoss: 0.002269\n","1275it [01:20, 11.64it/s]Train epoch: 11 [batch #1275, batch_size 16, seq length 1220]\tLoss: 0.002366\n","1299it [01:22, 11.38it/s]Train epoch: 11 [batch #1300, batch_size 16, seq length 1234]\tLoss: 0.002518\n","1325it [01:25, 11.13it/s]Train epoch: 11 [batch #1325, batch_size 16, seq length 1248]\tLoss: 0.002359\n","1349it [01:27, 11.25it/s]Train epoch: 11 [batch #1350, batch_size 16, seq length 1262]\tLoss: 0.002477\n","1375it [01:29, 11.00it/s]Train epoch: 11 [batch #1375, batch_size 16, seq length 1276]\tLoss: 0.002589\n","1399it [01:31, 10.83it/s]Train epoch: 11 [batch #1400, batch_size 16, seq length 1290]\tLoss: 0.002636\n","1425it [01:34, 10.71it/s]Train epoch: 11 [batch #1425, batch_size 16, seq length 1304]\tLoss: 0.002704\n","1449it [01:36, 10.55it/s]Train epoch: 11 [batch #1450, batch_size 16, seq length 1319]\tLoss: 0.002478\n","1475it [01:38, 10.37it/s]Train epoch: 11 [batch #1475, batch_size 16, seq length 1332]\tLoss: 0.002681\n","1499it [01:41, 10.34it/s]Train epoch: 11 [batch #1500, batch_size 16, seq length 1346]\tLoss: 0.002561\n","1525it [01:43, 10.14it/s]Train epoch: 11 [batch #1525, batch_size 16, seq length 1362]\tLoss: 0.002270\n","1549it [01:46, 10.07it/s]Train epoch: 11 [batch #1550, batch_size 16, seq length 1377]\tLoss: 0.002763\n","1575it [01:48,  9.88it/s]Train epoch: 11 [batch #1575, batch_size 16, seq length 1392]\tLoss: 0.002333\n","1600it [01:51,  9.88it/s]Train epoch: 11 [batch #1600, batch_size 16, seq length 1406]\tLoss: 0.002491\n","1624it [01:53,  9.88it/s]Train epoch: 11 [batch #1625, batch_size 16, seq length 1422]\tLoss: 0.002734\n","1650it [01:56,  9.85it/s]Train epoch: 11 [batch #1650, batch_size 16, seq length 1437]\tLoss: 0.002619\n","1674it [01:58,  9.86it/s]Train epoch: 11 [batch #1675, batch_size 16, seq length 1454]\tLoss: 0.002653\n","1700it [02:01,  9.80it/s]Train epoch: 11 [batch #1700, batch_size 16, seq length 1469]\tLoss: 0.002887\n","1725it [02:04,  9.47it/s]Train epoch: 11 [batch #1725, batch_size 16, seq length 1484]\tLoss: 0.002650\n","1750it [02:06,  9.44it/s]Train epoch: 11 [batch #1750, batch_size 16, seq length 1501]\tLoss: 0.002451\n","1775it [02:09,  9.18it/s]Train epoch: 11 [batch #1775, batch_size 16, seq length 1518]\tLoss: 0.002837\n","1800it [02:12,  9.08it/s]Train epoch: 11 [batch #1800, batch_size 16, seq length 1533]\tLoss: 0.002944\n","1825it [02:15,  8.97it/s]Train epoch: 11 [batch #1825, batch_size 16, seq length 1548]\tLoss: 0.002903\n","1850it [02:17,  8.89it/s]Train epoch: 11 [batch #1850, batch_size 16, seq length 1566]\tLoss: 0.002689\n","1875it [02:20,  9.00it/s]Train epoch: 11 [batch #1875, batch_size 16, seq length 1582]\tLoss: 0.002862\n","1900it [02:23,  8.81it/s]Train epoch: 11 [batch #1900, batch_size 16, seq length 1599]\tLoss: 0.002911\n","1925it [02:26,  8.67it/s]Train epoch: 11 [batch #1925, batch_size 16, seq length 1616]\tLoss: 0.002879\n","1950it [02:29,  8.49it/s]Train epoch: 11 [batch #1950, batch_size 16, seq length 1633]\tLoss: 0.002850\n","1975it [02:32,  8.29it/s]Train epoch: 11 [batch #1975, batch_size 16, seq length 1651]\tLoss: 0.002845\n","2000it [02:35,  8.33it/s]Train epoch: 11 [batch #2000, batch_size 16, seq length 1670]\tLoss: 0.002775\n","2025it [02:38,  7.91it/s]Train epoch: 11 [batch #2025, batch_size 16, seq length 1688]\tLoss: 0.002877\n","2050it [02:41,  8.24it/s]Train epoch: 11 [batch #2050, batch_size 16, seq length 1707]\tLoss: 0.002870\n","2075it [02:44,  8.13it/s]Train epoch: 11 [batch #2075, batch_size 16, seq length 1728]\tLoss: 0.003012\n","2100it [02:47,  8.13it/s]Train epoch: 11 [batch #2100, batch_size 16, seq length 1748]\tLoss: 0.002876\n","2125it [02:50,  7.91it/s]Train epoch: 11 [batch #2125, batch_size 16, seq length 1768]\tLoss: 0.003197\n","2150it [02:53,  7.89it/s]Train epoch: 11 [batch #2150, batch_size 16, seq length 1787]\tLoss: 0.002771\n","2175it [02:56,  7.77it/s]Train epoch: 11 [batch #2175, batch_size 16, seq length 1809]\tLoss: 0.003239\n","2200it [03:00,  7.69it/s]Train epoch: 11 [batch #2200, batch_size 16, seq length 1833]\tLoss: 0.002974\n","2225it [03:03,  7.59it/s]Train epoch: 11 [batch #2225, batch_size 16, seq length 1855]\tLoss: 0.002971\n","2250it [03:06,  7.25it/s]Train epoch: 11 [batch #2250, batch_size 16, seq length 1878]\tLoss: 0.003266\n","2275it [03:10,  7.40it/s]Train epoch: 11 [batch #2275, batch_size 16, seq length 1901]\tLoss: 0.003162\n","2300it [03:13,  7.23it/s]Train epoch: 11 [batch #2300, batch_size 16, seq length 1927]\tLoss: 0.003202\n","2325it [03:17,  7.28it/s]Train epoch: 11 [batch #2325, batch_size 16, seq length 1953]\tLoss: 0.003326\n","2350it [03:20,  7.05it/s]Train epoch: 11 [batch #2350, batch_size 16, seq length 1980]\tLoss: 0.003192\n","2375it [03:24,  6.88it/s]Train epoch: 11 [batch #2375, batch_size 16, seq length 2006]\tLoss: 0.003280\n","2400it [03:27,  6.87it/s]Train epoch: 11 [batch #2400, batch_size 16, seq length 2034]\tLoss: 0.003393\n","2425it [03:31,  6.82it/s]Train epoch: 11 [batch #2425, batch_size 16, seq length 2064]\tLoss: 0.003330\n","2450it [03:35,  6.63it/s]Train epoch: 11 [batch #2450, batch_size 16, seq length 2094]\tLoss: 0.003470\n","2475it [03:38,  6.68it/s]Train epoch: 11 [batch #2475, batch_size 16, seq length 2126]\tLoss: 0.003497\n","2500it [03:42,  6.53it/s]Train epoch: 11 [batch #2500, batch_size 16, seq length 2160]\tLoss: 0.003445\n","2525it [03:46,  6.40it/s]Train epoch: 11 [batch #2525, batch_size 16, seq length 2192]\tLoss: 0.003631\n","2550it [03:50,  6.31it/s]Train epoch: 11 [batch #2550, batch_size 16, seq length 2228]\tLoss: 0.003599\n","2575it [03:54,  6.19it/s]Train epoch: 11 [batch #2575, batch_size 16, seq length 2266]\tLoss: 0.003799\n","2600it [03:58,  6.17it/s]Train epoch: 11 [batch #2600, batch_size 16, seq length 2307]\tLoss: 0.003684\n","2625it [04:02,  5.81it/s]Train epoch: 11 [batch #2625, batch_size 16, seq length 2350]\tLoss: 0.003551\n","2650it [04:06,  5.77it/s]Train epoch: 11 [batch #2650, batch_size 16, seq length 2392]\tLoss: 0.003730\n","2675it [04:11,  5.74it/s]Train epoch: 11 [batch #2675, batch_size 16, seq length 2439]\tLoss: 0.003890\n","2700it [04:15,  5.66it/s]Train epoch: 11 [batch #2700, batch_size 16, seq length 2491]\tLoss: 0.003850\n","2725it [04:20,  5.66it/s]Train epoch: 11 [batch #2725, batch_size 16, seq length 2500]\tLoss: 0.003962\n","2750it [04:24,  5.64it/s]Train epoch: 11 [batch #2750, batch_size 16, seq length 2500]\tLoss: 0.003794\n","2775it [04:29,  5.67it/s]Train epoch: 11 [batch #2775, batch_size 16, seq length 2500]\tLoss: 0.004022\n","2800it [04:33,  5.62it/s]Train epoch: 11 [batch #2800, batch_size 16, seq length 2500]\tLoss: 0.003916\n","2825it [04:37,  5.65it/s]Train epoch: 11 [batch #2825, batch_size 16, seq length 2500]\tLoss: 0.003964\n","2850it [04:42,  5.51it/s]Train epoch: 11 [batch #2850, batch_size 16, seq length 2500]\tLoss: 0.004139\n","2875it [04:46,  5.58it/s]Train epoch: 11 [batch #2875, batch_size 16, seq length 2500]\tLoss: 0.004425\n","2900it [04:51,  5.62it/s]Train epoch: 11 [batch #2900, batch_size 16, seq length 2500]\tLoss: 0.004574\n","2925it [04:55,  5.61it/s]Train epoch: 11 [batch #2925, batch_size 16, seq length 2500]\tLoss: 0.004872\n","2950it [05:00,  5.60it/s]Train epoch: 11 [batch #2950, batch_size 16, seq length 2500]\tLoss: 0.004849\n","2975it [05:04,  5.37it/s]Train epoch: 11 [batch #2975, batch_size 16, seq length 2500]\tLoss: 0.005720\n","2983it [05:06,  9.74it/s]\n","epoch loss: 0.002734111155221876\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:45, 36.22it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0303, 0.0491, 0.0441, 0.0464, 0.8296\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2786, 0.4882, 0.3936, 0.4358, 0.9691\n","rec_at_8: 0.3236\n","prec_at_8: 0.5942\n","rec_at_15: 0.4415\n","prec_at_15: 0.4517\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/rnn_May_04_023326\n","\n","EPOCH 12\n","0it [00:00, ?it/s]Train epoch: 12 [batch #0, batch_size 16, seq length 117]\tLoss: 0.004238\n","24it [00:00, 37.20it/s]Train epoch: 12 [batch #25, batch_size 16, seq length 337]\tLoss: 0.002304\n","48it [00:01, 33.14it/s]Train epoch: 12 [batch #50, batch_size 16, seq length 402]\tLoss: 0.001842\n","72it [00:02, 29.97it/s]Train epoch: 12 [batch #75, batch_size 16, seq length 452]\tLoss: 0.001614\n","100it [00:03, 27.54it/s]Train epoch: 12 [batch #100, batch_size 16, seq length 490]\tLoss: 0.001545\n","124it [00:04, 25.78it/s]Train epoch: 12 [batch #125, batch_size 16, seq length 520]\tLoss: 0.001749\n","148it [00:05, 24.57it/s]Train epoch: 12 [batch #150, batch_size 16, seq length 548]\tLoss: 0.001839\n","175it [00:06, 23.35it/s]Train epoch: 12 [batch #175, batch_size 16, seq length 574]\tLoss: 0.001737\n","199it [00:07, 22.95it/s]Train epoch: 12 [batch #200, batch_size 16, seq length 596]\tLoss: 0.001784\n","223it [00:08, 21.91it/s]Train epoch: 12 [batch #225, batch_size 16, seq length 618]\tLoss: 0.001821\n","250it [00:09, 21.28it/s]Train epoch: 12 [batch #250, batch_size 16, seq length 638]\tLoss: 0.001732\n","274it [00:10, 20.16it/s]Train epoch: 12 [batch #275, batch_size 16, seq length 656]\tLoss: 0.001889\n","298it [00:12, 19.57it/s]Train epoch: 12 [batch #300, batch_size 16, seq length 674]\tLoss: 0.001851\n","325it [00:13, 18.80it/s]Train epoch: 12 [batch #325, batch_size 16, seq length 693]\tLoss: 0.001751\n","349it [00:14, 18.46it/s]Train epoch: 12 [batch #350, batch_size 16, seq length 710]\tLoss: 0.001895\n","375it [00:16, 18.69it/s]Train epoch: 12 [batch #375, batch_size 16, seq length 726]\tLoss: 0.002048\n","399it [00:17, 17.76it/s]Train epoch: 12 [batch #400, batch_size 16, seq length 743]\tLoss: 0.001752\n","425it [00:18, 17.83it/s]Train epoch: 12 [batch #425, batch_size 16, seq length 758]\tLoss: 0.001835\n","449it [00:20, 17.54it/s]Train epoch: 12 [batch #450, batch_size 16, seq length 773]\tLoss: 0.002010\n","475it [00:21, 16.61it/s]Train epoch: 12 [batch #475, batch_size 16, seq length 789]\tLoss: 0.001773\n","499it [00:23, 16.47it/s]Train epoch: 12 [batch #500, batch_size 16, seq length 803]\tLoss: 0.001969\n","525it [00:24, 16.43it/s]Train epoch: 12 [batch #525, batch_size 16, seq length 818]\tLoss: 0.001917\n","549it [00:26, 16.52it/s]Train epoch: 12 [batch #550, batch_size 16, seq length 833]\tLoss: 0.001801\n","575it [00:27, 16.28it/s]Train epoch: 12 [batch #575, batch_size 16, seq length 846]\tLoss: 0.001837\n","599it [00:29, 15.71it/s]Train epoch: 12 [batch #600, batch_size 16, seq length 860]\tLoss: 0.002040\n","625it [00:31, 15.53it/s]Train epoch: 12 [batch #625, batch_size 16, seq length 874]\tLoss: 0.001859\n","649it [00:32, 15.37it/s]Train epoch: 12 [batch #650, batch_size 16, seq length 887]\tLoss: 0.001785\n","675it [00:34, 15.01it/s]Train epoch: 12 [batch #675, batch_size 16, seq length 901]\tLoss: 0.001850\n","699it [00:36, 15.08it/s]Train epoch: 12 [batch #700, batch_size 16, seq length 915]\tLoss: 0.001703\n","725it [00:37, 14.66it/s]Train epoch: 12 [batch #725, batch_size 16, seq length 929]\tLoss: 0.001873\n","749it [00:39, 14.41it/s]Train epoch: 12 [batch #750, batch_size 16, seq length 942]\tLoss: 0.002054\n","775it [00:41, 14.24it/s]Train epoch: 12 [batch #775, batch_size 16, seq length 954]\tLoss: 0.002025\n","799it [00:42, 14.12it/s]Train epoch: 12 [batch #800, batch_size 16, seq length 968]\tLoss: 0.002101\n","825it [00:44, 13.47it/s]Train epoch: 12 [batch #825, batch_size 16, seq length 981]\tLoss: 0.002000\n","849it [00:46, 13.66it/s]Train epoch: 12 [batch #850, batch_size 16, seq length 994]\tLoss: 0.002089\n","875it [00:48, 13.73it/s]Train epoch: 12 [batch #875, batch_size 16, seq length 1006]\tLoss: 0.001941\n","899it [00:50, 13.45it/s]Train epoch: 12 [batch #900, batch_size 16, seq length 1020]\tLoss: 0.002155\n","925it [00:52, 13.27it/s]Train epoch: 12 [batch #925, batch_size 16, seq length 1033]\tLoss: 0.001930\n","949it [00:54, 13.23it/s]Train epoch: 12 [batch #950, batch_size 16, seq length 1046]\tLoss: 0.002125\n","975it [00:56, 13.16it/s]Train epoch: 12 [batch #975, batch_size 16, seq length 1059]\tLoss: 0.002072\n","999it [00:57, 13.04it/s]Train epoch: 12 [batch #1000, batch_size 16, seq length 1073]\tLoss: 0.002095\n","1025it [00:59, 12.54it/s]Train epoch: 12 [batch #1025, batch_size 16, seq length 1085]\tLoss: 0.002195\n","1049it [01:01, 12.44it/s]Train epoch: 12 [batch #1050, batch_size 16, seq length 1099]\tLoss: 0.002181\n","1075it [01:03, 12.37it/s]Train epoch: 12 [batch #1075, batch_size 16, seq length 1113]\tLoss: 0.002233\n","1099it [01:05, 12.27it/s]Train epoch: 12 [batch #1100, batch_size 16, seq length 1126]\tLoss: 0.002062\n","1125it [01:08, 12.35it/s]Train epoch: 12 [batch #1125, batch_size 16, seq length 1140]\tLoss: 0.002247\n","1149it [01:09, 12.24it/s]Train epoch: 12 [batch #1150, batch_size 16, seq length 1154]\tLoss: 0.002147\n","1175it [01:12, 11.98it/s]Train epoch: 12 [batch #1175, batch_size 16, seq length 1167]\tLoss: 0.002250\n","1199it [01:14, 11.98it/s]Train epoch: 12 [batch #1200, batch_size 16, seq length 1180]\tLoss: 0.002156\n","1225it [01:16, 11.80it/s]Train epoch: 12 [batch #1225, batch_size 16, seq length 1195]\tLoss: 0.002232\n","1249it [01:18, 12.01it/s]Train epoch: 12 [batch #1250, batch_size 16, seq length 1208]\tLoss: 0.002138\n","1275it [01:20, 11.63it/s]Train epoch: 12 [batch #1275, batch_size 16, seq length 1220]\tLoss: 0.002217\n","1299it [01:22, 11.39it/s]Train epoch: 12 [batch #1300, batch_size 16, seq length 1234]\tLoss: 0.002347\n","1325it [01:24, 11.24it/s]Train epoch: 12 [batch #1325, batch_size 16, seq length 1248]\tLoss: 0.002240\n","1349it [01:27, 11.24it/s]Train epoch: 12 [batch #1350, batch_size 16, seq length 1262]\tLoss: 0.002355\n","1375it [01:29, 10.88it/s]Train epoch: 12 [batch #1375, batch_size 16, seq length 1276]\tLoss: 0.002499\n","1399it [01:31, 10.75it/s]Train epoch: 12 [batch #1400, batch_size 16, seq length 1290]\tLoss: 0.002466\n","1425it [01:34, 10.74it/s]Train epoch: 12 [batch #1425, batch_size 16, seq length 1304]\tLoss: 0.002596\n","1449it [01:36, 10.53it/s]Train epoch: 12 [batch #1450, batch_size 16, seq length 1319]\tLoss: 0.002317\n","1475it [01:38, 10.52it/s]Train epoch: 12 [batch #1475, batch_size 16, seq length 1332]\tLoss: 0.002548\n","1499it [01:41, 10.28it/s]Train epoch: 12 [batch #1500, batch_size 16, seq length 1346]\tLoss: 0.002439\n","1525it [01:43, 10.33it/s]Train epoch: 12 [batch #1525, batch_size 16, seq length 1362]\tLoss: 0.002154\n","1549it [01:46, 10.11it/s]Train epoch: 12 [batch #1550, batch_size 16, seq length 1377]\tLoss: 0.002619\n","1574it [01:48,  9.93it/s]Train epoch: 12 [batch #1575, batch_size 16, seq length 1392]\tLoss: 0.002270\n","1600it [01:51,  9.89it/s]Train epoch: 12 [batch #1600, batch_size 16, seq length 1406]\tLoss: 0.002411\n","1625it [01:53,  9.93it/s]Train epoch: 12 [batch #1625, batch_size 16, seq length 1422]\tLoss: 0.002652\n","1650it [01:56,  9.97it/s]Train epoch: 12 [batch #1650, batch_size 16, seq length 1437]\tLoss: 0.002557\n","1675it [01:58,  9.64it/s]Train epoch: 12 [batch #1675, batch_size 16, seq length 1454]\tLoss: 0.002526\n","1700it [02:01,  9.72it/s]Train epoch: 12 [batch #1700, batch_size 16, seq length 1469]\tLoss: 0.002724\n","1725it [02:03,  9.25it/s]Train epoch: 12 [batch #1725, batch_size 16, seq length 1484]\tLoss: 0.002531\n","1750it [02:06,  9.36it/s]Train epoch: 12 [batch #1750, batch_size 16, seq length 1501]\tLoss: 0.002320\n","1775it [02:09,  9.19it/s]Train epoch: 12 [batch #1775, batch_size 16, seq length 1518]\tLoss: 0.002716\n","1800it [02:12,  9.16it/s]Train epoch: 12 [batch #1800, batch_size 16, seq length 1533]\tLoss: 0.002820\n","1825it [02:14,  8.83it/s]Train epoch: 12 [batch #1825, batch_size 16, seq length 1548]\tLoss: 0.002806\n","1850it [02:17,  8.99it/s]Train epoch: 12 [batch #1850, batch_size 16, seq length 1566]\tLoss: 0.002511\n","1875it [02:20,  8.91it/s]Train epoch: 12 [batch #1875, batch_size 16, seq length 1582]\tLoss: 0.002749\n","1900it [02:23,  8.63it/s]Train epoch: 12 [batch #1900, batch_size 16, seq length 1599]\tLoss: 0.002765\n","1925it [02:26,  8.83it/s]Train epoch: 12 [batch #1925, batch_size 16, seq length 1616]\tLoss: 0.002723\n","1950it [02:28,  8.54it/s]Train epoch: 12 [batch #1950, batch_size 16, seq length 1633]\tLoss: 0.002667\n","1975it [02:31,  8.59it/s]Train epoch: 12 [batch #1975, batch_size 16, seq length 1651]\tLoss: 0.002688\n","2000it [02:34,  8.49it/s]Train epoch: 12 [batch #2000, batch_size 16, seq length 1670]\tLoss: 0.002577\n","2025it [02:37,  8.14it/s]Train epoch: 12 [batch #2025, batch_size 16, seq length 1688]\tLoss: 0.002750\n","2050it [02:40,  8.24it/s]Train epoch: 12 [batch #2050, batch_size 16, seq length 1707]\tLoss: 0.002776\n","2075it [02:43,  8.18it/s]Train epoch: 12 [batch #2075, batch_size 16, seq length 1728]\tLoss: 0.002931\n","2100it [02:47,  8.11it/s]Train epoch: 12 [batch #2100, batch_size 16, seq length 1748]\tLoss: 0.002747\n","2125it [02:50,  8.05it/s]Train epoch: 12 [batch #2125, batch_size 16, seq length 1768]\tLoss: 0.003074\n","2150it [02:53,  7.92it/s]Train epoch: 12 [batch #2150, batch_size 16, seq length 1787]\tLoss: 0.002621\n","2175it [02:56,  7.72it/s]Train epoch: 12 [batch #2175, batch_size 16, seq length 1809]\tLoss: 0.003089\n","2200it [02:59,  7.65it/s]Train epoch: 12 [batch #2200, batch_size 16, seq length 1833]\tLoss: 0.002866\n","2225it [03:03,  7.65it/s]Train epoch: 12 [batch #2225, batch_size 16, seq length 1855]\tLoss: 0.002890\n","2250it [03:06,  7.46it/s]Train epoch: 12 [batch #2250, batch_size 16, seq length 1878]\tLoss: 0.003209\n","2275it [03:09,  7.39it/s]Train epoch: 12 [batch #2275, batch_size 16, seq length 1901]\tLoss: 0.002957\n","2300it [03:13,  7.24it/s]Train epoch: 12 [batch #2300, batch_size 16, seq length 1927]\tLoss: 0.002984\n","2325it [03:16,  7.22it/s]Train epoch: 12 [batch #2325, batch_size 16, seq length 1953]\tLoss: 0.003109\n","2350it [03:20,  7.10it/s]Train epoch: 12 [batch #2350, batch_size 16, seq length 1980]\tLoss: 0.003024\n","2375it [03:23,  6.95it/s]Train epoch: 12 [batch #2375, batch_size 16, seq length 2006]\tLoss: 0.003060\n","2400it [03:27,  6.73it/s]Train epoch: 12 [batch #2400, batch_size 16, seq length 2034]\tLoss: 0.003195\n","2425it [03:31,  6.84it/s]Train epoch: 12 [batch #2425, batch_size 16, seq length 2064]\tLoss: 0.003193\n","2450it [03:34,  6.60it/s]Train epoch: 12 [batch #2450, batch_size 16, seq length 2094]\tLoss: 0.003317\n","2475it [03:38,  6.66it/s]Train epoch: 12 [batch #2475, batch_size 16, seq length 2126]\tLoss: 0.003365\n","2500it [03:42,  6.47it/s]Train epoch: 12 [batch #2500, batch_size 16, seq length 2160]\tLoss: 0.003277\n","2525it [03:46,  6.43it/s]Train epoch: 12 [batch #2525, batch_size 16, seq length 2192]\tLoss: 0.003430\n","2550it [03:50,  6.26it/s]Train epoch: 12 [batch #2550, batch_size 16, seq length 2228]\tLoss: 0.003389\n","2575it [03:54,  6.24it/s]Train epoch: 12 [batch #2575, batch_size 16, seq length 2266]\tLoss: 0.003649\n","2600it [03:58,  6.15it/s]Train epoch: 12 [batch #2600, batch_size 16, seq length 2307]\tLoss: 0.003552\n","2625it [04:02,  6.02it/s]Train epoch: 12 [batch #2625, batch_size 16, seq length 2350]\tLoss: 0.003494\n","2650it [04:06,  5.87it/s]Train epoch: 12 [batch #2650, batch_size 16, seq length 2392]\tLoss: 0.003502\n","2675it [04:10,  5.71it/s]Train epoch: 12 [batch #2675, batch_size 16, seq length 2439]\tLoss: 0.003751\n","2700it [04:15,  5.50it/s]Train epoch: 12 [batch #2700, batch_size 16, seq length 2491]\tLoss: 0.003670\n","2725it [04:19,  5.67it/s]Train epoch: 12 [batch #2725, batch_size 16, seq length 2500]\tLoss: 0.003791\n","2750it [04:24,  5.53it/s]Train epoch: 12 [batch #2750, batch_size 16, seq length 2500]\tLoss: 0.003598\n","2775it [04:28,  5.51it/s]Train epoch: 12 [batch #2775, batch_size 16, seq length 2500]\tLoss: 0.003890\n","2800it [04:33,  5.65it/s]Train epoch: 12 [batch #2800, batch_size 16, seq length 2500]\tLoss: 0.003744\n","2825it [04:37,  5.68it/s]Train epoch: 12 [batch #2825, batch_size 16, seq length 2500]\tLoss: 0.003839\n","2850it [04:42,  5.62it/s]Train epoch: 12 [batch #2850, batch_size 16, seq length 2500]\tLoss: 0.003923\n","2875it [04:46,  5.54it/s]Train epoch: 12 [batch #2875, batch_size 16, seq length 2500]\tLoss: 0.004267\n","2900it [04:51,  5.55it/s]Train epoch: 12 [batch #2900, batch_size 16, seq length 2500]\tLoss: 0.004330\n","2925it [04:55,  5.56it/s]Train epoch: 12 [batch #2925, batch_size 16, seq length 2500]\tLoss: 0.004704\n","2950it [05:00,  5.48it/s]Train epoch: 12 [batch #2950, batch_size 16, seq length 2500]\tLoss: 0.004622\n","2975it [05:04,  5.37it/s]Train epoch: 12 [batch #2975, batch_size 16, seq length 2500]\tLoss: 0.005485\n","2983it [05:06,  9.74it/s]\n","epoch loss: 0.002605072915034373\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:45, 36.04it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0291, 0.0490, 0.0415, 0.0449, 0.8276\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2772, 0.4895, 0.3899, 0.4341, 0.9677\n","rec_at_8: 0.3236\n","prec_at_8: 0.5977\n","rec_at_15: 0.4383\n","prec_at_15: 0.4506\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/rnn_May_04_023326\n","\n","EPOCH 13\n","0it [00:00, ?it/s]Train epoch: 13 [batch #0, batch_size 16, seq length 117]\tLoss: 0.004233\n","24it [00:00, 37.11it/s]Train epoch: 13 [batch #25, batch_size 16, seq length 337]\tLoss: 0.002244\n","48it [00:01, 32.50it/s]Train epoch: 13 [batch #50, batch_size 16, seq length 402]\tLoss: 0.001804\n","75it [00:02, 29.63it/s]Train epoch: 13 [batch #75, batch_size 16, seq length 452]\tLoss: 0.001592\n","99it [00:03, 26.88it/s]Train epoch: 13 [batch #100, batch_size 16, seq length 490]\tLoss: 0.001483\n","123it [00:04, 25.62it/s]Train epoch: 13 [batch #125, batch_size 16, seq length 520]\tLoss: 0.001668\n","150it [00:05, 24.86it/s]Train epoch: 13 [batch #150, batch_size 16, seq length 548]\tLoss: 0.001739\n","174it [00:06, 23.50it/s]Train epoch: 13 [batch #175, batch_size 16, seq length 574]\tLoss: 0.001618\n","198it [00:07, 22.80it/s]Train epoch: 13 [batch #200, batch_size 16, seq length 596]\tLoss: 0.001748\n","225it [00:08, 21.94it/s]Train epoch: 13 [batch #225, batch_size 16, seq length 618]\tLoss: 0.001738\n","249it [00:09, 20.83it/s]Train epoch: 13 [batch #250, batch_size 16, seq length 638]\tLoss: 0.001651\n","273it [00:10, 20.52it/s]Train epoch: 13 [batch #275, batch_size 16, seq length 656]\tLoss: 0.001838\n","299it [00:12, 19.90it/s]Train epoch: 13 [batch #300, batch_size 16, seq length 674]\tLoss: 0.001742\n","324it [00:13, 19.49it/s]Train epoch: 13 [batch #325, batch_size 16, seq length 693]\tLoss: 0.001660\n","350it [00:14, 19.16it/s]Train epoch: 13 [batch #350, batch_size 16, seq length 710]\tLoss: 0.001830\n","374it [00:15, 18.75it/s]Train epoch: 13 [batch #375, batch_size 16, seq length 726]\tLoss: 0.001925\n","400it [00:17, 18.24it/s]Train epoch: 13 [batch #400, batch_size 16, seq length 743]\tLoss: 0.001685\n","424it [00:18, 17.50it/s]Train epoch: 13 [batch #425, batch_size 16, seq length 758]\tLoss: 0.001727\n","450it [00:20, 16.76it/s]Train epoch: 13 [batch #450, batch_size 16, seq length 773]\tLoss: 0.001933\n","474it [00:21, 16.46it/s]Train epoch: 13 [batch #475, batch_size 16, seq length 789]\tLoss: 0.001725\n","500it [00:23, 16.51it/s]Train epoch: 13 [batch #500, batch_size 16, seq length 803]\tLoss: 0.001898\n","524it [00:24, 16.33it/s]Train epoch: 13 [batch #525, batch_size 16, seq length 818]\tLoss: 0.001913\n","550it [00:26, 16.48it/s]Train epoch: 13 [batch #550, batch_size 16, seq length 833]\tLoss: 0.001730\n","574it [00:27, 16.46it/s]Train epoch: 13 [batch #575, batch_size 16, seq length 846]\tLoss: 0.001756\n","600it [00:29, 15.99it/s]Train epoch: 13 [batch #600, batch_size 16, seq length 860]\tLoss: 0.002000\n","624it [00:30, 15.38it/s]Train epoch: 13 [batch #625, batch_size 16, seq length 874]\tLoss: 0.001752\n","650it [00:32, 15.17it/s]Train epoch: 13 [batch #650, batch_size 16, seq length 887]\tLoss: 0.001708\n","674it [00:34, 15.06it/s]Train epoch: 13 [batch #675, batch_size 16, seq length 901]\tLoss: 0.001770\n","700it [00:36, 14.93it/s]Train epoch: 13 [batch #700, batch_size 16, seq length 915]\tLoss: 0.001599\n","724it [00:37, 14.66it/s]Train epoch: 13 [batch #725, batch_size 16, seq length 929]\tLoss: 0.001812\n","750it [00:39, 14.57it/s]Train epoch: 13 [batch #750, batch_size 16, seq length 942]\tLoss: 0.001938\n","774it [00:41, 14.29it/s]Train epoch: 13 [batch #775, batch_size 16, seq length 954]\tLoss: 0.001868\n","800it [00:42, 14.12it/s]Train epoch: 13 [batch #800, batch_size 16, seq length 968]\tLoss: 0.002026\n","824it [00:44, 13.86it/s]Train epoch: 13 [batch #825, batch_size 16, seq length 981]\tLoss: 0.001909\n","850it [00:46, 13.76it/s]Train epoch: 13 [batch #850, batch_size 16, seq length 994]\tLoss: 0.001940\n","874it [00:48, 13.44it/s]Train epoch: 13 [batch #875, batch_size 16, seq length 1006]\tLoss: 0.001839\n","900it [00:50, 13.27it/s]Train epoch: 13 [batch #900, batch_size 16, seq length 1020]\tLoss: 0.002044\n","924it [00:52, 13.33it/s]Train epoch: 13 [batch #925, batch_size 16, seq length 1033]\tLoss: 0.001838\n","950it [00:54, 13.20it/s]Train epoch: 13 [batch #950, batch_size 16, seq length 1046]\tLoss: 0.001996\n","974it [00:55, 13.08it/s]Train epoch: 13 [batch #975, batch_size 16, seq length 1059]\tLoss: 0.002003\n","1000it [00:57, 12.70it/s]Train epoch: 13 [batch #1000, batch_size 16, seq length 1073]\tLoss: 0.001976\n","1024it [00:59, 12.51it/s]Train epoch: 13 [batch #1025, batch_size 16, seq length 1085]\tLoss: 0.002111\n","1050it [01:01, 12.38it/s]Train epoch: 13 [batch #1050, batch_size 16, seq length 1099]\tLoss: 0.002085\n","1074it [01:03, 12.22it/s]Train epoch: 13 [batch #1075, batch_size 16, seq length 1113]\tLoss: 0.002081\n","1100it [01:05, 12.32it/s]Train epoch: 13 [batch #1100, batch_size 16, seq length 1126]\tLoss: 0.001997\n","1124it [01:07, 12.22it/s]Train epoch: 13 [batch #1125, batch_size 16, seq length 1140]\tLoss: 0.002192\n","1150it [01:09, 12.36it/s]Train epoch: 13 [batch #1150, batch_size 16, seq length 1154]\tLoss: 0.001999\n","1174it [01:11, 12.01it/s]Train epoch: 13 [batch #1175, batch_size 16, seq length 1167]\tLoss: 0.002127\n","1200it [01:14, 11.86it/s]Train epoch: 13 [batch #1200, batch_size 16, seq length 1180]\tLoss: 0.002040\n","1224it [01:16, 12.16it/s]Train epoch: 13 [batch #1225, batch_size 16, seq length 1195]\tLoss: 0.002173\n","1250it [01:18, 11.80it/s]Train epoch: 13 [batch #1250, batch_size 16, seq length 1208]\tLoss: 0.002023\n","1274it [01:20, 11.58it/s]Train epoch: 13 [batch #1275, batch_size 16, seq length 1220]\tLoss: 0.002106\n","1300it [01:22, 11.40it/s]Train epoch: 13 [batch #1300, batch_size 16, seq length 1234]\tLoss: 0.002277\n","1324it [01:24, 11.11it/s]Train epoch: 13 [batch #1325, batch_size 16, seq length 1248]\tLoss: 0.002189\n","1350it [01:27, 10.99it/s]Train epoch: 13 [batch #1350, batch_size 16, seq length 1262]\tLoss: 0.002234\n","1374it [01:29, 10.97it/s]Train epoch: 13 [batch #1375, batch_size 16, seq length 1276]\tLoss: 0.002384\n","1400it [01:31, 10.94it/s]Train epoch: 13 [batch #1400, batch_size 16, seq length 1290]\tLoss: 0.002346\n","1424it [01:33, 10.81it/s]Train epoch: 13 [batch #1425, batch_size 16, seq length 1304]\tLoss: 0.002510\n","1450it [01:36, 10.75it/s]Train epoch: 13 [batch #1450, batch_size 16, seq length 1319]\tLoss: 0.002204\n","1474it [01:38, 10.35it/s]Train epoch: 13 [batch #1475, batch_size 16, seq length 1332]\tLoss: 0.002464\n","1500it [01:41, 10.34it/s]Train epoch: 13 [batch #1500, batch_size 16, seq length 1346]\tLoss: 0.002273\n","1524it [01:43, 10.31it/s]Train epoch: 13 [batch #1525, batch_size 16, seq length 1362]\tLoss: 0.002107\n","1550it [01:46,  9.95it/s]Train epoch: 13 [batch #1550, batch_size 16, seq length 1377]\tLoss: 0.002542\n","1575it [01:48,  9.93it/s]Train epoch: 13 [batch #1575, batch_size 16, seq length 1392]\tLoss: 0.002087\n","1599it [01:51,  9.90it/s]Train epoch: 13 [batch #1600, batch_size 16, seq length 1406]\tLoss: 0.002231\n","1625it [01:53,  9.67it/s]Train epoch: 13 [batch #1625, batch_size 16, seq length 1422]\tLoss: 0.002516\n","1650it [01:56,  9.83it/s]Train epoch: 13 [batch #1650, batch_size 16, seq length 1437]\tLoss: 0.002405\n","1675it [01:58,  9.79it/s]Train epoch: 13 [batch #1675, batch_size 16, seq length 1454]\tLoss: 0.002404\n","1700it [02:01,  9.26it/s]Train epoch: 13 [batch #1700, batch_size 16, seq length 1469]\tLoss: 0.002586\n","1725it [02:04,  9.53it/s]Train epoch: 13 [batch #1725, batch_size 16, seq length 1484]\tLoss: 0.002383\n","1750it [02:06,  9.60it/s]Train epoch: 13 [batch #1750, batch_size 16, seq length 1501]\tLoss: 0.002219\n","1775it [02:09,  9.27it/s]Train epoch: 13 [batch #1775, batch_size 16, seq length 1518]\tLoss: 0.002519\n","1800it [02:12,  8.98it/s]Train epoch: 13 [batch #1800, batch_size 16, seq length 1533]\tLoss: 0.002633\n","1825it [02:14,  9.08it/s]Train epoch: 13 [batch #1825, batch_size 16, seq length 1548]\tLoss: 0.002653\n","1850it [02:17,  8.92it/s]Train epoch: 13 [batch #1850, batch_size 16, seq length 1566]\tLoss: 0.002413\n","1875it [02:20,  8.84it/s]Train epoch: 13 [batch #1875, batch_size 16, seq length 1582]\tLoss: 0.002627\n","1900it [02:23,  8.82it/s]Train epoch: 13 [batch #1900, batch_size 16, seq length 1599]\tLoss: 0.002690\n","1925it [02:26,  8.52it/s]Train epoch: 13 [batch #1925, batch_size 16, seq length 1616]\tLoss: 0.002504\n","1950it [02:29,  8.55it/s]Train epoch: 13 [batch #1950, batch_size 16, seq length 1633]\tLoss: 0.002565\n","1975it [02:32,  8.60it/s]Train epoch: 13 [batch #1975, batch_size 16, seq length 1651]\tLoss: 0.002594\n","2000it [02:35,  8.32it/s]Train epoch: 13 [batch #2000, batch_size 16, seq length 1670]\tLoss: 0.002502\n","2025it [02:38,  8.26it/s]Train epoch: 13 [batch #2025, batch_size 16, seq length 1688]\tLoss: 0.002600\n","2050it [02:41,  8.27it/s]Train epoch: 13 [batch #2050, batch_size 16, seq length 1707]\tLoss: 0.002580\n","2075it [02:44,  8.27it/s]Train epoch: 13 [batch #2075, batch_size 16, seq length 1728]\tLoss: 0.002757\n","2100it [02:47,  8.06it/s]Train epoch: 13 [batch #2100, batch_size 16, seq length 1748]\tLoss: 0.002592\n","2125it [02:50,  7.99it/s]Train epoch: 13 [batch #2125, batch_size 16, seq length 1768]\tLoss: 0.002902\n","2150it [02:53,  7.94it/s]Train epoch: 13 [batch #2150, batch_size 16, seq length 1787]\tLoss: 0.002522\n","2175it [02:56,  7.82it/s]Train epoch: 13 [batch #2175, batch_size 16, seq length 1809]\tLoss: 0.002949\n","2200it [02:59,  7.62it/s]Train epoch: 13 [batch #2200, batch_size 16, seq length 1833]\tLoss: 0.002659\n","2225it [03:03,  7.65it/s]Train epoch: 13 [batch #2225, batch_size 16, seq length 1855]\tLoss: 0.002742\n","2250it [03:06,  7.43it/s]Train epoch: 13 [batch #2250, batch_size 16, seq length 1878]\tLoss: 0.002991\n","2275it [03:09,  7.29it/s]Train epoch: 13 [batch #2275, batch_size 16, seq length 1901]\tLoss: 0.002834\n","2300it [03:13,  7.28it/s]Train epoch: 13 [batch #2300, batch_size 16, seq length 1927]\tLoss: 0.002904\n","2325it [03:16,  7.15it/s]Train epoch: 13 [batch #2325, batch_size 16, seq length 1953]\tLoss: 0.003010\n","2350it [03:20,  6.98it/s]Train epoch: 13 [batch #2350, batch_size 16, seq length 1980]\tLoss: 0.002837\n","2375it [03:23,  7.02it/s]Train epoch: 13 [batch #2375, batch_size 16, seq length 2006]\tLoss: 0.002935\n","2400it [03:27,  6.88it/s]Train epoch: 13 [batch #2400, batch_size 16, seq length 2034]\tLoss: 0.003077\n","2425it [03:31,  6.67it/s]Train epoch: 13 [batch #2425, batch_size 16, seq length 2064]\tLoss: 0.002991\n","2450it [03:34,  6.76it/s]Train epoch: 13 [batch #2450, batch_size 16, seq length 2094]\tLoss: 0.003110\n","2475it [03:38,  6.62it/s]Train epoch: 13 [batch #2475, batch_size 16, seq length 2126]\tLoss: 0.003230\n","2500it [03:42,  6.52it/s]Train epoch: 13 [batch #2500, batch_size 16, seq length 2160]\tLoss: 0.003113\n","2525it [03:46,  6.44it/s]Train epoch: 13 [batch #2525, batch_size 16, seq length 2192]\tLoss: 0.003315\n","2550it [03:50,  6.32it/s]Train epoch: 13 [batch #2550, batch_size 16, seq length 2228]\tLoss: 0.003255\n","2575it [03:54,  6.10it/s]Train epoch: 13 [batch #2575, batch_size 16, seq length 2266]\tLoss: 0.003562\n","2600it [03:58,  6.14it/s]Train epoch: 13 [batch #2600, batch_size 16, seq length 2307]\tLoss: 0.003358\n","2625it [04:02,  6.06it/s]Train epoch: 13 [batch #2625, batch_size 16, seq length 2350]\tLoss: 0.003286\n","2650it [04:06,  5.93it/s]Train epoch: 13 [batch #2650, batch_size 16, seq length 2392]\tLoss: 0.003384\n","2675it [04:10,  5.80it/s]Train epoch: 13 [batch #2675, batch_size 16, seq length 2439]\tLoss: 0.003559\n","2700it [04:15,  5.71it/s]Train epoch: 13 [batch #2700, batch_size 16, seq length 2491]\tLoss: 0.003546\n","2725it [04:19,  5.60it/s]Train epoch: 13 [batch #2725, batch_size 16, seq length 2500]\tLoss: 0.003650\n","2750it [04:24,  5.52it/s]Train epoch: 13 [batch #2750, batch_size 16, seq length 2500]\tLoss: 0.003539\n","2775it [04:28,  5.66it/s]Train epoch: 13 [batch #2775, batch_size 16, seq length 2500]\tLoss: 0.003741\n","2800it [04:33,  5.62it/s]Train epoch: 13 [batch #2800, batch_size 16, seq length 2500]\tLoss: 0.003662\n","2825it [04:37,  5.57it/s]Train epoch: 13 [batch #2825, batch_size 16, seq length 2500]\tLoss: 0.003657\n","2850it [04:42,  5.65it/s]Train epoch: 13 [batch #2850, batch_size 16, seq length 2500]\tLoss: 0.003818\n","2875it [04:46,  5.63it/s]Train epoch: 13 [batch #2875, batch_size 16, seq length 2500]\tLoss: 0.004067\n","2900it [04:50,  5.56it/s]Train epoch: 13 [batch #2900, batch_size 16, seq length 2500]\tLoss: 0.004239\n","2925it [04:55,  5.47it/s]Train epoch: 13 [batch #2925, batch_size 16, seq length 2500]\tLoss: 0.004496\n","2950it [05:00,  5.51it/s]Train epoch: 13 [batch #2950, batch_size 16, seq length 2500]\tLoss: 0.004523\n","2975it [05:04,  5.44it/s]Train epoch: 13 [batch #2975, batch_size 16, seq length 2500]\tLoss: 0.005306\n","2983it [05:06,  9.74it/s]\n","epoch loss: 0.0024940433977231097\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:45, 36.16it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0311, 0.0470, 0.0478, 0.0474, 0.8280\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2668, 0.4233, 0.4193, 0.4213, 0.9668\n","rec_at_8: 0.3149\n","prec_at_8: 0.5802\n","rec_at_15: 0.4305\n","prec_at_15: 0.4396\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/rnn_May_04_023326\n","\n","EPOCH 14\n","0it [00:00, ?it/s]Train epoch: 14 [batch #0, batch_size 16, seq length 117]\tLoss: 0.004199\n","23it [00:00, 36.53it/s]Train epoch: 14 [batch #25, batch_size 16, seq length 337]\tLoss: 0.002135\n","47it [00:01, 33.20it/s]Train epoch: 14 [batch #50, batch_size 16, seq length 402]\tLoss: 0.001753\n","75it [00:02, 29.91it/s]Train epoch: 14 [batch #75, batch_size 16, seq length 452]\tLoss: 0.001487\n","99it [00:03, 27.05it/s]Train epoch: 14 [batch #100, batch_size 16, seq length 490]\tLoss: 0.001414\n","123it [00:04, 25.79it/s]Train epoch: 14 [batch #125, batch_size 16, seq length 520]\tLoss: 0.001607\n","150it [00:05, 24.73it/s]Train epoch: 14 [batch #150, batch_size 16, seq length 548]\tLoss: 0.001655\n","174it [00:06, 23.20it/s]Train epoch: 14 [batch #175, batch_size 16, seq length 574]\tLoss: 0.001593\n","198it [00:07, 22.73it/s]Train epoch: 14 [batch #200, batch_size 16, seq length 596]\tLoss: 0.001637\n","225it [00:08, 21.55it/s]Train epoch: 14 [batch #225, batch_size 16, seq length 618]\tLoss: 0.001654\n","249it [00:09, 21.32it/s]Train epoch: 14 [batch #250, batch_size 16, seq length 638]\tLoss: 0.001585\n","273it [00:10, 20.69it/s]Train epoch: 14 [batch #275, batch_size 16, seq length 656]\tLoss: 0.001772\n","299it [00:12, 19.75it/s]Train epoch: 14 [batch #300, batch_size 16, seq length 674]\tLoss: 0.001707\n","325it [00:13, 18.82it/s]Train epoch: 14 [batch #325, batch_size 16, seq length 693]\tLoss: 0.001582\n","349it [00:14, 18.88it/s]Train epoch: 14 [batch #350, batch_size 16, seq length 710]\tLoss: 0.001762\n","375it [00:16, 17.90it/s]Train epoch: 14 [batch #375, batch_size 16, seq length 726]\tLoss: 0.001830\n","399it [00:17, 17.36it/s]Train epoch: 14 [batch #400, batch_size 16, seq length 743]\tLoss: 0.001618\n","425it [00:18, 17.54it/s]Train epoch: 14 [batch #425, batch_size 16, seq length 758]\tLoss: 0.001726\n","449it [00:20, 17.34it/s]Train epoch: 14 [batch #450, batch_size 16, seq length 773]\tLoss: 0.001876\n","475it [00:21, 16.52it/s]Train epoch: 14 [batch #475, batch_size 16, seq length 789]\tLoss: 0.001628\n","499it [00:23, 16.35it/s]Train epoch: 14 [batch #500, batch_size 16, seq length 803]\tLoss: 0.001797\n","525it [00:24, 16.17it/s]Train epoch: 14 [batch #525, batch_size 16, seq length 818]\tLoss: 0.001775\n","549it [00:26, 16.43it/s]Train epoch: 14 [batch #550, batch_size 16, seq length 833]\tLoss: 0.001669\n","575it [00:28, 16.05it/s]Train epoch: 14 [batch #575, batch_size 16, seq length 846]\tLoss: 0.001714\n","599it [00:29, 15.87it/s]Train epoch: 14 [batch #600, batch_size 16, seq length 860]\tLoss: 0.001847\n","625it [00:31, 15.64it/s]Train epoch: 14 [batch #625, batch_size 16, seq length 874]\tLoss: 0.001688\n","649it [00:32, 15.47it/s]Train epoch: 14 [batch #650, batch_size 16, seq length 887]\tLoss: 0.001674\n","675it [00:34, 15.22it/s]Train epoch: 14 [batch #675, batch_size 16, seq length 901]\tLoss: 0.001691\n","699it [00:36, 14.93it/s]Train epoch: 14 [batch #700, batch_size 16, seq length 915]\tLoss: 0.001527\n","725it [00:37, 14.43it/s]Train epoch: 14 [batch #725, batch_size 16, seq length 929]\tLoss: 0.001774\n","749it [00:39, 14.32it/s]Train epoch: 14 [batch #750, batch_size 16, seq length 942]\tLoss: 0.001886\n","775it [00:41, 14.28it/s]Train epoch: 14 [batch #775, batch_size 16, seq length 954]\tLoss: 0.001804\n","799it [00:43, 13.94it/s]Train epoch: 14 [batch #800, batch_size 16, seq length 968]\tLoss: 0.001886\n","825it [00:44, 13.94it/s]Train epoch: 14 [batch #825, batch_size 16, seq length 981]\tLoss: 0.001828\n","849it [00:46, 13.96it/s]Train epoch: 14 [batch #850, batch_size 16, seq length 994]\tLoss: 0.001881\n","875it [00:48, 13.56it/s]Train epoch: 14 [batch #875, batch_size 16, seq length 1006]\tLoss: 0.001801\n","899it [00:50, 13.54it/s]Train epoch: 14 [batch #900, batch_size 16, seq length 1020]\tLoss: 0.001967\n","925it [00:52, 13.13it/s]Train epoch: 14 [batch #925, batch_size 16, seq length 1033]\tLoss: 0.001742\n","949it [00:54, 13.03it/s]Train epoch: 14 [batch #950, batch_size 16, seq length 1046]\tLoss: 0.001909\n","975it [00:56, 13.04it/s]Train epoch: 14 [batch #975, batch_size 16, seq length 1059]\tLoss: 0.001884\n","999it [00:57, 12.75it/s]Train epoch: 14 [batch #1000, batch_size 16, seq length 1073]\tLoss: 0.001922\n","1025it [01:00, 12.54it/s]Train epoch: 14 [batch #1025, batch_size 16, seq length 1085]\tLoss: 0.002018\n","1049it [01:01, 12.50it/s]Train epoch: 14 [batch #1050, batch_size 16, seq length 1099]\tLoss: 0.001939\n","1075it [01:04, 12.27it/s]Train epoch: 14 [batch #1075, batch_size 16, seq length 1113]\tLoss: 0.002039\n","1099it [01:06, 12.23it/s]Train epoch: 14 [batch #1100, batch_size 16, seq length 1126]\tLoss: 0.001921\n","1125it [01:08, 12.32it/s]Train epoch: 14 [batch #1125, batch_size 16, seq length 1140]\tLoss: 0.002074\n","1149it [01:10, 12.47it/s]Train epoch: 14 [batch #1150, batch_size 16, seq length 1154]\tLoss: 0.001890\n","1175it [01:12, 12.23it/s]Train epoch: 14 [batch #1175, batch_size 16, seq length 1167]\tLoss: 0.002066\n","1199it [01:14, 12.15it/s]Train epoch: 14 [batch #1200, batch_size 16, seq length 1180]\tLoss: 0.001917\n","1225it [01:16, 11.83it/s]Train epoch: 14 [batch #1225, batch_size 16, seq length 1195]\tLoss: 0.002044\n","1249it [01:18, 11.83it/s]Train epoch: 14 [batch #1250, batch_size 16, seq length 1208]\tLoss: 0.001975\n","1275it [01:20, 11.75it/s]Train epoch: 14 [batch #1275, batch_size 16, seq length 1220]\tLoss: 0.001996\n","1299it [01:22, 11.32it/s]Train epoch: 14 [batch #1300, batch_size 16, seq length 1234]\tLoss: 0.002179\n","1325it [01:25, 11.21it/s]Train epoch: 14 [batch #1325, batch_size 16, seq length 1248]\tLoss: 0.002026\n","1349it [01:27, 11.27it/s]Train epoch: 14 [batch #1350, batch_size 16, seq length 1262]\tLoss: 0.002130\n","1375it [01:29, 10.99it/s]Train epoch: 14 [batch #1375, batch_size 16, seq length 1276]\tLoss: 0.002302\n","1399it [01:31, 10.91it/s]Train epoch: 14 [batch #1400, batch_size 16, seq length 1290]\tLoss: 0.002242\n","1425it [01:34, 10.71it/s]Train epoch: 14 [batch #1425, batch_size 16, seq length 1304]\tLoss: 0.002333\n","1449it [01:36, 10.56it/s]Train epoch: 14 [batch #1450, batch_size 16, seq length 1319]\tLoss: 0.002112\n","1475it [01:38, 10.56it/s]Train epoch: 14 [batch #1475, batch_size 16, seq length 1332]\tLoss: 0.002365\n","1499it [01:41, 10.29it/s]Train epoch: 14 [batch #1500, batch_size 16, seq length 1346]\tLoss: 0.002176\n","1525it [01:43, 10.04it/s]Train epoch: 14 [batch #1525, batch_size 16, seq length 1362]\tLoss: 0.001989\n","1550it [01:46, 10.01it/s]Train epoch: 14 [batch #1550, batch_size 16, seq length 1377]\tLoss: 0.002422\n","1574it [01:48,  9.87it/s]Train epoch: 14 [batch #1575, batch_size 16, seq length 1392]\tLoss: 0.002033\n","1600it [01:51,  9.67it/s]Train epoch: 14 [batch #1600, batch_size 16, seq length 1406]\tLoss: 0.002141\n","1625it [01:53,  9.94it/s]Train epoch: 14 [batch #1625, batch_size 16, seq length 1422]\tLoss: 0.002341\n","1649it [01:56,  9.83it/s]Train epoch: 14 [batch #1650, batch_size 16, seq length 1437]\tLoss: 0.002276\n","1675it [01:59,  9.82it/s]Train epoch: 14 [batch #1675, batch_size 16, seq length 1454]\tLoss: 0.002314\n","1700it [02:01,  9.47it/s]Train epoch: 14 [batch #1700, batch_size 16, seq length 1469]\tLoss: 0.002448\n","1725it [02:04,  9.49it/s]Train epoch: 14 [batch #1725, batch_size 16, seq length 1484]\tLoss: 0.002257\n","1750it [02:06,  9.46it/s]Train epoch: 14 [batch #1750, batch_size 16, seq length 1501]\tLoss: 0.002118\n","1775it [02:09,  9.30it/s]Train epoch: 14 [batch #1775, batch_size 16, seq length 1518]\tLoss: 0.002404\n","1800it [02:12,  9.12it/s]Train epoch: 14 [batch #1800, batch_size 16, seq length 1533]\tLoss: 0.002612\n","1825it [02:15,  8.85it/s]Train epoch: 14 [batch #1825, batch_size 16, seq length 1548]\tLoss: 0.002545\n","1850it [02:17,  9.10it/s]Train epoch: 14 [batch #1850, batch_size 16, seq length 1566]\tLoss: 0.002223\n","1875it [02:20,  8.78it/s]Train epoch: 14 [batch #1875, batch_size 16, seq length 1582]\tLoss: 0.002495\n","1900it [02:23,  8.86it/s]Train epoch: 14 [batch #1900, batch_size 16, seq length 1599]\tLoss: 0.002599\n","1925it [02:26,  8.58it/s]Train epoch: 14 [batch #1925, batch_size 16, seq length 1616]\tLoss: 0.002425\n","1950it [02:29,  8.38it/s]Train epoch: 14 [batch #1950, batch_size 16, seq length 1633]\tLoss: 0.002388\n","1975it [02:32,  8.35it/s]Train epoch: 14 [batch #1975, batch_size 16, seq length 1651]\tLoss: 0.002485\n","2000it [02:35,  8.32it/s]Train epoch: 14 [batch #2000, batch_size 16, seq length 1670]\tLoss: 0.002365\n","2025it [02:38,  8.15it/s]Train epoch: 14 [batch #2025, batch_size 16, seq length 1688]\tLoss: 0.002480\n","2050it [02:41,  8.07it/s]Train epoch: 14 [batch #2050, batch_size 16, seq length 1707]\tLoss: 0.002515\n","2075it [02:44,  8.18it/s]Train epoch: 14 [batch #2075, batch_size 16, seq length 1728]\tLoss: 0.002667\n","2100it [02:47,  7.89it/s]Train epoch: 14 [batch #2100, batch_size 16, seq length 1748]\tLoss: 0.002484\n","2125it [02:50,  7.93it/s]Train epoch: 14 [batch #2125, batch_size 16, seq length 1768]\tLoss: 0.002760\n","2150it [02:53,  7.94it/s]Train epoch: 14 [batch #2150, batch_size 16, seq length 1787]\tLoss: 0.002391\n","2175it [02:57,  7.80it/s]Train epoch: 14 [batch #2175, batch_size 16, seq length 1809]\tLoss: 0.002835\n","2200it [03:00,  7.55it/s]Train epoch: 14 [batch #2200, batch_size 16, seq length 1833]\tLoss: 0.002573\n","2225it [03:03,  7.58it/s]Train epoch: 14 [batch #2225, batch_size 16, seq length 1855]\tLoss: 0.002608\n","2250it [03:06,  7.20it/s]Train epoch: 14 [batch #2250, batch_size 16, seq length 1878]\tLoss: 0.002868\n","2275it [03:10,  7.44it/s]Train epoch: 14 [batch #2275, batch_size 16, seq length 1901]\tLoss: 0.002713\n","2300it [03:13,  7.31it/s]Train epoch: 14 [batch #2300, batch_size 16, seq length 1927]\tLoss: 0.002795\n","2325it [03:17,  7.08it/s]Train epoch: 14 [batch #2325, batch_size 16, seq length 1953]\tLoss: 0.002851\n","2350it [03:20,  7.04it/s]Train epoch: 14 [batch #2350, batch_size 16, seq length 1980]\tLoss: 0.002797\n","2375it [03:24,  7.03it/s]Train epoch: 14 [batch #2375, batch_size 16, seq length 2006]\tLoss: 0.002877\n","2400it [03:27,  6.87it/s]Train epoch: 14 [batch #2400, batch_size 16, seq length 2034]\tLoss: 0.002966\n","2425it [03:31,  6.79it/s]Train epoch: 14 [batch #2425, batch_size 16, seq length 2064]\tLoss: 0.002894\n","2450it [03:35,  6.65it/s]Train epoch: 14 [batch #2450, batch_size 16, seq length 2094]\tLoss: 0.003003\n","2475it [03:39,  6.62it/s]Train epoch: 14 [batch #2475, batch_size 16, seq length 2126]\tLoss: 0.003068\n","2500it [03:42,  6.47it/s]Train epoch: 14 [batch #2500, batch_size 16, seq length 2160]\tLoss: 0.002997\n","2525it [03:46,  6.45it/s]Train epoch: 14 [batch #2525, batch_size 16, seq length 2192]\tLoss: 0.003202\n","2550it [03:50,  6.22it/s]Train epoch: 14 [batch #2550, batch_size 16, seq length 2228]\tLoss: 0.003117\n","2575it [03:54,  6.24it/s]Train epoch: 14 [batch #2575, batch_size 16, seq length 2266]\tLoss: 0.003327\n","2600it [03:58,  6.11it/s]Train epoch: 14 [batch #2600, batch_size 16, seq length 2307]\tLoss: 0.003210\n","2625it [04:02,  6.03it/s]Train epoch: 14 [batch #2625, batch_size 16, seq length 2350]\tLoss: 0.003142\n","2650it [04:07,  5.92it/s]Train epoch: 14 [batch #2650, batch_size 16, seq length 2392]\tLoss: 0.003259\n","2675it [04:11,  5.73it/s]Train epoch: 14 [batch #2675, batch_size 16, seq length 2439]\tLoss: 0.003429\n","2700it [04:15,  5.67it/s]Train epoch: 14 [batch #2700, batch_size 16, seq length 2491]\tLoss: 0.003393\n","2725it [04:20,  5.60it/s]Train epoch: 14 [batch #2725, batch_size 16, seq length 2500]\tLoss: 0.003526\n","2750it [04:24,  5.57it/s]Train epoch: 14 [batch #2750, batch_size 16, seq length 2500]\tLoss: 0.003405\n","2775it [04:29,  5.62it/s]Train epoch: 14 [batch #2775, batch_size 16, seq length 2500]\tLoss: 0.003645\n","2800it [04:33,  5.43it/s]Train epoch: 14 [batch #2800, batch_size 16, seq length 2500]\tLoss: 0.003485\n","2825it [04:38,  5.60it/s]Train epoch: 14 [batch #2825, batch_size 16, seq length 2500]\tLoss: 0.003500\n","2850it [04:42,  5.64it/s]Train epoch: 14 [batch #2850, batch_size 16, seq length 2500]\tLoss: 0.003637\n","2875it [04:47,  5.54it/s]Train epoch: 14 [batch #2875, batch_size 16, seq length 2500]\tLoss: 0.003909\n","2900it [04:51,  5.59it/s]Train epoch: 14 [batch #2900, batch_size 16, seq length 2500]\tLoss: 0.004089\n","2925it [04:56,  5.56it/s]Train epoch: 14 [batch #2925, batch_size 16, seq length 2500]\tLoss: 0.004307\n","2950it [05:00,  5.37it/s]Train epoch: 14 [batch #2950, batch_size 16, seq length 2500]\tLoss: 0.004307\n","2975it [05:05,  5.39it/s]Train epoch: 14 [batch #2975, batch_size 16, seq length 2500]\tLoss: 0.004921\n","2983it [05:06,  9.72it/s]\n","epoch loss: 0.0023839576092966716\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:45, 36.13it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0308, 0.0447, 0.0535, 0.0487, 0.8248\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2528, 0.3664, 0.4492, 0.4036, 0.9653\n","rec_at_8: 0.3078\n","prec_at_8: 0.5645\n","rec_at_15: 0.4185\n","prec_at_15: 0.4273\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/rnn_May_04_023326\n","\n","EPOCH 15\n","0it [00:00, ?it/s]Train epoch: 15 [batch #0, batch_size 16, seq length 117]\tLoss: 0.003971\n","23it [00:00, 35.95it/s]Train epoch: 15 [batch #25, batch_size 16, seq length 337]\tLoss: 0.002095\n","47it [00:01, 33.33it/s]Train epoch: 15 [batch #50, batch_size 16, seq length 402]\tLoss: 0.001690\n","75it [00:02, 29.81it/s]Train epoch: 15 [batch #75, batch_size 16, seq length 452]\tLoss: 0.001461\n","99it [00:03, 26.89it/s]Train epoch: 15 [batch #100, batch_size 16, seq length 490]\tLoss: 0.001382\n","123it [00:04, 25.62it/s]Train epoch: 15 [batch #125, batch_size 16, seq length 520]\tLoss: 0.001514\n","150it [00:05, 24.85it/s]Train epoch: 15 [batch #150, batch_size 16, seq length 548]\tLoss: 0.001609\n","174it [00:06, 23.76it/s]Train epoch: 15 [batch #175, batch_size 16, seq length 574]\tLoss: 0.001448\n","198it [00:07, 22.74it/s]Train epoch: 15 [batch #200, batch_size 16, seq length 596]\tLoss: 0.001595\n","225it [00:08, 21.79it/s]Train epoch: 15 [batch #225, batch_size 16, seq length 618]\tLoss: 0.001600\n","249it [00:09, 21.08it/s]Train epoch: 15 [batch #250, batch_size 16, seq length 638]\tLoss: 0.001514\n","273it [00:10, 20.28it/s]Train epoch: 15 [batch #275, batch_size 16, seq length 656]\tLoss: 0.001685\n","300it [00:12, 20.13it/s]Train epoch: 15 [batch #300, batch_size 16, seq length 674]\tLoss: 0.001601\n","324it [00:13, 19.16it/s]Train epoch: 15 [batch #325, batch_size 16, seq length 693]\tLoss: 0.001508\n","350it [00:14, 18.93it/s]Train epoch: 15 [batch #350, batch_size 16, seq length 710]\tLoss: 0.001701\n","374it [00:16, 18.14it/s]Train epoch: 15 [batch #375, batch_size 16, seq length 726]\tLoss: 0.001761\n","400it [00:17, 18.03it/s]Train epoch: 15 [batch #400, batch_size 16, seq length 743]\tLoss: 0.001532\n","424it [00:18, 17.49it/s]Train epoch: 15 [batch #425, batch_size 16, seq length 758]\tLoss: 0.001659\n","450it [00:20, 17.60it/s]Train epoch: 15 [batch #450, batch_size 16, seq length 773]\tLoss: 0.001810\n","474it [00:21, 16.96it/s]Train epoch: 15 [batch #475, batch_size 16, seq length 789]\tLoss: 0.001607\n","500it [00:23, 16.64it/s]Train epoch: 15 [batch #500, batch_size 16, seq length 803]\tLoss: 0.001777\n","524it [00:24, 16.43it/s]Train epoch: 15 [batch #525, batch_size 16, seq length 818]\tLoss: 0.001751\n","550it [00:26, 15.92it/s]Train epoch: 15 [batch #550, batch_size 16, seq length 833]\tLoss: 0.001615\n","574it [00:27, 16.32it/s]Train epoch: 15 [batch #575, batch_size 16, seq length 846]\tLoss: 0.001645\n","600it [00:29, 15.68it/s]Train epoch: 15 [batch #600, batch_size 16, seq length 860]\tLoss: 0.001838\n","624it [00:31, 15.63it/s]Train epoch: 15 [batch #625, batch_size 16, seq length 874]\tLoss: 0.001598\n","650it [00:32, 14.98it/s]Train epoch: 15 [batch #650, batch_size 16, seq length 887]\tLoss: 0.001641\n","674it [00:34, 15.33it/s]Train epoch: 15 [batch #675, batch_size 16, seq length 901]\tLoss: 0.001622\n","700it [00:36, 14.88it/s]Train epoch: 15 [batch #700, batch_size 16, seq length 915]\tLoss: 0.001506\n","724it [00:37, 14.63it/s]Train epoch: 15 [batch #725, batch_size 16, seq length 929]\tLoss: 0.001657\n","750it [00:39, 14.22it/s]Train epoch: 15 [batch #750, batch_size 16, seq length 942]\tLoss: 0.001775\n","774it [00:41, 14.29it/s]Train epoch: 15 [batch #775, batch_size 16, seq length 954]\tLoss: 0.001731\n","800it [00:43, 14.02it/s]Train epoch: 15 [batch #800, batch_size 16, seq length 968]\tLoss: 0.001787\n","824it [00:44, 13.76it/s]Train epoch: 15 [batch #825, batch_size 16, seq length 981]\tLoss: 0.001760\n","850it [00:46, 13.66it/s]Train epoch: 15 [batch #850, batch_size 16, seq length 994]\tLoss: 0.001820\n","874it [00:48, 13.29it/s]Train epoch: 15 [batch #875, batch_size 16, seq length 1006]\tLoss: 0.001698\n","900it [00:50, 13.43it/s]Train epoch: 15 [batch #900, batch_size 16, seq length 1020]\tLoss: 0.001873\n","924it [00:52, 13.13it/s]Train epoch: 15 [batch #925, batch_size 16, seq length 1033]\tLoss: 0.001664\n","950it [00:54, 13.11it/s]Train epoch: 15 [batch #950, batch_size 16, seq length 1046]\tLoss: 0.001853\n","974it [00:56, 13.12it/s]Train epoch: 15 [batch #975, batch_size 16, seq length 1059]\tLoss: 0.001852\n","1000it [00:58, 12.73it/s]Train epoch: 15 [batch #1000, batch_size 16, seq length 1073]\tLoss: 0.001806\n","1024it [01:00, 12.35it/s]Train epoch: 15 [batch #1025, batch_size 16, seq length 1085]\tLoss: 0.001925\n","1050it [01:02, 12.43it/s]Train epoch: 15 [batch #1050, batch_size 16, seq length 1099]\tLoss: 0.001897\n","1074it [01:04, 12.39it/s]Train epoch: 15 [batch #1075, batch_size 16, seq length 1113]\tLoss: 0.001943\n","1100it [01:06, 12.36it/s]Train epoch: 15 [batch #1100, batch_size 16, seq length 1126]\tLoss: 0.001865\n","1124it [01:08, 11.99it/s]Train epoch: 15 [batch #1125, batch_size 16, seq length 1140]\tLoss: 0.001941\n","1150it [01:10, 12.38it/s]Train epoch: 15 [batch #1150, batch_size 16, seq length 1154]\tLoss: 0.001854\n","1174it [01:12, 11.94it/s]Train epoch: 15 [batch #1175, batch_size 16, seq length 1167]\tLoss: 0.001912\n","1200it [01:14, 11.97it/s]Train epoch: 15 [batch #1200, batch_size 16, seq length 1180]\tLoss: 0.001819\n","1224it [01:16, 11.52it/s]Train epoch: 15 [batch #1225, batch_size 16, seq length 1195]\tLoss: 0.001968\n","1250it [01:18, 11.77it/s]Train epoch: 15 [batch #1250, batch_size 16, seq length 1208]\tLoss: 0.001872\n","1274it [01:20, 11.56it/s]Train epoch: 15 [batch #1275, batch_size 16, seq length 1220]\tLoss: 0.001888\n","1300it [01:23, 11.43it/s]Train epoch: 15 [batch #1300, batch_size 16, seq length 1234]\tLoss: 0.002032\n","1324it [01:25, 11.23it/s]Train epoch: 15 [batch #1325, batch_size 16, seq length 1248]\tLoss: 0.001944\n","1350it [01:27, 11.09it/s]Train epoch: 15 [batch #1350, batch_size 16, seq length 1262]\tLoss: 0.002005\n","1374it [01:29, 10.98it/s]Train epoch: 15 [batch #1375, batch_size 16, seq length 1276]\tLoss: 0.002183\n","1400it [01:32, 10.90it/s]Train epoch: 15 [batch #1400, batch_size 16, seq length 1290]\tLoss: 0.002225\n","1424it [01:34, 10.72it/s]Train epoch: 15 [batch #1425, batch_size 16, seq length 1304]\tLoss: 0.002215\n","1450it [01:36, 10.57it/s]Train epoch: 15 [batch #1450, batch_size 16, seq length 1319]\tLoss: 0.002033\n","1474it [01:39, 10.37it/s]Train epoch: 15 [batch #1475, batch_size 16, seq length 1332]\tLoss: 0.002268\n","1500it [01:41, 10.36it/s]Train epoch: 15 [batch #1500, batch_size 16, seq length 1346]\tLoss: 0.002105\n","1524it [01:43, 10.03it/s]Train epoch: 15 [batch #1525, batch_size 16, seq length 1362]\tLoss: 0.001909\n","1550it [01:46,  9.68it/s]Train epoch: 15 [batch #1550, batch_size 16, seq length 1377]\tLoss: 0.002301\n","1574it [01:48,  9.87it/s]Train epoch: 15 [batch #1575, batch_size 16, seq length 1392]\tLoss: 0.001915\n","1599it [01:51,  9.97it/s]Train epoch: 15 [batch #1600, batch_size 16, seq length 1406]\tLoss: 0.002104\n","1624it [01:54,  9.84it/s]Train epoch: 15 [batch #1625, batch_size 16, seq length 1422]\tLoss: 0.002322\n","1649it [01:56,  9.85it/s]Train epoch: 15 [batch #1650, batch_size 16, seq length 1437]\tLoss: 0.002196\n","1675it [01:59,  9.60it/s]Train epoch: 15 [batch #1675, batch_size 16, seq length 1454]\tLoss: 0.002266\n","1700it [02:01,  9.43it/s]Train epoch: 15 [batch #1700, batch_size 16, seq length 1469]\tLoss: 0.002418\n","1725it [02:04,  9.38it/s]Train epoch: 15 [batch #1725, batch_size 16, seq length 1484]\tLoss: 0.002206\n","1750it [02:07,  9.30it/s]Train epoch: 15 [batch #1750, batch_size 16, seq length 1501]\tLoss: 0.002075\n","1775it [02:09,  9.29it/s]Train epoch: 15 [batch #1775, batch_size 16, seq length 1518]\tLoss: 0.002377\n","1800it [02:12,  9.15it/s]Train epoch: 15 [batch #1800, batch_size 16, seq length 1533]\tLoss: 0.002500\n","1825it [02:15,  9.04it/s]Train epoch: 15 [batch #1825, batch_size 16, seq length 1548]\tLoss: 0.002473\n","1850it [02:18,  8.77it/s]Train epoch: 15 [batch #1850, batch_size 16, seq length 1566]\tLoss: 0.002242\n","1875it [02:20,  8.85it/s]Train epoch: 15 [batch #1875, batch_size 16, seq length 1582]\tLoss: 0.002451\n","1900it [02:23,  8.54it/s]Train epoch: 15 [batch #1900, batch_size 16, seq length 1599]\tLoss: 0.002491\n","1925it [02:26,  8.70it/s]Train epoch: 15 [batch #1925, batch_size 16, seq length 1616]\tLoss: 0.002262\n","1950it [02:29,  8.55it/s]Train epoch: 15 [batch #1950, batch_size 16, seq length 1633]\tLoss: 0.002379\n","1975it [02:32,  8.32it/s]Train epoch: 15 [batch #1975, batch_size 16, seq length 1651]\tLoss: 0.002367\n","2000it [02:35,  8.12it/s]Train epoch: 15 [batch #2000, batch_size 16, seq length 1670]\tLoss: 0.002273\n","2025it [02:38,  8.11it/s]Train epoch: 15 [batch #2025, batch_size 16, seq length 1688]\tLoss: 0.002414\n","2050it [02:41,  8.28it/s]Train epoch: 15 [batch #2050, batch_size 16, seq length 1707]\tLoss: 0.002375\n","2075it [02:44,  8.21it/s]Train epoch: 15 [batch #2075, batch_size 16, seq length 1728]\tLoss: 0.002557\n","2100it [02:47,  8.27it/s]Train epoch: 15 [batch #2100, batch_size 16, seq length 1748]\tLoss: 0.002386\n","2125it [02:50,  7.92it/s]Train epoch: 15 [batch #2125, batch_size 16, seq length 1768]\tLoss: 0.002661\n","2150it [02:53,  8.00it/s]Train epoch: 15 [batch #2150, batch_size 16, seq length 1787]\tLoss: 0.002339\n","2175it [02:57,  7.76it/s]Train epoch: 15 [batch #2175, batch_size 16, seq length 1809]\tLoss: 0.002738\n","2200it [03:00,  7.46it/s]Train epoch: 15 [batch #2200, batch_size 16, seq length 1833]\tLoss: 0.002412\n","2225it [03:03,  7.64it/s]Train epoch: 15 [batch #2225, batch_size 16, seq length 1855]\tLoss: 0.002469\n","2250it [03:07,  7.49it/s]Train epoch: 15 [batch #2250, batch_size 16, seq length 1878]\tLoss: 0.002775\n","2275it [03:10,  7.36it/s]Train epoch: 15 [batch #2275, batch_size 16, seq length 1901]\tLoss: 0.002564\n","2300it [03:13,  7.00it/s]Train epoch: 15 [batch #2300, batch_size 16, seq length 1927]\tLoss: 0.002707\n","2325it [03:17,  7.11it/s]Train epoch: 15 [batch #2325, batch_size 16, seq length 1953]\tLoss: 0.002765\n","2350it [03:20,  7.04it/s]Train epoch: 15 [batch #2350, batch_size 16, seq length 1980]\tLoss: 0.002638\n","2375it [03:24,  6.93it/s]Train epoch: 15 [batch #2375, batch_size 16, seq length 2006]\tLoss: 0.002789\n","2400it [03:28,  6.84it/s]Train epoch: 15 [batch #2400, batch_size 16, seq length 2034]\tLoss: 0.002791\n","2425it [03:31,  6.69it/s]Train epoch: 15 [batch #2425, batch_size 16, seq length 2064]\tLoss: 0.002753\n","2450it [03:35,  6.64it/s]Train epoch: 15 [batch #2450, batch_size 16, seq length 2094]\tLoss: 0.002891\n","2475it [03:39,  6.56it/s]Train epoch: 15 [batch #2475, batch_size 16, seq length 2126]\tLoss: 0.002939\n","2500it [03:43,  6.42it/s]Train epoch: 15 [batch #2500, batch_size 16, seq length 2160]\tLoss: 0.002903\n","2525it [03:46,  6.33it/s]Train epoch: 15 [batch #2525, batch_size 16, seq length 2192]\tLoss: 0.003022\n","2550it [03:50,  6.35it/s]Train epoch: 15 [batch #2550, batch_size 16, seq length 2228]\tLoss: 0.003085\n","2575it [03:54,  6.23it/s]Train epoch: 15 [batch #2575, batch_size 16, seq length 2266]\tLoss: 0.003183\n","2600it [03:59,  6.06it/s]Train epoch: 15 [batch #2600, batch_size 16, seq length 2307]\tLoss: 0.003071\n","2625it [04:03,  6.05it/s]Train epoch: 15 [batch #2625, batch_size 16, seq length 2350]\tLoss: 0.003030\n","2650it [04:07,  5.90it/s]Train epoch: 15 [batch #2650, batch_size 16, seq length 2392]\tLoss: 0.003085\n","2675it [04:11,  5.84it/s]Train epoch: 15 [batch #2675, batch_size 16, seq length 2439]\tLoss: 0.003299\n","2700it [04:16,  5.71it/s]Train epoch: 15 [batch #2700, batch_size 16, seq length 2491]\tLoss: 0.003234\n","2725it [04:20,  5.64it/s]Train epoch: 15 [batch #2725, batch_size 16, seq length 2500]\tLoss: 0.003317\n","2750it [04:24,  5.57it/s]Train epoch: 15 [batch #2750, batch_size 16, seq length 2500]\tLoss: 0.003242\n","2775it [04:29,  5.64it/s]Train epoch: 15 [batch #2775, batch_size 16, seq length 2500]\tLoss: 0.003468\n","2800it [04:33,  5.63it/s]Train epoch: 15 [batch #2800, batch_size 16, seq length 2500]\tLoss: 0.003369\n","2825it [04:38,  5.67it/s]Train epoch: 15 [batch #2825, batch_size 16, seq length 2500]\tLoss: 0.003385\n","2850it [04:42,  5.61it/s]Train epoch: 15 [batch #2850, batch_size 16, seq length 2500]\tLoss: 0.003612\n","2875it [04:47,  5.52it/s]Train epoch: 15 [batch #2875, batch_size 16, seq length 2500]\tLoss: 0.003778\n","2900it [04:51,  5.59it/s]Train epoch: 15 [batch #2900, batch_size 16, seq length 2500]\tLoss: 0.003898\n","2925it [04:56,  5.55it/s]Train epoch: 15 [batch #2925, batch_size 16, seq length 2500]\tLoss: 0.004165\n","2950it [05:00,  5.48it/s]Train epoch: 15 [batch #2950, batch_size 16, seq length 2500]\tLoss: 0.004217\n","2975it [05:05,  5.42it/s]Train epoch: 15 [batch #2975, batch_size 16, seq length 2500]\tLoss: 0.004749\n","2983it [05:07,  9.72it/s]\n","epoch loss: 0.002294603018510832\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:45, 35.73it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0310, 0.0458, 0.0507, 0.0481, 0.8229\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2591, 0.3928, 0.4320, 0.4115, 0.9635\n","rec_at_8: 0.3069\n","prec_at_8: 0.5642\n","rec_at_15: 0.4197\n","prec_at_15: 0.4291\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/rnn_May_04_023326\n","\n","prec_at_8 hasn't improved in 10 epochs, early stopping...\n","loading pretrained embeddings...\n","adding unk embedding\n","file for evaluation: ../../mimicdata/mimic3/dev_full.csv\n","1631it [00:45, 35.99it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0292, 0.0495, 0.0400, 0.0442, 0.8474\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2938, 0.5485, 0.3875, 0.4541, 0.9754\n","rec_at_8: 0.3363\n","prec_at_8: 0.6227\n","rec_at_15: 0.4543\n","prec_at_15: 0.4697\n","\n","\n","evaluating on test\n","file for evaluation: ../../mimicdata/mimic3/test_full.csv\n","3372it [01:34, 35.68it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0298, 0.0559, 0.0414, 0.0476, 0.8398\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2850, 0.5404, 0.3761, 0.4435, 0.9749\n","rec_at_8: 0.3204\n","prec_at_8: 0.6140\n","rec_at_15: 0.4388\n","prec_at_15: 0.4682\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/rnn_May_04_023326\n","\n","TOTAL ELAPSED TIME FOR rnn MODEL AND 17 EPOCHS: 6009.513403\n"]}]},{"cell_type":"markdown","source":["### GRU_mimic3_50\n","No starting model"],"metadata":{"id":"QQ28n-IQzcEi"}},{"cell_type":"code","source":["%cd /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/predictions/GRU_mimic3_50/\n","!python ../../learn/training.py ../../mimicdata/mimic3/train_50.csv ../../mimicdata/mimic3/vocab.csv 50 rnn 200 --rnn-dim 512 --cell-type gru --lr 0.003 --embed-file ../../mimicdata/mimic3/processed_full.embed --gpu "],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"VjYGAnPRzd4Q","executionInfo":{"status":"ok","timestamp":1651643548341,"user_tz":420,"elapsed":1117884,"user":{"displayName":"Chris Rock","userId":"13280703149428069160"}},"outputId":"99e40d1c-a7f8-431e-f8d0-fe9fec385cb9"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/predictions/GRU_mimic3_50\n","ARGS: Namespace(Y='50', batch_size=16, bidirectional=None, cell_type='gru', code_emb=None, command='python ../../learn/training.py ../../mimicdata/mimic3/train_50.csv ../../mimicdata/mimic3/vocab.csv 50 rnn 200 --rnn-dim 512 --cell-type gru --lr 0.003 --embed-file ../../mimicdata/mimic3/processed_full.embed --gpu', criterion='f1_micro', data_path='../../mimicdata/mimic3/train_50.csv', dropout=0.5, embed_file='../../mimicdata/mimic3/processed_full.embed', embed_size=100, filter_size=4, gpu=True, lmbda=0, lr=0.003, model='rnn', n_epochs=200, num_filter_maps=50, patience=3, pool=None, public_model=None, quiet=None, rnn_dim=512, rnn_layers=1, samples=None, stack_filters=None, test_model=None, version='mimic3', vocab='../../mimicdata/mimic3/vocab.csv', weight_decay=0)\n","loading lookups...\n","==========settings=============\n","<class 'int'> <class 'str'> <class 'dict'> <class 'int'> <class 'str'> <class 'int'> <class 'bool'> <class 'int'> <class 'NoneType'>\n","loading pretrained embeddings...\n","adding unk embedding\n","VanillaRNN(\n","  (embed_drop): Dropout(p=0.5, inplace=False)\n","  (embed): Embedding(51919, 100, padding_idx=0)\n","  (rnn): GRU(100, 512)\n","  (final): Linear(in_features=512, out_features=50, bias=True)\n",")\n","EPOCH 0\n","0it [00:00, ?it/s]Train epoch: 0 [batch #0, batch_size 16, seq length 212]\tLoss: 0.693944\n","24it [00:01, 21.70it/s]Train epoch: 0 [batch #25, batch_size 16, seq length 571]\tLoss: 0.253111\n","49it [00:02, 17.26it/s]Train epoch: 0 [batch #50, batch_size 16, seq length 709]\tLoss: 0.277327\n","75it [00:04, 14.82it/s]Train epoch: 0 [batch #75, batch_size 16, seq length 806]\tLoss: 0.298220\n","99it [00:05, 13.29it/s]Train epoch: 0 [batch #100, batch_size 16, seq length 892]\tLoss: 0.262021\n","125it [00:08, 12.27it/s]Train epoch: 0 [batch #125, batch_size 16, seq length 978]\tLoss: 0.268275\n","149it [00:10, 11.18it/s]Train epoch: 0 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.288918\n","175it [00:12, 10.43it/s]Train epoch: 0 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.269290\n","200it [00:15,  9.69it/s]Train epoch: 0 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.280751\n","225it [00:17,  7.82it/s]Train epoch: 0 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.290717\n","250it [00:20,  8.47it/s]Train epoch: 0 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.293370\n","275it [00:23,  8.01it/s]Train epoch: 0 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.316743\n","300it [00:27,  7.46it/s]Train epoch: 0 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.316893\n","325it [00:30,  7.02it/s]Train epoch: 0 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.305215\n","350it [00:34,  6.62it/s]Train epoch: 0 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.325543\n","375it [00:38,  6.20it/s]Train epoch: 0 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.313927\n","400it [00:42,  5.81it/s]Train epoch: 0 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.328939\n","425it [00:47,  5.24it/s]Train epoch: 0 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.323445\n","450it [00:52,  4.77it/s]Train epoch: 0 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.332613\n","475it [00:57,  4.66it/s]Train epoch: 0 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.371450\n","500it [01:03,  4.54it/s]Train epoch: 0 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.365030\n","505it [01:04,  7.88it/s]\n","epoch loss: 0.30448756645811664\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:26, 59.69it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0970, 0.2143, 0.1682, 0.1885, 0.7329\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.1749, 0.4244, 0.2293, 0.2978, 0.7449\n","rec_at_5: 0.2891\n","prec_at_5: 0.3397\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/rnn_May_04_053400\n","\n","EPOCH 1\n","0it [00:00, ?it/s]Train epoch: 1 [batch #0, batch_size 16, seq length 212]\tLoss: 0.311477\n","24it [00:00, 21.93it/s]Train epoch: 1 [batch #25, batch_size 16, seq length 571]\tLoss: 0.204723\n","49it [00:02, 16.56it/s]Train epoch: 1 [batch #50, batch_size 16, seq length 709]\tLoss: 0.212542\n","75it [00:04, 14.33it/s]Train epoch: 1 [batch #75, batch_size 16, seq length 806]\tLoss: 0.243064\n","99it [00:06, 12.70it/s]Train epoch: 1 [batch #100, batch_size 16, seq length 892]\tLoss: 0.216054\n","125it [00:08, 11.71it/s]Train epoch: 1 [batch #125, batch_size 16, seq length 978]\tLoss: 0.231343\n","149it [00:10, 10.89it/s]Train epoch: 1 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.251666\n","174it [00:12,  9.83it/s]Train epoch: 1 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.237613\n","200it [00:15,  9.46it/s]Train epoch: 1 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.250988\n","225it [00:18,  8.68it/s]Train epoch: 1 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.259486\n","250it [00:21,  8.16it/s]Train epoch: 1 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.258799\n","275it [00:24,  7.70it/s]Train epoch: 1 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.282363\n","300it [00:27,  7.20it/s]Train epoch: 1 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.284477\n","325it [00:31,  6.95it/s]Train epoch: 1 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.272925\n","350it [00:35,  6.31it/s]Train epoch: 1 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.289907\n","375it [00:39,  5.93it/s]Train epoch: 1 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.286077\n","400it [00:43,  5.57it/s]Train epoch: 1 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.291630\n","425it [00:48,  5.19it/s]Train epoch: 1 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.287955\n","450it [00:53,  4.62it/s]Train epoch: 1 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.298435\n","475it [00:59,  4.51it/s]Train epoch: 1 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.329828\n","500it [01:04,  4.42it/s]Train epoch: 1 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.333992\n","505it [01:05,  7.69it/s]\n","epoch loss: 0.2638722335348035\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:26, 58.53it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.1731, 0.3368, 0.2409, 0.2809, 0.7668\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2578, 0.5624, 0.3224, 0.4099, 0.8058\n","rec_at_5: 0.3866\n","prec_at_5: 0.4273\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/rnn_May_04_053400\n","\n","EPOCH 2\n","0it [00:00, ?it/s]Train epoch: 2 [batch #0, batch_size 16, seq length 212]\tLoss: 0.292581\n","23it [00:00, 21.68it/s]Train epoch: 2 [batch #25, batch_size 16, seq length 571]\tLoss: 0.192449\n","50it [00:02, 16.20it/s]Train epoch: 2 [batch #50, batch_size 16, seq length 709]\tLoss: 0.193520\n","74it [00:04, 14.17it/s]Train epoch: 2 [batch #75, batch_size 16, seq length 806]\tLoss: 0.222402\n","100it [00:06, 12.41it/s]Train epoch: 2 [batch #100, batch_size 16, seq length 892]\tLoss: 0.196334\n","124it [00:08, 11.58it/s]Train epoch: 2 [batch #125, batch_size 16, seq length 978]\tLoss: 0.206919\n","150it [00:10, 10.54it/s]Train epoch: 2 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.233274\n","175it [00:13,  9.91it/s]Train epoch: 2 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.213524\n","200it [00:15,  9.32it/s]Train epoch: 2 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.231054\n","225it [00:18,  8.62it/s]Train epoch: 2 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.240610\n","250it [00:21,  8.21it/s]Train epoch: 2 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.238422\n","275it [00:24,  7.59it/s]Train epoch: 2 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.256332\n","300it [00:28,  7.03it/s]Train epoch: 2 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.259228\n","325it [00:31,  6.70it/s]Train epoch: 2 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.249035\n","350it [00:35,  6.21it/s]Train epoch: 2 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.263488\n","375it [00:39,  5.91it/s]Train epoch: 2 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.265162\n","400it [00:44,  5.48it/s]Train epoch: 2 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.272432\n","425it [00:49,  5.07it/s]Train epoch: 2 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.271027\n","450it [00:54,  4.55it/s]Train epoch: 2 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.279032\n","475it [01:00,  4.49it/s]Train epoch: 2 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.294692\n","500it [01:05,  4.33it/s]Train epoch: 2 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.298425\n","505it [01:06,  7.57it/s]\n","epoch loss: 0.24143163809681883\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:27, 57.08it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.2035, 0.4110, 0.2804, 0.3334, 0.7895\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2864, 0.5754, 0.3631, 0.4453, 0.8310\n","rec_at_5: 0.4333\n","prec_at_5: 0.4598\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/rnn_May_04_053400\n","\n","EPOCH 3\n","0it [00:00, ?it/s]Train epoch: 3 [batch #0, batch_size 16, seq length 212]\tLoss: 0.292830\n","23it [00:00, 21.08it/s]Train epoch: 3 [batch #25, batch_size 16, seq length 571]\tLoss: 0.177117\n","49it [00:02, 15.99it/s]Train epoch: 3 [batch #50, batch_size 16, seq length 709]\tLoss: 0.180425\n","75it [00:04, 13.64it/s]Train epoch: 3 [batch #75, batch_size 16, seq length 806]\tLoss: 0.206016\n","99it [00:06, 12.37it/s]Train epoch: 3 [batch #100, batch_size 16, seq length 892]\tLoss: 0.182501\n","125it [00:08, 11.33it/s]Train epoch: 3 [batch #125, batch_size 16, seq length 978]\tLoss: 0.189819\n","149it [00:10, 10.42it/s]Train epoch: 3 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.214574\n","175it [00:13,  9.87it/s]Train epoch: 3 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.202480\n","200it [00:16,  9.02it/s]Train epoch: 3 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.208945\n","225it [00:18,  8.34it/s]Train epoch: 3 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.224822\n","250it [00:22,  8.20it/s]Train epoch: 3 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.225564\n","275it [00:25,  7.50it/s]Train epoch: 3 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.237637\n","300it [00:28,  6.99it/s]Train epoch: 3 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.243657\n","325it [00:32,  6.62it/s]Train epoch: 3 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.233542\n","350it [00:36,  6.20it/s]Train epoch: 3 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.246336\n","375it [00:40,  5.82it/s]Train epoch: 3 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.252602\n","400it [00:45,  5.47it/s]Train epoch: 3 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.257942\n","425it [00:49,  5.01it/s]Train epoch: 3 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.253363\n","450it [00:55,  4.53it/s]Train epoch: 3 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.258931\n","475it [01:00,  4.42it/s]Train epoch: 3 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.262944\n","500it [01:06,  4.31it/s]Train epoch: 3 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.262068\n","505it [01:07,  7.46it/s]\n","epoch loss: 0.2243334012751532\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:27, 57.05it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.2203, 0.4656, 0.2802, 0.3498, 0.7966\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.2968, 0.6252, 0.3610, 0.4577, 0.8438\n","rec_at_5: 0.4495\n","prec_at_5: 0.4763\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/rnn_May_04_053400\n","\n","EPOCH 4\n","0it [00:00, ?it/s]Train epoch: 4 [batch #0, batch_size 16, seq length 212]\tLoss: 0.278561\n","24it [00:01, 21.29it/s]Train epoch: 4 [batch #25, batch_size 16, seq length 571]\tLoss: 0.161565\n","50it [00:02, 16.02it/s]Train epoch: 4 [batch #50, batch_size 16, seq length 709]\tLoss: 0.169589\n","74it [00:04, 13.67it/s]Train epoch: 4 [batch #75, batch_size 16, seq length 806]\tLoss: 0.191033\n","100it [00:06, 12.27it/s]Train epoch: 4 [batch #100, batch_size 16, seq length 892]\tLoss: 0.167238\n","124it [00:08, 11.30it/s]Train epoch: 4 [batch #125, batch_size 16, seq length 978]\tLoss: 0.175573\n","150it [00:10, 10.44it/s]Train epoch: 4 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.198504\n","175it [00:13,  9.86it/s]Train epoch: 4 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.180852\n","200it [00:16,  8.89it/s]Train epoch: 4 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.194001\n","225it [00:18,  8.44it/s]Train epoch: 4 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.209432\n","250it [00:21,  8.21it/s]Train epoch: 4 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.208598\n","275it [00:25,  7.49it/s]Train epoch: 4 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.217769\n","300it [00:28,  7.03it/s]Train epoch: 4 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.226617\n","325it [00:32,  6.65it/s]Train epoch: 4 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.214124\n","350it [00:36,  6.19it/s]Train epoch: 4 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.225865\n","375it [00:40,  5.85it/s]Train epoch: 4 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.229834\n","400it [00:44,  5.46it/s]Train epoch: 4 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.239428\n","425it [00:49,  5.06it/s]Train epoch: 4 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.230856\n","450it [00:54,  4.49it/s]Train epoch: 4 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.242135\n","475it [01:00,  4.43it/s]Train epoch: 4 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.230727\n","500it [01:06,  4.32it/s]Train epoch: 4 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.226720\n","505it [01:07,  7.50it/s]\n","epoch loss: 0.20541727128300336\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:27, 56.71it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.2366, 0.4593, 0.3026, 0.3648, 0.8009\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3062, 0.6079, 0.3816, 0.4688, 0.8459\n","rec_at_5: 0.4647\n","prec_at_5: 0.4873\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/rnn_May_04_053400\n","\n","EPOCH 5\n","0it [00:00, ?it/s]Train epoch: 5 [batch #0, batch_size 16, seq length 212]\tLoss: 0.275442\n","24it [00:01, 21.18it/s]Train epoch: 5 [batch #25, batch_size 16, seq length 571]\tLoss: 0.149040\n","50it [00:02, 16.06it/s]Train epoch: 5 [batch #50, batch_size 16, seq length 709]\tLoss: 0.157456\n","74it [00:04, 13.82it/s]Train epoch: 5 [batch #75, batch_size 16, seq length 806]\tLoss: 0.179238\n","100it [00:06, 12.39it/s]Train epoch: 5 [batch #100, batch_size 16, seq length 892]\tLoss: 0.155336\n","124it [00:08, 11.36it/s]Train epoch: 5 [batch #125, batch_size 16, seq length 978]\tLoss: 0.162656\n","150it [00:10, 10.34it/s]Train epoch: 5 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.181517\n","175it [00:13,  9.91it/s]Train epoch: 5 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.166677\n","200it [00:16,  9.00it/s]Train epoch: 5 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.176777\n","225it [00:18,  8.34it/s]Train epoch: 5 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.192951\n","250it [00:21,  8.20it/s]Train epoch: 5 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.190814\n","275it [00:25,  7.56it/s]Train epoch: 5 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.200395\n","300it [00:28,  7.02it/s]Train epoch: 5 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.209546\n","325it [00:32,  6.76it/s]Train epoch: 5 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.196663\n","350it [00:36,  6.11it/s]Train epoch: 5 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.210382\n","375it [00:40,  5.88it/s]Train epoch: 5 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.217600\n","400it [00:44,  5.51it/s]Train epoch: 5 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.228931\n","425it [00:49,  4.96it/s]Train epoch: 5 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.219914\n","450it [00:55,  4.54it/s]Train epoch: 5 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.228534\n","475it [01:00,  4.41it/s]Train epoch: 5 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.198382\n","500it [01:06,  4.32it/s]Train epoch: 5 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.200838\n","505it [01:07,  7.48it/s]\n","epoch loss: 0.18943618898934658\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:27, 56.97it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.2574, 0.4544, 0.3406, 0.3894, 0.8048\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3287, 0.5940, 0.4240, 0.4948, 0.8500\n","rec_at_5: 0.4723\n","prec_at_5: 0.4956\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/rnn_May_04_053400\n","\n","EPOCH 6\n","0it [00:00, ?it/s]Train epoch: 6 [batch #0, batch_size 16, seq length 212]\tLoss: 0.248461\n","24it [00:01, 21.22it/s]Train epoch: 6 [batch #25, batch_size 16, seq length 571]\tLoss: 0.138686\n","50it [00:02, 15.96it/s]Train epoch: 6 [batch #50, batch_size 16, seq length 709]\tLoss: 0.151247\n","74it [00:04, 13.81it/s]Train epoch: 6 [batch #75, batch_size 16, seq length 806]\tLoss: 0.165616\n","100it [00:06, 12.34it/s]Train epoch: 6 [batch #100, batch_size 16, seq length 892]\tLoss: 0.143166\n","124it [00:08, 11.53it/s]Train epoch: 6 [batch #125, batch_size 16, seq length 978]\tLoss: 0.150516\n","150it [00:10, 10.36it/s]Train epoch: 6 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.170363\n","175it [00:13,  9.89it/s]Train epoch: 6 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.155910\n","200it [00:15,  9.00it/s]Train epoch: 6 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.163660\n","225it [00:18,  8.43it/s]Train epoch: 6 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.187589\n","250it [00:21,  8.16it/s]Train epoch: 6 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.182722\n","275it [00:25,  7.56it/s]Train epoch: 6 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.190943\n","300it [00:28,  7.05it/s]Train epoch: 6 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.200377\n","325it [00:32,  6.79it/s]Train epoch: 6 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.187047\n","350it [00:36,  6.19it/s]Train epoch: 6 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.197948\n","375it [00:40,  5.89it/s]Train epoch: 6 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.206773\n","400it [00:44,  5.45it/s]Train epoch: 6 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.211572\n","425it [00:49,  5.07it/s]Train epoch: 6 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.211865\n","450it [00:54,  4.55it/s]Train epoch: 6 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.216078\n","475it [01:00,  4.46it/s]Train epoch: 6 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.171117\n","500it [01:06,  4.37it/s]Train epoch: 6 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.181667\n","505it [01:07,  7.52it/s]\n","epoch loss: 0.17757706971156714\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:27, 56.88it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.2578, 0.4421, 0.3706, 0.4032, 0.8000\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3358, 0.5323, 0.4764, 0.5028, 0.8450\n","rec_at_5: 0.4631\n","prec_at_5: 0.4842\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/rnn_May_04_053400\n","\n","EPOCH 7\n","0it [00:00, ?it/s]Train epoch: 7 [batch #0, batch_size 16, seq length 212]\tLoss: 0.233182\n","24it [00:01, 21.16it/s]Train epoch: 7 [batch #25, batch_size 16, seq length 571]\tLoss: 0.134889\n","50it [00:02, 16.05it/s]Train epoch: 7 [batch #50, batch_size 16, seq length 709]\tLoss: 0.141844\n","74it [00:04, 13.82it/s]Train epoch: 7 [batch #75, batch_size 16, seq length 806]\tLoss: 0.159849\n","100it [00:06, 12.37it/s]Train epoch: 7 [batch #100, batch_size 16, seq length 892]\tLoss: 0.134460\n","124it [00:08, 11.38it/s]Train epoch: 7 [batch #125, batch_size 16, seq length 978]\tLoss: 0.143995\n","150it [00:10, 10.21it/s]Train epoch: 7 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.162432\n","175it [00:13,  9.84it/s]Train epoch: 7 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.144794\n","200it [00:16,  9.03it/s]Train epoch: 7 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.157121\n","225it [00:18,  8.27it/s]Train epoch: 7 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.176695\n","250it [00:21,  8.08it/s]Train epoch: 7 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.169207\n","275it [00:25,  7.47it/s]Train epoch: 7 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.178272\n","300it [00:28,  7.07it/s]Train epoch: 7 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.191458\n","325it [00:32,  6.71it/s]Train epoch: 7 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.174681\n","350it [00:36,  6.16it/s]Train epoch: 7 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.186991\n","375it [00:40,  5.89it/s]Train epoch: 7 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.193137\n","400it [00:44,  5.47it/s]Train epoch: 7 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.201704\n","425it [00:49,  5.00it/s]Train epoch: 7 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.192007\n","450it [00:55,  4.53it/s]Train epoch: 7 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.199578\n","475it [01:00,  4.37it/s]Train epoch: 7 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.155661\n","500it [01:06,  4.33it/s]Train epoch: 7 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.165940\n","505it [01:07,  7.47it/s]\n","epoch loss: 0.1665851195407386\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:27, 56.85it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.2660, 0.4568, 0.3551, 0.3996, 0.7997\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3424, 0.5758, 0.4578, 0.5101, 0.8499\n","rec_at_5: 0.4728\n","prec_at_5: 0.4987\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/rnn_May_04_053400\n","\n","EPOCH 8\n","0it [00:00, ?it/s]Train epoch: 8 [batch #0, batch_size 16, seq length 212]\tLoss: 0.222880\n","23it [00:00, 21.46it/s]Train epoch: 8 [batch #25, batch_size 16, seq length 571]\tLoss: 0.132152\n","49it [00:02, 16.21it/s]Train epoch: 8 [batch #50, batch_size 16, seq length 709]\tLoss: 0.139791\n","75it [00:04, 13.71it/s]Train epoch: 8 [batch #75, batch_size 16, seq length 806]\tLoss: 0.154023\n","99it [00:06, 12.39it/s]Train epoch: 8 [batch #100, batch_size 16, seq length 892]\tLoss: 0.132245\n","125it [00:08, 11.46it/s]Train epoch: 8 [batch #125, batch_size 16, seq length 978]\tLoss: 0.134116\n","149it [00:10, 10.39it/s]Train epoch: 8 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.157987\n","175it [00:13,  9.82it/s]Train epoch: 8 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.140111\n","200it [00:15,  9.09it/s]Train epoch: 8 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.145932\n","225it [00:18,  8.32it/s]Train epoch: 8 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.165662\n","250it [00:21,  7.94it/s]Train epoch: 8 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.165564\n","275it [00:25,  7.52it/s]Train epoch: 8 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.169339\n","300it [00:28,  7.02it/s]Train epoch: 8 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.174797\n","325it [00:32,  6.73it/s]Train epoch: 8 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.162656\n","350it [00:36,  6.18it/s]Train epoch: 8 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.179650\n","375it [00:40,  5.83it/s]Train epoch: 8 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.186240\n","400it [00:44,  5.43it/s]Train epoch: 8 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.190583\n","425it [00:49,  5.00it/s]Train epoch: 8 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.185670\n","450it [00:54,  4.54it/s]Train epoch: 8 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.187670\n","475it [01:00,  4.44it/s]Train epoch: 8 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.134825\n","500it [01:06,  4.36it/s]Train epoch: 8 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.145444\n","505it [01:07,  7.51it/s]\n","epoch loss: 0.1576738751391963\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:27, 56.61it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.2620, 0.4409, 0.3733, 0.4043, 0.7971\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3330, 0.5317, 0.4712, 0.4996, 0.8408\n","rec_at_5: 0.4521\n","prec_at_5: 0.4832\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/rnn_May_04_053400\n","\n","EPOCH 9\n","0it [00:00, ?it/s]Train epoch: 9 [batch #0, batch_size 16, seq length 212]\tLoss: 0.210574\n","24it [00:01, 21.16it/s]Train epoch: 9 [batch #25, batch_size 16, seq length 571]\tLoss: 0.119554\n","50it [00:02, 15.69it/s]Train epoch: 9 [batch #50, batch_size 16, seq length 709]\tLoss: 0.131619\n","74it [00:04, 13.92it/s]Train epoch: 9 [batch #75, batch_size 16, seq length 806]\tLoss: 0.148311\n","100it [00:06, 12.39it/s]Train epoch: 9 [batch #100, batch_size 16, seq length 892]\tLoss: 0.124480\n","124it [00:08, 11.35it/s]Train epoch: 9 [batch #125, batch_size 16, seq length 978]\tLoss: 0.127872\n","150it [00:10, 10.27it/s]Train epoch: 9 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.148371\n","175it [00:13,  9.89it/s]Train epoch: 9 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.131373\n","200it [00:16,  9.09it/s]Train epoch: 9 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.137790\n","225it [00:18,  8.57it/s]Train epoch: 9 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.153265\n","250it [00:21,  8.04it/s]Train epoch: 9 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.152128\n","275it [00:25,  7.42it/s]Train epoch: 9 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.158108\n","300it [00:28,  6.98it/s]Train epoch: 9 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.167874\n","325it [00:32,  6.69it/s]Train epoch: 9 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.150999\n","350it [00:36,  6.14it/s]Train epoch: 9 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.167426\n","375it [00:40,  5.88it/s]Train epoch: 9 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.170293\n","400it [00:44,  5.49it/s]Train epoch: 9 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.177172\n","425it [00:49,  5.02it/s]Train epoch: 9 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.173207\n","450it [00:55,  4.51it/s]Train epoch: 9 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.174442\n","475it [01:00,  4.43it/s]Train epoch: 9 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.129887\n","500it [01:06,  4.39it/s]Train epoch: 9 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.121108\n","505it [01:07,  7.50it/s]\n","epoch loss: 0.14767008985180666\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:27, 56.61it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.2588, 0.4301, 0.3667, 0.3959, 0.7957\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3291, 0.5320, 0.4633, 0.4953, 0.8408\n","rec_at_5: 0.4534\n","prec_at_5: 0.4843\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/rnn_May_04_053400\n","\n","EPOCH 10\n","0it [00:00, ?it/s]Train epoch: 10 [batch #0, batch_size 16, seq length 212]\tLoss: 0.213080\n","24it [00:01, 21.11it/s]Train epoch: 10 [batch #25, batch_size 16, seq length 571]\tLoss: 0.114378\n","50it [00:02, 16.00it/s]Train epoch: 10 [batch #50, batch_size 16, seq length 709]\tLoss: 0.123488\n","74it [00:04, 13.89it/s]Train epoch: 10 [batch #75, batch_size 16, seq length 806]\tLoss: 0.140069\n","100it [00:06, 12.38it/s]Train epoch: 10 [batch #100, batch_size 16, seq length 892]\tLoss: 0.118525\n","124it [00:08, 11.48it/s]Train epoch: 10 [batch #125, batch_size 16, seq length 978]\tLoss: 0.117401\n","150it [00:10, 10.19it/s]Train epoch: 10 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.140226\n","175it [00:13,  9.86it/s]Train epoch: 10 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.124480\n","200it [00:15,  9.07it/s]Train epoch: 10 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.126317\n","225it [00:18,  8.40it/s]Train epoch: 10 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.142901\n","250it [00:21,  8.16it/s]Train epoch: 10 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.145994\n","275it [00:25,  7.50it/s]Train epoch: 10 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.154343\n","300it [00:28,  6.99it/s]Train epoch: 10 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.158252\n","325it [00:32,  6.76it/s]Train epoch: 10 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.143649\n","350it [00:36,  6.21it/s]Train epoch: 10 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.156736\n","375it [00:40,  5.74it/s]Train epoch: 10 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.161919\n","400it [00:44,  5.48it/s]Train epoch: 10 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.168997\n","425it [00:49,  5.05it/s]Train epoch: 10 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.168633\n","450it [00:54,  4.52it/s]Train epoch: 10 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.170747\n","475it [01:00,  4.43it/s]Train epoch: 10 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.116731\n","500it [01:06,  4.33it/s]Train epoch: 10 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.112869\n","505it [01:07,  7.50it/s]\n","epoch loss: 0.13913982269344943\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:27, 56.62it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.2521, 0.4369, 0.3469, 0.3867, 0.7955\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3309, 0.5512, 0.4529, 0.4972, 0.8416\n","rec_at_5: 0.4546\n","prec_at_5: 0.4846\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/rnn_May_04_053400\n","\n","f1_micro hasn't improved in 3 epochs, early stopping...\n","==========settings=============\n","<class 'int'> <class 'str'> <class 'dict'> <class 'int'> <class 'str'> <class 'int'> <class 'bool'> <class 'int'> <class 'NoneType'>\n","loading pretrained embeddings...\n","adding unk embedding\n","file for evaluation: ../../mimicdata/mimic3/dev_50.csv\n","1573it [00:27, 56.55it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.2660, 0.4568, 0.3551, 0.3996, 0.7997\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3424, 0.5758, 0.4578, 0.5101, 0.8499\n","rec_at_5: 0.4728\n","prec_at_5: 0.4987\n","\n","\n","evaluating on test\n","file for evaluation: ../../mimicdata/mimic3/test_50.csv\n","1729it [00:31, 55.37it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.2691, 0.4471, 0.3583, 0.3978, 0.7937\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3447, 0.5775, 0.4609, 0.5127, 0.8466\n","rec_at_5: 0.4742\n","prec_at_5: 0.5078\n","\n","saved metrics, params, model to directory /content/gdrive/Othercomputers/Pebble-MBP16/mcs/cs598-healthcare/paper-project/code/CS598DL4H/caml-mimic/mimicdata/saved_models/rnn_May_04_053400\n","\n","TOTAL ELAPSED TIME FOR rnn MODEL AND 12 EPOCHS: 1115.653681\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"easuS5mXzxa-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Get Metrics from Saved Predictions\n"],"metadata":{"id":"7xDPBTJJr5HJ"}},{"cell_type":"code","source":["print()\n","print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n","print(\"CAML_mimic3_50 ------------------------------------------------------\")\n","!python get_metrics_for_saved_predictions.py predictions/CAML_mimic3_50/\n","print()\n","print(\"CAML_mimic3_ful -----------------------------------------------------\")\n","!python get_metrics_for_saved_predictions.py predictions/CAML_mimic3_full/\n","\n","print()\n","print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n","print(\"DRCAML_mimic3_50 ----------------------------------------------------\")\n","!python get_metrics_for_saved_predictions.py predictions/DRCAML_mimic3_50/\n","print()\n","print(\"DRCAML_mimic3_full --------------------------------------------------\")\n","!python get_metrics_for_saved_predictions.py predictions/DRCAML_mimic3_full/\n","\n","\n","print()\n","print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n","print(\"CNN_mimic3_50 -------------------------------------------------------\")\n","!python get_metrics_for_saved_predictions.py predictions/CNN_mimic3_50/\n","print()\n","print(\"CNN_mimic3_full -----------------------------------------------------\")\n","!python get_metrics_for_saved_predictions.py predictions/CNN_mimic3_full/\n","\n","print()\n","print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n","print(\"GRU_mimic3_50 -------------------------------------------------------\")\n","!python get_metrics_for_saved_predictions.py predictions/GRU_mimic3_50/\n","print()\n","print(\"GRU_mimic3_full -----------------------------------------------------\")\n","!python get_metrics_for_saved_predictions.py predictions/GRU_mimic3_full/\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZSprqFursAJQ","executionInfo":{"status":"ok","timestamp":1651812852377,"user_tz":420,"elapsed":112586,"user":{"displayName":"Chris Rock","userId":"13280703149428069160"}},"outputId":"ef0d2dd8-23a3-4554-c929-870516840b03"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n","CAML_mimic3_50 ------------------------------------------------------\n","loading predictions\n","loading ground truth\n","reformatting predictions\n","1729it [00:00, 57642.55it/s]\n","evaluating all other metrics\n","\n","[MACRO] accuracy, precision, recall, f-measure\n","0.3933, 0.6035, 0.4799, 0.5346\n","[MICRO] accuracy, precision, recall, f-measure\n","0.4431, 0.7144, 0.5384, 0.6141\n","\n","\n","CAML_mimic3_ful ------------------------------------------------------\n","loading predictions\n","loading ground truth\n","reformatting predictions\n","3372it [00:08, 411.78it/s]\n","evaluating code-type metrics\n","3367it [00:03, 958.40it/s]\n","3367it [00:00, 3426.26it/s]\n","[BY CODE TYPE] f1-diag f1-proc\n","0.5239 0.6087\n","evaluating all other metrics\n","\n","[MACRO] accuracy, precision, recall, f-measure\n","0.0608, 0.0913, 0.0856, 0.0884\n","[MICRO] accuracy, precision, recall, f-measure\n","0.3686, 0.6070, 0.4842, 0.5387\n","\n","\n","+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n","DRCAML_mimic3_50 ------------------------------------------------------\n","loading predictions\n","loading ground truth\n","reformatting predictions\n","1729it [00:00, 56232.81it/s]\n","evaluating all other metrics\n","\n","[MACRO] accuracy, precision, recall, f-measure\n","0.4158, 0.6390, 0.5235, 0.5755\n","[MICRO] accuracy, precision, recall, f-measure\n","0.4628, 0.6909, 0.5836, 0.6327\n","\n","\n","DRCAML_mimic3_ful ------------------------------------------------------\n","loading predictions\n","loading ground truth\n","reformatting predictions\n","3372it [00:08, 409.45it/s]\n","evaluating code-type metrics\n","3369it [00:03, 982.12it/s]\n","3369it [00:00, 3393.19it/s]\n","[BY CODE TYPE] f1-diag f1-proc\n","0.5146 0.5954\n","evaluating all other metrics\n","\n","[MACRO] accuracy, precision, recall, f-measure\n","0.0590, 0.0848, 0.0881, 0.0864\n","[MICRO] accuracy, precision, recall, f-measure\n","0.3596, 0.5525, 0.5075, 0.5290\n","\n","\n","+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n","CNN_mimic3_50 ------------------------------------------------------\n","loading predictions\n","loading ground truth\n","reformatting predictions\n","1729it [00:00, 62496.35it/s]\n","evaluating all other metrics\n","\n","[MACRO] accuracy, precision, recall, f-measure\n","0.4204, 0.5163, 0.6509, 0.5759\n","[MICRO] accuracy, precision, recall, f-measure\n","0.4542, 0.5673, 0.6950, 0.6247\n","\n","\n","CNN_mimic3_full ------------------------------------------------------\n","loading predictions\n","loading ground truth\n","reformatting predictions\n","3372it [00:08, 416.46it/s]\n","evaluating code-type metrics\n","3357it [00:03, 908.38it/s]\n","3357it [00:00, 3379.08it/s]\n","[BY CODE TYPE] f1-diag f1-proc\n","0.4017 0.4909\n","evaluating all other metrics\n","\n","[MACRO] accuracy, precision, recall, f-measure\n","0.0253, 0.0449, 0.0417, 0.0432\n","[MICRO] accuracy, precision, recall, f-measure\n","0.2648, 0.4727, 0.3757, 0.4187\n","\n","\n","+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n","GRU_mimic3_50 ------------------------------------------------------\n","loading predictions\n","loading ground truth\n","reformatting predictions\n","1729it [00:00, 54404.05it/s]\n","evaluating all other metrics\n","\n","[MACRO] accuracy, precision, recall, f-measure\n","0.3826, 0.5523, 0.5141, 0.5325\n","[MICRO] accuracy, precision, recall, f-measure\n","0.4316, 0.6196, 0.5871, 0.6029\n","\n","\n","GRU_mimic3_full ------------------------------------------------------\n","loading predictions\n","loading ground truth\n","reformatting predictions\n","3372it [00:08, 420.43it/s]\n","evaluating code-type metrics\n","3358it [00:03, 969.58it/s]\n","3358it [00:00, 3620.70it/s]\n","[BY CODE TYPE] f1-diag f1-proc\n","0.3931 0.5139\n","evaluating all other metrics\n","\n","[MACRO] accuracy, precision, recall, f-measure\n","0.0275, 0.0554, 0.0379, 0.0450\n","[MICRO] accuracy, precision, recall, f-measure\n","0.2635, 0.5318, 0.3431, 0.4171\n","\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"30K_f3f4uGAm"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"run_predictions.ipynb","provenance":[]},"interpreter":{"hash":"7c46650b2459f4bca0ae8ea07113b0c3cbb3fe1d21e7ae09b4709d29ac9d9646"},"kernelspec":{"display_name":"Python 3.7.9 ('py37')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"nbformat":4,"nbformat_minor":0}